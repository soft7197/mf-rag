{
    "Math-6": {
        "function_num": 7,
        "functions": [
            {
                "path": "src/main/java/org/apache/commons/math3/optim/BaseOptimizer.java",
                "start_loc": 47,
                "end_loc": 52,
                "buggy_function": "    protected BaseOptimizer(ConvergenceChecker<PAIR> checker) {\n        this.checker = checker;\n\n        evaluations = new Incrementor(0, new MaxEvalCallback());\n        iterations = new Incrementor(0, new MaxIterCallback());\n    }\n",
                "fixed_function": "    protected BaseOptimizer(ConvergenceChecker<PAIR> checker) {\n        this.checker = checker;\n\n        evaluations = new Incrementor(0, new MaxEvalCallback());\n        iterations = new Incrementor(Integer.MAX_VALUE, new MaxIterCallback());\n    }\n",
                "comment": "    /**\n     * @param checker Convergence checker.\n     */\n",
                "normalized_body": [
                    "    protected BaseOptimizer(ConvergenceChecker<PAIR> checker) {\n        this.checker = checker;\n\n        evaluations = new Incrementor(0, new MaxEvalCallback());\n        iterations = new Incrementor(0, new MaxIterCallback());\n    }\n"
                ],
                "isConstructor": true,
                "top_similar_methods": [
                    [
                        0.9650534987449646,
                        "@Test public void testParseProper() { String source = \\\"1 2 / 3\\\"; { BigFraction c = properFormat.parse(source); Assert.assertNotNull(c); Assert.assertEquals(5, c.getNumeratorAsInt()); Assert.assertEquals(3, c.getDenominatorAsInt()); } try { improperFormat.parse(source); Assert.fail(\\\"invalid improper fraction.\\\"); } catch (MathParseException ex) { } }"
                    ],
                    [
                        0.9632243514060974,
                        "@Test public void testConstrainedRosenWithMoreInterpolationPoints() { final double[] startPoint = point(DIM, 0.1); final double[][] boundaries = boundaries(DIM, -1, 2); final PointValuePair expected = new PointValuePair(point(DIM, 1.0), 0.0); final int maxAdditionalPoints = 47; for (int num = 1; num <= maxAdditionalPoints; num++) { doTest(new Rosen(), startPoint, boundaries, GoalType.MINIMIZE, 1e-12, 1e-6, 2000, num, expected, \\\"num=\\\" + num); } }"
                    ],
                    [
                        0.9632243514060974,
                        "@Test public void testConstrainedRosenWithMoreInterpolationPoints() { final double[] startPoint = point(DIM, 0.1); final double[][] boundaries = boundaries(DIM, -1, 2); final PointValuePair expected = new PointValuePair(point(DIM, 1.0), 0.0); final int maxAdditionalPoints = 47; for (int num = 1; num <= maxAdditionalPoints; num++) { doTest(new Rosen(), startPoint, boundaries, GoalType.MINIMIZE, 1e-12, 1e-6, 2000, num, expected, \\\"num=\\\" + num); } }"
                    ],
                    [
                        0.9626363515853882,
                        "@Test public void testNexBytes() { long[] count = new long[256]; byte[] bytes = new byte[10]; double[] expected = new double[256]; final int sampleSize = 100000; for (int i = 0; i < 256; i++) { expected[i] = (double) sampleSize / 265f; } for (int k = 0; k < sampleSize; ++k) { generator.nextBytes(bytes); for (byte b : bytes) { ++count[b + 128]; } } TestUtils.assertChiSquareAccept(expected, count, 0.001); }"
                    ],
                    [
                        0.9620957970619202,
                        "@Test public void testPowField() { for (double x = -0.9; x < 0.9; x += 0.05) { for (double y = 0.1; y < 4; y += 0.2) { checkRelative(FastMath.pow(x, y), build(x).pow(build(y))); } } }"
                    ],
                    [
                        0.9620920419692993,
                        "@Test public void testNextPoissonConsistency() { for (int i = 1; i < 100; i++) { checkNextPoissonConsistency(i); } for (int i = 1; i < 10; i++) { checkNextPoissonConsistency(randomData.nextUniform(1, 1000)); } for (int i = 1; i < 10; i++) { checkNextPoissonConsistency(randomData.nextUniform(1000, 3000)); } }"
                    ],
                    [
                        0.9610763788223267,
                        "@Test public void testRandomData() { for (int run = 0; run < 100; run++) { Random r = new Random(System.currentTimeMillis()); int size = r.nextInt(20) + 4; double[][] data = new double[size][size]; for (int i = 0; i < size; i++) { for (int j = 0; j < size; j++) { data[i][j] = r.nextInt(100); } } RealMatrix m = MatrixUtils.createRealMatrix(data); RealMatrix h = checkAEqualPHPt(m); checkHessenbergForm(h); } }"
                    ],
                    [
                        0.9608742594718933,
                        "@Test public void testRandomData() { for (int run = 0; run < 100; run++) { Random r = new Random(System.currentTimeMillis()); int size = r.nextInt(20) + 4; double[][] data = new double[size][size]; for (int i = 0; i < size; i++) { for (int j = 0; j < size; j++) { data[i][j] = r.nextInt(100); } } RealMatrix m = MatrixUtils.createRealMatrix(data); RealMatrix s = checkAEqualPTPt(m); checkSchurForm(s); } }"
                    ],
                    [
                        0.9608557224273682,
                        "@Test public void testRosen() { double[] startPoint = point(DIM, 0.1); double[][] boundaries = null; PointValuePair expected = new PointValuePair(point(DIM, 1.0), 0.0); doTest(new Rosen(), startPoint, boundaries, GoalType.MINIMIZE, 1e-13, 1e-6, 2000, expected); }"
                    ],
                    [
                        0.9608557224273682,
                        "@Test public void testRosen() { double[] startPoint = point(DIM, 0.1); double[][] boundaries = null; PointValuePair expected = new PointValuePair(point(DIM, 1.0), 0.0); doTest(new Rosen(), startPoint, boundaries, GoalType.MINIMIZE, 1e-13, 1e-6, 2000, expected); }"
                    ],
                    [
                        0.9601073265075684,
                        "@Test public void testNoReset() throws MaxCountExceededException { double[] y = { 0.0, 1.0, -2.0 }; DummyStepInterpolator interpolator = new DummyStepInterpolator(y, new double[y.length], true); interpolator.storeTime(0); interpolator.shift(); interpolator.storeTime(1); double[] result = interpolator.getInterpolatedState(); for (int i = 0; i < result.length; ++i) { Assert.assertTrue(FastMath.abs(result[i] - y[i]) < 1.0e-10); } }"
                    ],
                    [
                        0.9596816301345825,
                        "@Test(expected = TooManyEvaluationsException.class) public void testMaxIterations() { Powell powell = new Powell(); SimplexOptimizer optimizer = new SimplexOptimizer(-1, 1e-3); optimizer.setSimplex(new NelderMeadSimplex(4)); optimizer.optimize(20, powell, GoalType.MINIMIZE, new double[] { 3, -1, 0, 1 }); }"
                    ]
                ],
                "directly_related_tests": [
                    "org.apache.commons.math3.genetics.FixedElapsedTimeTest::testIsSatisfied"
                ],
                "generation": {
                    "1": {
                        "prompt": "You are an expert Java developer. Below is a buggy method from a large Java project. This method causes one or more test failures.\nYour task is to fix the bug in this method. Use the provided test failure(s) to guide your reasoning.\n\n---\n\n## Buggy Method\n\n```java\n    /**\n     * @param checker Convergence checker.\n     */\n\n    protected BaseOptimizer(ConvergenceChecker<PAIR> checker) {\n        this.checker = checker;\n\n        evaluations = new Incrementor(0, new MaxEvalCallback());\n        iterations = new Incrementor(0, new MaxIterCallback());\n    }\n\n```\n---\n\n## Failing Test Case(s)\n\n\n---\n\n## Your Goal\n\nFix the buggy method. Return only the fixed Java method. Do not include explanations or other text.\n",
                        "patches": [
                            "\n    /**\n     * @param checker Convergence checker.\n     */\n\n    protected BaseOptimizer(ConvergenceChecker<PAIR> checker) {\n        this.checker = checker;\n\n        evaluations = new Incrementor(0, Integer.MAX_VALUE, new MaxEvalCallback());\n        iterations = new Incrementor(0, Integer.MAX_VALUE, new MaxIterCallback());\n    }\n",
                            "\n    /**\n     * @param checker Convergence checker.\n     */\n\n    protected BaseOptimizer(ConvergenceChecker<PAIR> checker) {\n        this.checker = checker;\n\n        // Assuming there should be a default non-zero value for max evaluations and iterations.\n        evaluations = new Incrementor(Integer.MAX_VALUE, new MaxEvalCallback());\n        iterations = new Incrementor(Integer.MAX_VALUE, new MaxIterCallback());\n    }\n",
                            "\n    /**\n     * @param checker Convergence checker.\n     * @param maxEvaluations Maximum number of function evaluations.\n     * @param maxIterations Maximum number of iterations.\n     */\n    protected BaseOptimizer(ConvergenceChecker<PAIR> checker, int maxEvaluations, int maxIterations) {\n        this.checker = checker;\n        \n        evaluations = new Incrementor(maxEvaluations, new MaxEvalCallback());\n        iterations = new Incrementor(maxIterations, new MaxIterCallback());\n    }\n",
                            "\n/**\n * @param checker Convergence checker.\n * @param maxEvaluations Maximum number of evaluations.\n * @param maxIterations Maximum number of iterations.\n */\nprotected BaseOptimizer(ConvergenceChecker<PAIR> checker, int maxEvaluations, int maxIterations) {\n    this.checker = checker;\n\n    evaluations = new Incrementor(maxEvaluations, new MaxEvalCallback());\n    iterations = new Incrementor(maxIterations, new MaxIterCallback());\n}\n",
                            "\n    /**\n     * @param checker Convergence checker.\n     */\n\n    protected BaseOptimizer(ConvergenceChecker<PAIR> checker) {\n        this.checker = checker;\n\n        // Initialize evaluations and iterations with potentially meaningful default limits\n        evaluations = new Incrementor(Integer.MAX_VALUE, new MaxEvalCallback());\n        iterations = new Incrementor(Integer.MAX_VALUE, new MaxIterCallback());\n    }\n",
                            "\n    /**\n     * @param checker Convergence checker.\n     */\n\n    protected BaseOptimizer(ConvergenceChecker<PAIR> checker) {\n        this.checker = checker;\n\n        evaluations = new Incrementor(0, Integer.MAX_VALUE, new MaxEvalCallback());\n        iterations = new Incrementor(0, Integer.MAX_VALUE, new MaxIterCallback());\n    }\n",
                            "\n    /**\n     * @param checker Convergence checker.\n     */\n\n    protected BaseOptimizer(ConvergenceChecker<PAIR> checker) {\n        this.checker = checker;\n\n        evaluations = new Incrementor(Integer.MAX_VALUE, new MaxEvalCallback());\n        iterations = new Incrementor(Integer.MAX_VALUE, new MaxIterCallback());\n    }\n",
                            "\n    /**\n     * @param checker Convergence checker.\n     */\n\n    protected BaseOptimizer(ConvergenceChecker<PAIR> checker) {\n        this.checker = checker;\n\n        evaluations = new Incrementor(0, Integer.MAX_VALUE, new MaxEvalCallback());\n        iterations = new Incrementor(0, Integer.MAX_VALUE, new MaxIterCallback());\n    }\n",
                            "\n    /**\n     * @param checker Convergence checker.\n     * @param maxEvaluations Maximum number of evaluations.\n     * @param maxIterations Maximum number of iterations.\n     */\n    protected BaseOptimizer(ConvergenceChecker<PAIR> checker, int maxEvaluations, int maxIterations) {\n        this.checker = checker;\n\n        evaluations = new Incrementor(maxEvaluations, new MaxEvalCallback());\n        iterations = new Incrementor(maxIterations, new MaxIterCallback());\n    }\n",
                            "\n    /**\n     * @param checker Convergence checker.\n     */\n\n    protected BaseOptimizer(ConvergenceChecker<PAIR> checker) {\n        this.checker = checker;\n\n        evaluations = new Incrementor(Integer.MAX_VALUE, new MaxEvalCallback());\n        iterations = new Incrementor(Integer.MAX_VALUE, new MaxIterCallback());\n    }\n"
                        ],
                        "plausible_patches": [
                            "\n    /**\n     * @param checker Convergence checker.\n     */\n\n    protected BaseOptimizer(ConvergenceChecker<PAIR> checker) {\n        this.checker = checker;\n\n        // Assuming there should be a default non-zero value for max evaluations and iterations.\n        evaluations = new Incrementor(Integer.MAX_VALUE, new MaxEvalCallback());\n        iterations = new Incrementor(Integer.MAX_VALUE, new MaxIterCallback());\n    }\n",
                            "\n    /**\n     * @param checker Convergence checker.\n     * @param maxEvaluations Maximum number of function evaluations.\n     * @param maxIterations Maximum number of iterations.\n     */\n    protected BaseOptimizer(ConvergenceChecker<PAIR> checker, int maxEvaluations, int maxIterations) {\n        this.checker = checker;\n        \n        evaluations = new Incrementor(maxEvaluations, new MaxEvalCallback());\n        iterations = new Incrementor(maxIterations, new MaxIterCallback());\n    }\n"
                        ]
                    }
                }
            },
            {
                "path": "src/main/java/org/apache/commons/math3/optim/nonlinear/scalar/gradient/NonLinearConjugateGradientOptimizer.java",
                "start_loc": 191,
                "end_loc": 288,
                "buggy_function": "    @Override\n    protected PointValuePair doOptimize() {\n        final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n        final double[] point = getStartPoint();\n        final GoalType goal = getGoalType();\n        final int n = point.length;\n        double[] r = computeObjectiveGradient(point);\n        if (goal == GoalType.MINIMIZE) {\n            for (int i = 0; i < n; i++) {\n                r[i] = -r[i];\n            }\n        }\n\n        // Initial search direction.\n        double[] steepestDescent = preconditioner.precondition(point, r);\n        double[] searchDirection = steepestDescent.clone();\n\n        double delta = 0;\n        for (int i = 0; i < n; ++i) {\n            delta += r[i] * searchDirection[i];\n        }\n\n        PointValuePair current = null;\n        int iter = 0;\n        int maxEval = getMaxEvaluations();\n        while (true) {\n            ++iter;\n\n            final double objective = computeObjectiveValue(point);\n            PointValuePair previous = current;\n            current = new PointValuePair(point, objective);\n            if (previous != null) {\n                if (checker.converged(iter, previous, current)) {\n                    // We have found an optimum.\n                    return current;\n                }\n            }\n\n            // Find the optimal step in the search direction.\n            final UnivariateFunction lsf = new LineSearchFunction(point, searchDirection);\n            final double uB = findUpperBound(lsf, 0, initialStep);\n            // XXX Last parameters is set to a value close to zero in order to\n            // work around the divergence problem in the \"testCircleFitting\"\n            // unit test (see MATH-439).\n            final double step = solver.solve(maxEval, lsf, 0, uB, 1e-15);\n            maxEval -= solver.getEvaluations(); // Subtract used up evaluations.\n\n            // Validate new point.\n            for (int i = 0; i < point.length; ++i) {\n                point[i] += step * searchDirection[i];\n            }\n\n            r = computeObjectiveGradient(point);\n            if (goal == GoalType.MINIMIZE) {\n                for (int i = 0; i < n; ++i) {\n                    r[i] = -r[i];\n                }\n            }\n\n            // Compute beta.\n            final double deltaOld = delta;\n            final double[] newSteepestDescent = preconditioner.precondition(point, r);\n            delta = 0;\n            for (int i = 0; i < n; ++i) {\n                delta += r[i] * newSteepestDescent[i];\n            }\n\n            final double beta;\n            switch (updateFormula) {\n            case FLETCHER_REEVES:\n                beta = delta / deltaOld;\n                break;\n            case POLAK_RIBIERE:\n                double deltaMid = 0;\n                for (int i = 0; i < r.length; ++i) {\n                    deltaMid += r[i] * steepestDescent[i];\n                }\n                beta = (delta - deltaMid) / deltaOld;\n                break;\n            default:\n                // Should never happen.\n                throw new MathInternalError();\n            }\n            steepestDescent = newSteepestDescent;\n\n            // Compute conjugate search direction.\n            if (iter % n == 0 ||\n                beta < 0) {\n                // Break conjugation: reset search direction.\n                searchDirection = steepestDescent.clone();\n            } else {\n                // Compute new conjugate search direction.\n                for (int i = 0; i < n; ++i) {\n                    searchDirection[i] = steepestDescent[i] + beta * searchDirection[i];\n                }\n            }\n        }\n    }\n",
                "fixed_function": "    @Override\n    protected PointValuePair doOptimize() {\n        final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n        final double[] point = getStartPoint();\n        final GoalType goal = getGoalType();\n        final int n = point.length;\n        double[] r = computeObjectiveGradient(point);\n        if (goal == GoalType.MINIMIZE) {\n            for (int i = 0; i < n; i++) {\n                r[i] = -r[i];\n            }\n        }\n\n        // Initial search direction.\n        double[] steepestDescent = preconditioner.precondition(point, r);\n        double[] searchDirection = steepestDescent.clone();\n\n        double delta = 0;\n        for (int i = 0; i < n; ++i) {\n            delta += r[i] * searchDirection[i];\n        }\n\n        PointValuePair current = null;\n        int maxEval = getMaxEvaluations();\n        while (true) {\n            incrementIterationCount();\n\n            final double objective = computeObjectiveValue(point);\n            PointValuePair previous = current;\n            current = new PointValuePair(point, objective);\n            if (previous != null) {\n                if (checker.converged(getIterations(), previous, current)) {\n                    // We have found an optimum.\n                    return current;\n                }\n            }\n\n            // Find the optimal step in the search direction.\n            final UnivariateFunction lsf = new LineSearchFunction(point, searchDirection);\n            final double uB = findUpperBound(lsf, 0, initialStep);\n            // XXX Last parameters is set to a value close to zero in order to\n            // work around the divergence problem in the \"testCircleFitting\"\n            // unit test (see MATH-439).\n            final double step = solver.solve(maxEval, lsf, 0, uB, 1e-15);\n            maxEval -= solver.getEvaluations(); // Subtract used up evaluations.\n\n            // Validate new point.\n            for (int i = 0; i < point.length; ++i) {\n                point[i] += step * searchDirection[i];\n            }\n\n            r = computeObjectiveGradient(point);\n            if (goal == GoalType.MINIMIZE) {\n                for (int i = 0; i < n; ++i) {\n                    r[i] = -r[i];\n                }\n            }\n\n            // Compute beta.\n            final double deltaOld = delta;\n            final double[] newSteepestDescent = preconditioner.precondition(point, r);\n            delta = 0;\n            for (int i = 0; i < n; ++i) {\n                delta += r[i] * newSteepestDescent[i];\n            }\n\n            final double beta;\n            switch (updateFormula) {\n            case FLETCHER_REEVES:\n                beta = delta / deltaOld;\n                break;\n            case POLAK_RIBIERE:\n                double deltaMid = 0;\n                for (int i = 0; i < r.length; ++i) {\n                    deltaMid += r[i] * steepestDescent[i];\n                }\n                beta = (delta - deltaMid) / deltaOld;\n                break;\n            default:\n                // Should never happen.\n                throw new MathInternalError();\n            }\n            steepestDescent = newSteepestDescent;\n\n            // Compute conjugate search direction.\n            if (getIterations() % n == 0 ||\n                beta < 0) {\n                // Break conjugation: reset search direction.\n                searchDirection = steepestDescent.clone();\n            } else {\n                // Compute new conjugate search direction.\n                for (int i = 0; i < n; ++i) {\n                    searchDirection[i] = steepestDescent[i] + beta * searchDirection[i];\n                }\n            }\n        }\n    }\n",
                "comment": "    /** {@inheritDoc} */\n",
                "normalized_body": [
                    "@Override protected PointValuePair doOptimize() { final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker(); final double[] point = getStartPoint(); final GoalType goal = getGoalType(); final int n = point.length; double[] r = computeObjectiveGradient(point); if (goal == GoalType.MINIMIZE) { for (int i = 0; i < n; i++) { r[i] = -r[i]; } } double[] steepestDescent = preconditioner.precondition(point, r); double[] searchDirection = steepestDescent.clone(); double delta = 0; for (int i = 0; i < n; ++i) { delta += r[i] * searchDirection[i]; } PointValuePair current = null; int iter = 0; int maxEval = getMaxEvaluations(); while (true) { ++iter; final double objective = computeObjectiveValue(point); PointValuePair previous = current; current = new PointValuePair(point, objective); if (previous != null) { if (checker.converged(iter, previous, current)) { return current; } } final UnivariateFunction lsf = new LineSearchFunction(point, searchDirection); final double uB = findUpperBound(lsf, 0, initialStep); final double step = solver.solve(maxEval, lsf, 0, uB, 1e-15); maxEval -= solver.getEvaluations(); for (int i = 0; i < point.length; ++i) { point[i] += step * searchDirection[i]; } r = computeObjectiveGradient(point); if (goal == GoalType.MINIMIZE) { for (int i = 0; i < n; ++i) { r[i] = -r[i]; } } final double deltaOld = delta; final double[] newSteepestDescent = preconditioner.precondition(point, r); delta = 0; for (int i = 0; i < n; ++i) { delta += r[i] * newSteepestDescent[i]; } final double beta; switch(updateFormula) { case FLETCHER_REEVES: beta = delta / deltaOld; break; case POLAK_RIBIERE: double deltaMid = 0; for (int i = 0; i < r.length; ++i) { deltaMid += r[i] * steepestDescent[i]; } beta = (delta - deltaMid) / deltaOld; break; default: throw new MathInternalError(); } steepestDescent = newSteepestDescent; if (iter % n == 0 || beta < 0) { searchDirection = steepestDescent.clone(); } else { for (int i = 0; i < n; ++i) { searchDirection[i] = steepestDescent[i] + beta * searchDirection[i]; } } } }"
                ],
                "top_similar_methods": [
                    [
                        1.0000001788139343,
                        "@Override protected PointValuePair doOptimize() { final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker(); final double[] point = getStartPoint(); final GoalType goal = getGoalType(); final int n = point.length; double[] r = computeObjectiveGradient(point); if (goal == GoalType.MINIMIZE) { for (int i = 0; i < n; i++) { r[i] = -r[i]; } } double[] steepestDescent = preconditioner.precondition(point, r); double[] searchDirection = steepestDescent.clone(); double delta = 0; for (int i = 0; i < n; ++i) { delta += r[i] * searchDirection[i]; } PointValuePair current = null; int iter = 0; int maxEval = getMaxEvaluations(); while (true) { ++iter; final double objective = computeObjectiveValue(point); PointValuePair previous = current; current = new PointValuePair(point, objective); if (previous != null) { if (checker.converged(iter, previous, current)) { return current; } } final UnivariateFunction lsf = new LineSearchFunction(point, searchDirection); final double uB = findUpperBound(lsf, 0, initialStep); final double step = solver.solve(maxEval, lsf, 0, uB, 1e-15); maxEval -= solver.getEvaluations(); for (int i = 0; i < point.length; ++i) { point[i] += step * searchDirection[i]; } r = computeObjectiveGradient(point); if (goal == GoalType.MINIMIZE) { for (int i = 0; i < n; ++i) { r[i] = -r[i]; } } final double deltaOld = delta; final double[] newSteepestDescent = preconditioner.precondition(point, r); delta = 0; for (int i = 0; i < n; ++i) { delta += r[i] * newSteepestDescent[i]; } final double beta; switch(updateFormula) { case FLETCHER_REEVES: beta = delta / deltaOld; break; case POLAK_RIBIERE: double deltaMid = 0; for (int i = 0; i < r.length; ++i) { deltaMid += r[i] * steepestDescent[i]; } beta = (delta - deltaMid) / deltaOld; break; default: throw new MathInternalError(); } steepestDescent = newSteepestDescent; if (iter % n == 0 || beta < 0) { searchDirection = steepestDescent.clone(); } else { for (int i = 0; i < n; ++i) { searchDirection[i] = steepestDescent[i] + beta * searchDirection[i]; } } } }"
                    ],
                    [
                        0.9892449378967285,
                        "@Override protected PointValuePair doOptimize() { final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker(); point = getStartPoint(); final GoalType goal = getGoalType(); final int n = point.length; double[] r = computeObjectiveGradient(point); if (goal == GoalType.MINIMIZE) { for (int i = 0; i < n; ++i) { r[i] = -r[i]; } } double[] steepestDescent = preconditioner.precondition(point, r); double[] searchDirection = steepestDescent.clone(); double delta = 0; for (int i = 0; i < n; ++i) { delta += r[i] * searchDirection[i]; } PointValuePair current = null; int iter = 0; int maxEval = getMaxEvaluations(); while (true) { ++iter; final double objective = computeObjectiveValue(point); PointValuePair previous = current; current = new PointValuePair(point, objective); if (previous != null) { if (checker.converged(iter, previous, current)) { return current; } } final UnivariateFunction lsf = new LineSearchFunction(searchDirection); final double uB = findUpperBound(lsf, 0, initialStep); final double step = solver.solve(maxEval, lsf, 0, uB, 1e-15); maxEval -= solver.getEvaluations(); for (int i = 0; i < point.length; ++i) { point[i] += step * searchDirection[i]; } r = computeObjectiveGradient(point); if (goal == GoalType.MINIMIZE) { for (int i = 0; i < n; ++i) { r[i] = -r[i]; } } final double deltaOld = delta; final double[] newSteepestDescent = preconditioner.precondition(point, r); delta = 0; for (int i = 0; i < n; ++i) { delta += r[i] * newSteepestDescent[i]; } final double beta; if (updateFormula == ConjugateGradientFormula.FLETCHER_REEVES) { beta = delta / deltaOld; } else { double deltaMid = 0; for (int i = 0; i < r.length; ++i) { deltaMid += r[i] * steepestDescent[i]; } beta = (delta - deltaMid) / deltaOld; } steepestDescent = newSteepestDescent; if (iter % n == 0 || beta < 0) { searchDirection = steepestDescent.clone(); } else { for (int i = 0; i < n; ++i) { searchDirection[i] = steepestDescent[i] + beta * searchDirection[i]; } } } }"
                    ],
                    [
                        0.9818988740444183,
                        "@Override protected PointValuePair doOptimize() { checkParameters(); final GoalType goal = getGoalType(); final double[] guess = getStartPoint(); final int n = guess.length; final double[][] direc = new double[n][n]; for (int i = 0; i < n; i++) { direc[i][i] = 1; } final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker(); double[] x = guess; double fVal = computeObjectiveValue(x); double[] x1 = x.clone(); int iter = 0; while (true) { ++iter; double fX = fVal; double fX2 = 0; double delta = 0; int bigInd = 0; double alphaMin = 0; for (int i = 0; i < n; i++) { final double[] d = MathArrays.copyOf(direc[i]); fX2 = fVal; final UnivariatePointValuePair optimum = line.search(x, d); fVal = optimum.getValue(); alphaMin = optimum.getPoint(); final double[][] result = newPointAndDirection(x, d, alphaMin); x = result[0]; if ((fX2 - fVal) > delta) { delta = fX2 - fVal; bigInd = i; } } boolean stop = 2 * (fX - fVal) <= (relativeThreshold * (FastMath.abs(fX) + FastMath.abs(fVal)) + absoluteThreshold); final PointValuePair previous = new PointValuePair(x1, fX); final PointValuePair current = new PointValuePair(x, fVal); if (!stop) { if (checker != null) { stop = checker.converged(iter, previous, current); } } if (stop) { if (goal == GoalType.MINIMIZE) { return (fVal < fX) ? current : previous; } else { return (fVal > fX) ? current : previous; } } final double[] d = new double[n]; final double[] x2 = new double[n]; for (int i = 0; i < n; i++) { d[i] = x[i] - x1[i]; x2[i] = 2 * x[i] - x1[i]; } x1 = x.clone(); fX2 = computeObjectiveValue(x2); if (fX > fX2) { double t = 2 * (fX + fX2 - 2 * fVal); double temp = fX - fVal - delta; t *= temp * temp; temp = fX - fX2; t -= delta * temp * temp; if (t < 0.0) { final UnivariatePointValuePair optimum = line.search(x, d); fVal = optimum.getValue(); alphaMin = optimum.getPoint(); final double[][] result = newPointAndDirection(x, d, alphaMin); x = result[0]; final int lastInd = n - 1; direc[bigInd] = direc[lastInd]; direc[lastInd] = result[1]; } } } }"
                    ],
                    [
                        0.9817616641521454,
                        "@Override protected PointValuePair doOptimize() { final GoalType goal = getGoalType(); final double[] guess = getStartPoint(); final int n = guess.length; final double[][] direc = new double[n][n]; for (int i = 0; i < n; i++) { direc[i][i] = 1; } final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker(); double[] x = guess; double fVal = computeObjectiveValue(x); double[] x1 = x.clone(); int iter = 0; while (true) { ++iter; double fX = fVal; double fX2 = 0; double delta = 0; int bigInd = 0; double alphaMin = 0; for (int i = 0; i < n; i++) { final double[] d = MathArrays.copyOf(direc[i]); fX2 = fVal; final UnivariatePointValuePair optimum = line.search(x, d); fVal = optimum.getValue(); alphaMin = optimum.getPoint(); final double[][] result = newPointAndDirection(x, d, alphaMin); x = result[0]; if ((fX2 - fVal) > delta) { delta = fX2 - fVal; bigInd = i; } } boolean stop = 2 * (fX - fVal) <= (relativeThreshold * (FastMath.abs(fX) + FastMath.abs(fVal)) + absoluteThreshold); final PointValuePair previous = new PointValuePair(x1, fX); final PointValuePair current = new PointValuePair(x, fVal); if (!stop) { if (checker != null) { stop = checker.converged(iter, previous, current); } } if (stop) { if (goal == GoalType.MINIMIZE) { return (fVal < fX) ? current : previous; } else { return (fVal > fX) ? current : previous; } } final double[] d = new double[n]; final double[] x2 = new double[n]; for (int i = 0; i < n; i++) { d[i] = x[i] - x1[i]; x2[i] = 2 * x[i] - x1[i]; } x1 = x.clone(); fX2 = computeObjectiveValue(x2); if (fX > fX2) { double t = 2 * (fX + fX2 - 2 * fVal); double temp = fX - fVal - delta; t *= temp * temp; temp = fX - fX2; t -= delta * temp * temp; if (t < 0.0) { final UnivariatePointValuePair optimum = line.search(x, d); fVal = optimum.getValue(); alphaMin = optimum.getPoint(); final double[][] result = newPointAndDirection(x, d, alphaMin); x = result[0]; final int lastInd = n - 1; direc[bigInd] = direc[lastInd]; direc[lastInd] = result[1]; } } } }"
                    ],
                    [
                        0.9811828136444092,
                        "@Override public void integrate(final ExpandableStatefulODE equations, final double t) throws NumberIsTooSmallException, DimensionMismatchException, MaxCountExceededException, NoBracketingException { sanityChecks(equations, t); setEquations(equations); final boolean forward = t > equations.getTime(); final double[] y0 = equations.getCompleteState(); final double[] y = y0.clone(); final int stages = c.length + 1; final double[][] yDotK = new double[stages][y.length]; final double[] yTmp = y0.clone(); final double[] yDotTmp = new double[y.length]; final RungeKuttaStepInterpolator interpolator = (RungeKuttaStepInterpolator) prototype.copy(); interpolator.reinitialize(this, yTmp, yDotK, forward, equations.getPrimaryMapper(), equations.getSecondaryMappers()); interpolator.storeTime(equations.getTime()); stepStart = equations.getTime(); double hNew = 0; boolean firstTime = true; initIntegration(equations.getTime(), y0, t); isLastStep = false; do { interpolator.shift(); double error = 10; while (error >= 1.0) { if (firstTime || !fsal) { computeDerivatives(stepStart, y, yDotK[0]); } if (firstTime) { final double[] scale = new double[mainSetDimension]; if (vecAbsoluteTolerance == null) { for (int i = 0; i < scale.length; ++i) { scale[i] = scalAbsoluteTolerance + scalRelativeTolerance * FastMath.abs(y[i]); } } else { for (int i = 0; i < scale.length; ++i) { scale[i] = vecAbsoluteTolerance[i] + vecRelativeTolerance[i] * FastMath.abs(y[i]); } } hNew = initializeStep(forward, getOrder(), scale, stepStart, y, yDotK[0], yTmp, yDotK[1]); firstTime = false; } stepSize = hNew; if (forward) { if (stepStart + stepSize >= t) { stepSize = t - stepStart; } } else { if (stepStart + stepSize <= t) { stepSize = t - stepStart; } } for (int k = 1; k < stages; ++k) { for (int j = 0; j < y0.length; ++j) { double sum = a[k - 1][0] * yDotK[0][j]; for (int l = 1; l < k; ++l) { sum += a[k - 1][l] * yDotK[l][j]; } yTmp[j] = y[j] + stepSize * sum; } computeDerivatives(stepStart + c[k - 1] * stepSize, yTmp, yDotK[k]); } for (int j = 0; j < y0.length; ++j) { double sum = b[0] * yDotK[0][j]; for (int l = 1; l < stages; ++l) { sum += b[l] * yDotK[l][j]; } yTmp[j] = y[j] + stepSize * sum; } error = estimateError(yDotK, y, yTmp, stepSize); if (error >= 1.0) { final double factor = FastMath.min(maxGrowth, FastMath.max(minReduction, safety * FastMath.pow(error, exp))); hNew = filterStep(stepSize * factor, forward, false); } } interpolator.storeTime(stepStart + stepSize); System.arraycopy(yTmp, 0, y, 0, y0.length); System.arraycopy(yDotK[stages - 1], 0, yDotTmp, 0, y0.length); stepStart = acceptStep(interpolator, y, yDotTmp, t); System.arraycopy(y, 0, yTmp, 0, y.length); if (!isLastStep) { interpolator.storeTime(stepStart); if (fsal) { System.arraycopy(yDotTmp, 0, yDotK[0], 0, y0.length); } final double factor = FastMath.min(maxGrowth, FastMath.max(minReduction, safety * FastMath.pow(error, exp))); final double scaledH = stepSize * factor; final double nextT = stepStart + scaledH; final boolean nextIsLast = forward ? (nextT >= t) : (nextT <= t); hNew = filterStep(scaledH, forward, nextIsLast); final double filteredNextT = stepStart + hNew; final boolean filteredNextIsLast = forward ? (filteredNextT >= t) : (filteredNextT <= t); if (filteredNextIsLast) { hNew = t - stepStart; } } } while (!isLastStep); equations.setTime(stepStart); equations.setCompleteState(y); resetInternalState(); }"
                    ],
                    [
                        0.9806041419506073,
                        "@Override public void integrate(final ExpandableStatefulODE equations, final double t) throws NumberIsTooSmallException, DimensionMismatchException, MaxCountExceededException, NoBracketingException { sanityChecks(equations, t); setEquations(equations); final boolean forward = t > equations.getTime(); final double[] y0 = equations.getCompleteState(); final double[] y = y0.clone(); final int stages = c.length + 1; final double[][] yDotK = new double[stages][]; for (int i = 0; i < stages; ++i) { yDotK[i] = new double[y0.length]; } final double[] yTmp = y0.clone(); final double[] yDotTmp = new double[y0.length]; final RungeKuttaStepInterpolator interpolator = (RungeKuttaStepInterpolator) prototype.copy(); interpolator.reinitialize(this, yTmp, yDotK, forward, equations.getPrimaryMapper(), equations.getSecondaryMappers()); interpolator.storeTime(equations.getTime()); stepStart = equations.getTime(); stepSize = forward ? step : -step; initIntegration(equations.getTime(), y0, t); isLastStep = false; do { interpolator.shift(); computeDerivatives(stepStart, y, yDotK[0]); for (int k = 1; k < stages; ++k) { for (int j = 0; j < y0.length; ++j) { double sum = a[k - 1][0] * yDotK[0][j]; for (int l = 1; l < k; ++l) { sum += a[k - 1][l] * yDotK[l][j]; } yTmp[j] = y[j] + stepSize * sum; } computeDerivatives(stepStart + c[k - 1] * stepSize, yTmp, yDotK[k]); } for (int j = 0; j < y0.length; ++j) { double sum = b[0] * yDotK[0][j]; for (int l = 1; l < stages; ++l) { sum += b[l] * yDotK[l][j]; } yTmp[j] = y[j] + stepSize * sum; } interpolator.storeTime(stepStart + stepSize); System.arraycopy(yTmp, 0, y, 0, y0.length); System.arraycopy(yDotK[stages - 1], 0, yDotTmp, 0, y0.length); stepStart = acceptStep(interpolator, y, yDotTmp, t); if (!isLastStep) { interpolator.storeTime(stepStart); final double nextT = stepStart + stepSize; final boolean nextIsLast = forward ? (nextT >= t) : (nextT <= t); if (nextIsLast) { stepSize = t - stepStart; } } } while (!isLastStep); equations.setTime(stepStart); equations.setCompleteState(y); stepStart = Double.NaN; stepSize = Double.NaN; }"
                    ],
                    [
                        0.980412095785141,
                        "@Override protected double doIntegrate() throws TooManyEvaluationsException, MaxCountExceededException { final int m = iterations.getMaximalCount() + 1; double[] previousRow = new double[m]; double[] currentRow = new double[m]; TrapezoidIntegrator qtrap = new TrapezoidIntegrator(); currentRow[0] = qtrap.stage(this, 0); iterations.incrementCount(); double olds = currentRow[0]; while (true) { final int i = iterations.getCount(); final double[] tmpRow = previousRow; previousRow = currentRow; currentRow = tmpRow; currentRow[0] = qtrap.stage(this, i); iterations.incrementCount(); for (int j = 1; j <= i; j++) { final double r = (1L << (2 * j)) - 1; final double tIJm1 = currentRow[j - 1]; currentRow[j] = tIJm1 + (tIJm1 - previousRow[j - 1]) / r; } final double s = currentRow[i]; if (i >= getMinimalIterationCount()) { final double delta = FastMath.abs(s - olds); final double rLimit = getRelativeAccuracy() * (FastMath.abs(olds) + FastMath.abs(s)) * 0.5; if ((delta <= rLimit) || (delta <= getAbsoluteAccuracy())) { return s; } } olds = s; } }"
                    ],
                    [
                        0.9801218509674072,
                        "@Test public void testParaboloid2D() { MultivariateFunction f = new MultivariateFunction() { public double value(double[] x) { if (x.length != 2) { throw new IllegalArgumentException(); } return 2 * x[0] * x[0] - 3 * x[1] * x[1] + 4 * x[0] * x[1] - 5; } }; MultivariateInterpolator interpolator = new MicrosphereInterpolator(); final int n = 121; final int dim = 2; double[][] x = new double[n][dim]; double[] y = new double[n]; int index = 0; for (int i = -10; i <= 10; i += 2) { for (int j = -10; j <= 10; j += 2) { x[index][0] = i; x[index][1] = j; y[index] = f.value(x[index]); ++index; } } MultivariateFunction p = interpolator.interpolate(x, y); double[] c = new double[dim]; double expected, result; c[0] = 0; c[1] = 0; expected = f.value(c); result = p.value(c); Assert.assertEquals(\\\"On sample point\\\", expected, result, FastMath.ulp(1d)); c[0] = 2 + 1e-5; c[1] = 2 - 1e-5; expected = f.value(c); result = p.value(c); Assert.assertEquals(\\\"1e-5 away from sample point\\\", expected, result, 1e-3); }"
                    ],
                    [
                        0.9796776175498962,
                        "private DerivativeStructure evaluate(final DerivativeStructure t, final double t0, final double[] y) throws NumberIsTooLargeException { final double[] top = new double[nbPoints]; final double[] bottom = new double[nbPoints]; for (int i = 0; i < nbPoints; ++i) { bottom[i] = y[i]; for (int j = 1; j <= i; ++j) { bottom[i - j] = (bottom[i - j + 1] - bottom[i - j]) / (j * stepSize); } top[i] = bottom[0]; } final int order = t.getOrder(); final int parameters = t.getFreeParameters(); final double[] derivatives = t.getAllDerivatives(); final double dt0 = t.getValue() - t0; DerivativeStructure interpolation = new DerivativeStructure(parameters, order, 0.0); DerivativeStructure monomial = null; for (int i = 0; i < nbPoints; ++i) { if (i == 0) { monomial = new DerivativeStructure(parameters, order, 1.0); } else { derivatives[0] = dt0 - (i - 1) * stepSize; final DerivativeStructure deltaX = new DerivativeStructure(parameters, order, derivatives); monomial = monomial.multiply(deltaX); } interpolation = interpolation.add(monomial.multiply(top[i])); } return interpolation; }"
                    ],
                    [
                        0.9795789122581482,
                        "public static double[][][] timesAndResults(int repeatChunk, int repeatStat, boolean runGC, Callable<Double>... methods) { final int numMethods = methods.length; final double[][][] timesAndResults = new double[numMethods][repeatStat][2]; try { for (int k = 0; k < repeatStat; k++) { for (int j = 0; j < numMethods; j++) { if (runGC) { System.gc(); } final Callable<Double> r = methods[j]; final double[] result = new double[repeatChunk]; final long start = System.nanoTime(); for (int i = 0; i < repeatChunk; i++) { result[i] = r.call(); } final long stop = System.nanoTime(); timesAndResults[j][k][0] = (stop - start) * NANO_TO_MILLI; timesAndResults[j][k][1] = result[rng.nextInt(repeatChunk)]; } } } catch (Exception e) { throw new MathIllegalStateException(LocalizedFormats.SIMPLE_MESSAGE, e.getMessage()); } final double normFactor = 1d / repeatChunk; for (int j = 0; j < numMethods; j++) { for (int k = 0; k < repeatStat; k++) { timesAndResults[j][k][0] *= normFactor; } } return timesAndResults; }"
                    ],
                    [
                        0.9795418977737427,
                        "private void doTestCombine(final boolean inPlace, final boolean mixed) { final int n = values.length * values.length; final double[] data1 = new double[n]; final double[] data2 = new double[n]; for (int i = 0; i < values.length; i++) { for (int j = 0; j < values.length; j++) { final int index = values.length * i + j; data1[index] = values[i]; data2[index] = values[j]; } } final RealVector v1 = create(data1); final RealVector v2 = mixed ? createAlien(data2) : create(data2); final double[] expected = new double[n]; for (int i = 0; i < values.length; i++) { final double a1 = values[i]; for (int j = 0; j < values.length; j++) { final double a2 = values[j]; for (int k = 0; k < n; k++) { expected[k] = a1 * data1[k] + a2 * data2[k]; } final RealVector actual; if (inPlace) { final RealVector v1bis = v1.copy(); actual = v1bis.combineToSelf(a1, a2, v2); Assert.assertSame(v1bis, actual); } else { actual = v1.combine(a1, a2, v2); } TestUtils.assertEquals(\\\"a1 = \\\" + a1 + \\\", a2 = \\\" + a2, expected, actual, 0.); } } }"
                    ],
                    [
                        0.9794107675552368,
                        "private static void doTestTransformComplex(final int n, final double tol, final DftNormalization normalization, final TransformType type) { final FastFourierTransformer fft; fft = new FastFourierTransformer(normalization); final Complex[] x = createComplexData(n); final Complex[] expected; final double s; if (type == TransformType.FORWARD) { expected = dft(x, -1); if (normalization == DftNormalization.STANDARD) { s = 1.0; } else { s = 1.0 / FastMath.sqrt(n); } } else { expected = dft(x, 1); if (normalization == DftNormalization.STANDARD) { s = 1.0 / n; } else { s = 1.0 / FastMath.sqrt(n); } } final Complex[] actual = fft.transform(x, type); for (int i = 0; i < n; i++) { final String msg; msg = String.format(\\\"%s, %s, %d, %d\\\", normalization, type, n, i); final double re = s * expected[i].getReal(); Assert.assertEquals(msg, re, actual[i].getReal(), tol * FastMath.abs(re)); final double im = s * expected[i].getImaginary(); Assert.assertEquals(msg, im, actual[i].getImaginary(), tol * FastMath.abs(re)); } }"
                    ]
                ],
                "directly_related_tests": [
                    "org.apache.commons.math3.genetics.FixedElapsedTimeTest::testIsSatisfied"
                ],
                "generation": {
                    "1": {
                        "prompt": "You are an expert Java developer. Below is a buggy method from a large Java project. This method causes one or more test failures.\nYour task is to fix the bug in this method. Use the provided test failure(s) to guide your reasoning.\n\n---\n\n## Buggy Method\n\n```java\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n        final double[] point = getStartPoint();\n        final GoalType goal = getGoalType();\n        final int n = point.length;\n        double[] r = computeObjectiveGradient(point);\n        if (goal == GoalType.MINIMIZE) {\n            for (int i = 0; i < n; i++) {\n                r[i] = -r[i];\n            }\n        }\n\n        // Initial search direction.\n        double[] steepestDescent = preconditioner.precondition(point, r);\n        double[] searchDirection = steepestDescent.clone();\n\n        double delta = 0;\n        for (int i = 0; i < n; ++i) {\n            delta += r[i] * searchDirection[i];\n        }\n\n        PointValuePair current = null;\n        int iter = 0;\n        int maxEval = getMaxEvaluations();\n        while (true) {\n            ++iter;\n\n            final double objective = computeObjectiveValue(point);\n            PointValuePair previous = current;\n            current = new PointValuePair(point, objective);\n            if (previous != null) {\n                if (checker.converged(iter, previous, current)) {\n                    // We have found an optimum.\n                    return current;\n                }\n            }\n\n            // Find the optimal step in the search direction.\n            final UnivariateFunction lsf = new LineSearchFunction(point, searchDirection);\n            final double uB = findUpperBound(lsf, 0, initialStep);\n            // XXX Last parameters is set to a value close to zero in order to\n            // work around the divergence problem in the \"testCircleFitting\"\n            // unit test (see MATH-439).\n            final double step = solver.solve(maxEval, lsf, 0, uB, 1e-15);\n            maxEval -= solver.getEvaluations(); // Subtract used up evaluations.\n\n            // Validate new point.\n            for (int i = 0; i < point.length; ++i) {\n                point[i] += step * searchDirection[i];\n            }\n\n            r = computeObjectiveGradient(point);\n            if (goal == GoalType.MINIMIZE) {\n                for (int i = 0; i < n; ++i) {\n                    r[i] = -r[i];\n                }\n            }\n\n            // Compute beta.\n            final double deltaOld = delta;\n            final double[] newSteepestDescent = preconditioner.precondition(point, r);\n            delta = 0;\n            for (int i = 0; i < n; ++i) {\n                delta += r[i] * newSteepestDescent[i];\n            }\n\n            final double beta;\n            switch (updateFormula) {\n            case FLETCHER_REEVES:\n                beta = delta / deltaOld;\n                break;\n            case POLAK_RIBIERE:\n                double deltaMid = 0;\n                for (int i = 0; i < r.length; ++i) {\n                    deltaMid += r[i] * steepestDescent[i];\n                }\n                beta = (delta - deltaMid) / deltaOld;\n                break;\n            default:\n                // Should never happen.\n                throw new MathInternalError();\n            }\n            steepestDescent = newSteepestDescent;\n\n            // Compute conjugate search direction.\n            if (iter % n == 0 ||\n                beta < 0) {\n                // Break conjugation: reset search direction.\n                searchDirection = steepestDescent.clone();\n            } else {\n                // Compute new conjugate search direction.\n                for (int i = 0; i < n; ++i) {\n                    searchDirection[i] = steepestDescent[i] + beta * searchDirection[i];\n                }\n            }\n        }\n    }\n\n```\n---\n\n## Failing Test Case(s)\n\n\n---\n\n## Your Goal\n\nFix the buggy method. Return only the fixed Java method. Do not include explanations or other text.\n",
                        "patches": [
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n        final double[] point = getStartPoint();\n        final GoalType goal = getGoalType();\n        final int n = point.length;\n        double[] r = computeObjectiveGradient(point);\n        if (goal == GoalType.MINIMIZE) {\n            for (int i = 0; i < n; i++) {\n                r[i] = -r[i];\n            }\n        }\n\n        // Initial search direction.\n        double[] steepestDescent = preconditioner.precondition(point, r);\n        double[] searchDirection = steepestDescent.clone();\n\n        double delta = 0;\n        for (int i = 0; i < n; ++i) {\n            delta += r[i] * steepestDescent[i];\n        }\n\n        PointValuePair current = null;\n        int iter = 0;\n        int maxEval = getMaxEvaluations();\n        while (true) {\n            ++iter;\n\n            final double objective = computeObjectiveValue(point);\n            PointValuePair previous = current;\n            current = new PointValuePair(point, objective);\n            if (previous != null) {\n                if (checker.converged(iter, previous, current)) {\n                    // We have found an optimum.\n                    return current;\n                }\n            }\n\n            // Find the optimal step in the search direction.\n            final UnivariateFunction lsf = new LineSearchFunction(point, searchDirection);\n            final double uB = findUpperBound(lsf, 0, initialStep);\n            final double step = solver.solve(maxEval, lsf, 0, uB, 1e-15);\n            maxEval -= solver.getEvaluations(); // Subtract used up evaluations.\n\n            // Validate new point.\n            for (int i = 0; i < point.length; ++i) {\n                point[i] += step * searchDirection[i];\n            }\n\n            r = computeObjectiveGradient(point);\n            if (goal == GoalType.MINIMIZE) {\n                for (int i = 0; i < n; ++i) {\n                    r[i] = -r[i];\n                }\n            }\n\n            // Compute beta.\n            final double deltaOld = delta;\n            double deltaMid = 0;\n            for (int i = 0; i < n; ++i) {\n                deltaMid += r[i] * steepestDescent[i];\n            }\n            final double[] newSteepestDescent = preconditioner.precondition(point, r);\n            delta = 0;\n            for (int i = 0; i < n; ++i) {\n                delta += r[i] * newSteepestDescent[i];\n            }\n\n            final double beta;\n            switch (updateFormula) {\n            case FLETCHER_REEVES:\n                beta = delta / deltaOld;\n                break;\n            case POLAK_RIBIERE:\n                beta = (delta - deltaMid) / deltaOld;\n                break;\n            default:\n                throw new MathInternalError();\n            }\n            steepestDescent = newSteepestDescent;\n\n            // Compute conjugate search direction.\n            if (iter % n == 0 ||\n                beta < 0) {\n                // Break conjugation: reset search direction.\n                searchDirection = steepestDescent.clone();\n            } else {\n                // Compute new conjugate search direction.\n                for (int i = 0; i < n; ++i) {\n                    searchDirection[i] = steepestDescent[i] + beta * searchDirection[i];\n                }\n            }\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n        final double[] point = getStartPoint();\n        final GoalType goal = getGoalType();\n        final int n = point.length;\n        double[] r = computeObjectiveGradient(point);\n        if (goal == GoalType.MINIMIZE) {\n            for (int i = 0; i < n; i++) {\n                r[i] = -r[i];\n            }\n        }\n\n        // Initial search direction.\n        double[] steepestDescent = preconditioner.precondition(point, r);\n        double[] searchDirection = steepestDescent.clone();\n\n        double delta = 0;\n        for (int i = 0; i < n; ++i) {\n            delta += r[i] * searchDirection[i];\n        }\n\n        PointValuePair current = null;\n        int iter = 0;\n        int maxEval = getMaxEvaluations();\n        while (true) {\n            ++iter;\n\n            final double objective = computeObjectiveValue(point);\n            PointValuePair previous = current;\n            current = new PointValuePair(point.clone(), objective); // Fix 1: clone the point to ensure immutability\n            if (previous != null) {\n                if (checker.converged(iter, previous, current)) {\n                    // We have found an optimum.\n                    return current;\n                }\n            }\n\n            // Find the optimal step in the search direction.\n            final UnivariateFunction lsf = new LineSearchFunction(point, searchDirection);\n            final double uB = findUpperBound(lsf, 0, initialStep);\n            final double step = solver.solve(maxEval, lsf, 0, uB, 1e-15);\n            maxEval -= solver.getEvaluations(); // Subtract used up evaluations.\n\n            // Validate new point.\n            for (int i = 0; i < point.length; ++i) {\n                point[i] += step * searchDirection[i];\n            }\n\n            r = computeObjectiveGradient(point);\n            if (goal == GoalType.MINIMIZE) {\n                for (int i = 0; i < n; ++i) {\n                    r[i] = -r[i];\n                }\n            }\n\n            // Compute beta.\n            final double deltaOld = delta;\n            final double[] newSteepestDescent = preconditioner.precondition(point, r);\n            delta = 0;\n            for (int i = 0; i < n; ++i) {\n                delta += r[i] * newSteepestDescent[i];\n            }\n\n            final double beta;\n            switch (updateFormula) {\n            case FLETCHER_REEVES:\n                beta = delta / deltaOld;\n                break;\n            case POLAK_RIBIERE:\n                double deltaMid = 0;\n                for (int i = 0; i < r.length; ++i) {\n                    deltaMid += r[i] * steepestDescent[i];\n                }\n                beta = (delta - deltaMid) / deltaOld;\n                break;\n            default:\n                // Should never happen.\n                throw new MathInternalError();\n            }\n            steepestDescent = newSteepestDescent;\n\n            // Compute conjugate search direction.\n            if (iter % n == 0 ||\n                beta < 0) {\n                // Break conjugation: reset search direction.\n                searchDirection = steepestDescent.clone();\n            } else {\n                // Compute new conjugate search direction.\n                for (int i = 0; i < n; ++i) {\n                    searchDirection[i] = steepestDescent[i] + beta * searchDirection[i];\n                }\n            }\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n        final double[] point = getStartPoint();\n        final GoalType goal = getGoalType();\n        final int n = point.length;\n        double[] r = computeObjectiveGradient(point);\n        if (goal == GoalType.MINIMIZE) {\n            for (int i = 0; i < n; i++) {\n                r[i] = -r[i];\n            }\n        }\n\n        // Initial search direction.\n        double[] steepestDescent = preconditioner.precondition(point, r);\n        double[] searchDirection = steepestDescent.clone();\n\n        double delta = 0;\n        for (int i = 0; i < n; ++i) {\n            delta += r[i] * searchDirection[i];\n        }\n\n        PointValuePair current = null;\n        int iter = 0;\n        int maxEval = getMaxEvaluations();\n        while (true) {\n            ++iter;\n\n            final double objective = computeObjectiveValue(point);\n            PointValuePair previous = current;\n            current = new PointValuePair(point.clone(), objective);\n            if (previous != null) {\n                if (checker.converged(iter, previous, current)) {\n                    // We have found an optimum.\n                    return current;\n                }\n            }\n\n            // Find the optimal step in the search direction.\n            final UnivariateFunction lsf = new LineSearchFunction(point, searchDirection);\n            final double uB = findUpperBound(lsf, 0, initialStep);\n            // XXX Last parameters is set to a value close to zero in order to\n            // work around the divergence problem in the \"testCircleFitting\"\n            // unit test (see MATH-439).\n            final double step = solver.solve(maxEval, lsf, 0, uB, 1e-15);\n            maxEval -= solver.getEvaluations(); // Subtract used up evaluations.\n\n            // Validate new point.\n            for (int i = 0; i < point.length; ++i) {\n                point[i] += step * searchDirection[i];\n            }\n\n            r = computeObjectiveGradient(point);\n            if (goal == GoalType.MINIMIZE) {\n                for (int i = 0; i < n; ++i) {\n                    r[i] = -r[i];\n                }\n            }\n\n            // Compute beta.\n            final double deltaOld = delta;\n            final double[] newSteepestDescent = preconditioner.precondition(point, r);\n            delta = 0;\n            for (int i = 0; i < n; ++i) {\n                delta += r[i] * newSteepestDescent[i];\n            }\n\n            final double beta;\n            switch (updateFormula) {\n            case FLETCHER_REEVES:\n                beta = delta / deltaOld;\n                break;\n            case POLAK_RIBIERE:\n                double deltaMid = 0;\n                for (int i = 0; i < r.length; ++i) {\n                    deltaMid += r[i] * steepestDescent[i];\n                }\n                beta = (delta - deltaMid) / deltaOld;\n                break;\n            default:\n                // Should never happen.\n                throw new MathInternalError();\n            }\n            steepestDescent = newSteepestDescent;\n\n            // Compute conjugate search direction.\n            if (iter % n == 0 ||\n                beta < 0 || Double.isNaN(beta) || Double.isInfinite(beta)) {\n                // Break conjugation: reset search direction.\n                searchDirection = steepestDescent.clone();\n            } else {\n                // Compute new conjugate search direction.\n                for (int i = 0; i < n; ++i) {\n                    searchDirection[i] = steepestDescent[i] + beta * searchDirection[i];\n                }\n            }\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n        final double[] point = getStartPoint();\n        final GoalType goal = getGoalType();\n        final int n = point.length;\n        \n        double[] r = computeObjectiveGradient(point);\n        if (goal == GoalType.MINIMIZE) {\n            for (int i = 0; i < n; i++) {\n                r[i] = -r[i];\n            }\n        }\n\n        // Initial search direction.\n        double[] steepestDescent = preconditioner.precondition(point, r);\n        if (steepestDescent == null) {\n            throw new IllegalStateException(\"Preconditioner returned null\");\n        }\n        double[] searchDirection = steepestDescent.clone();\n        \n        double delta = 0;\n        for (int i = 0; i < n; ++i) {\n            delta += r[i] * searchDirection[i];\n        }\n\n        PointValuePair current = null;\n        int iter = 0;\n        int maxEval = getMaxEvaluations();\n        if (maxEval <= 0) {\n            throw new MathIllegalStateException(\"Number of evaluations must be positive\");\n        }\n\n        while (true) {\n            ++iter;\n\n            final double objective = computeObjectiveValue(point);\n            PointValuePair previous = current;\n            current = new PointValuePair(point.clone(), objective);  // Ensure we use a copy of the point\n            if (previous != null && checker != null) {\n                if (checker.converged(iter, previous, current)) {\n                    // We have found an optimum.\n                    return current;\n                }\n            }\n\n            // Find the optimal step in the search direction.\n            final UnivariateFunction lsf = new LineSearchFunction(point, searchDirection);\n            final double uB = findUpperBound(lsf, 0, initialStep);\n            // XXX Last parameter is set to a value close to zero in order to\n            // work around the divergence problem in the \"testCircleFitting\"\n            // unit test (see MATH-439).\n            final double step = solver.solve(maxEval, lsf, 0, uB, 1e-15);\n            maxEval -= solver.getEvaluations(); // Subtract used up evaluations.\n\n            // Validate new point.\n            for (int i = 0; i < n; ++i) {\n                point[i] += step * searchDirection[i];\n            }\n\n            r = computeObjectiveGradient(point);\n            if (goal == GoalType.MINIMIZE) {\n                for (int i = 0; i < n; ++i) {\n                    r[i] = -r[i];\n                }\n            }\n\n            // Compute beta.\n            final double deltaOld = delta;\n            final double[] newSteepestDescent = preconditioner.precondition(point, r);\n            if (newSteepestDescent == null) {\n                throw new IllegalStateException(\"Preconditioner returned null\");\n            }\n\n            delta = 0;\n            for (int i = 0; i < n; ++i) {\n                delta += r[i] * newSteepestDescent[i];\n            }\n\n            final double beta;\n            switch (updateFormula) {\n            case FLETCHER_REEVES:\n                beta = delta / deltaOld;\n                break;\n            case POLAK_RIBIERE:\n                double deltaMid = 0;\n                for (int i = 0; i < r.length; ++i) {\n                    deltaMid += r[i] * steepestDescent[i];\n                }\n                beta = (delta - deltaMid) / deltaOld;\n                break;\n            default:\n                // Should never happen.\n                throw new MathInternalError();\n            }\n            steepestDescent = newSteepestDescent;\n\n            // Compute conjugate search direction.\n            if (iter % n == 0 || beta < 0) {\n                // Break conjugation: reset search direction.\n                searchDirection = steepestDescent.clone();\n            } else {\n                // Compute new conjugate search direction.\n                for (int i = 0; i < n; ++i) {\n                    searchDirection[i] = steepestDescent[i] + beta * searchDirection[i];\n                }\n            }\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n        final double[] point = getStartPoint();\n        final GoalType goal = getGoalType();\n        final int n = point.length;\n        double[] r = computeObjectiveGradient(point);\n        if (goal == GoalType.MINIMIZE) {\n            for (int i = 0; i < n; i++) {\n                r[i] = -r[i];\n            }\n        }\n\n        // Initial search direction.\n        double[] steepestDescent = preconditioner.precondition(point, r);\n        double[] searchDirection = steepestDescent.clone();\n\n        double delta = 0;\n        for (int i = 0; i < n; ++i) {\n            delta += r[i] * steepestDescent[i]; // Fixed: use steepestDescent instead of searchDirection\n        }\n\n        PointValuePair current = null;\n        int iter = 0;\n        int maxEval = getMaxEvaluations();\n        while (true) {\n            ++iter;\n\n            final double objective = computeObjectiveValue(point);\n            PointValuePair previous = current;\n            current = new PointValuePair(point.clone(), objective); // Fixed: use clone to prevent mutation issues\n            if (previous != null) {\n                if (checker.converged(iter, previous, current)) {\n                    // We have found an optimum.\n                    return current;\n                }\n            }\n\n            // Find the optimal step in the search direction.\n            final UnivariateFunction lsf = new LineSearchFunction(point, searchDirection);\n            final double uB = findUpperBound(lsf, 0, initialStep);\n            // XXX Last parameters is set to a value close to zero in order to\n            // work around the divergence problem in the \"testCircleFitting\"\n            // unit test (see MATH-439).\n            final double step = solver.solve(maxEval, lsf, 0, uB, 1e-15);\n            maxEval -= solver.getEvaluations(); // Subtract used up evaluations.\n\n            // Validate new point.\n            for (int i = 0; i < point.length; ++i) {\n                point[i] += step * searchDirection[i];\n            }\n\n            r = computeObjectiveGradient(point);\n            if (goal == GoalType.MINIMIZE) {\n                for (int i = 0; i < n; ++i) {\n                    r[i] = -r[i];\n                }\n            }\n\n            // Compute beta.\n            final double deltaOld = delta;\n            final double[] newSteepestDescent = preconditioner.precondition(point, r);\n            delta = 0;\n            for (int i = 0; i < n; ++i) {\n                delta += r[i] * newSteepestDescent[i];\n            }\n\n            final double beta;\n            switch (updateFormula) {\n            case FLETCHER_REEVES:\n                beta = delta / deltaOld;\n                break;\n            case POLAK_RIBIERE:\n                double deltaMid = 0;\n                for (int i = 0; i < r.length; ++i) {\n                    deltaMid += r[i] * steepestDescent[i];\n                }\n                beta = (delta - deltaMid) / deltaOld;\n                break;\n            default:\n                // Should never happen.\n                throw new MathInternalError();\n            }\n            steepestDescent = newSteepestDescent;\n\n            // Compute conjugate search direction.\n            if (iter % n == 0 ||\n                beta < 0) {\n                // Break conjugation: reset search direction.\n                searchDirection = steepestDescent.clone();\n            } else {\n                // Compute new conjugate search direction.\n                for (int i = 0; i < n; ++i) {\n                    searchDirection[i] = steepestDescent[i] + beta * searchDirection[i];\n                }\n            }\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n        final double[] point = getStartPoint();\n        final GoalType goal = getGoalType();\n        final int n = point.length;\n        double[] r = computeObjectiveGradient(point);\n        if (goal == GoalType.MINIMIZE) {\n            for (int i = 0; i < n; i++) {\n                r[i] = -r[i];\n            }\n        }\n\n        // Initial search direction.\n        double[] steepestDescent = preconditioner.precondition(point, r);\n        double[] searchDirection = steepestDescent.clone();\n\n        double delta = 0;\n        for (int i = 0; i < n; ++i) {\n            delta += r[i] * searchDirection[i];\n        }\n\n        PointValuePair current = null;\n        int iter = 0;\n        int maxEval = getMaxEvaluations();\n        while (true) {\n            ++iter;\n\n            final double objective = computeObjectiveValue(point);\n            PointValuePair previous = current;\n            current = new PointValuePair(point.clone(), objective);  // Ensure a new copy of point is used\n            if (previous != null) {\n                if (checker.converged(iter, previous, current)) {\n                    // We have found an optimum.\n                    return current;\n                }\n            }\n\n            // Find the optimal step in the search direction.\n            final UnivariateFunction lsf = new LineSearchFunction(point, searchDirection);\n            final double uB = findUpperBound(lsf, 0, initialStep);\n            // XXX Last parameters is set to a value close to zero in order to\n            // work around the divergence problem in the \"testCircleFitting\"\n            // unit test (see MATH-439).\n            final double step = solver.solve(maxEval, lsf, 0, uB, 1e-15);\n            maxEval -= solver.getEvaluations(); // Subtract used up evaluations.\n\n            // Validate new point.\n            for (int i = 0; i < point.length; ++i) {\n                point[i] += step * searchDirection[i];\n            }\n\n            r = computeObjectiveGradient(point);\n            if (goal == GoalType.MINIMIZE) {\n                for (int i = 0; i < n; ++i) {\n                    r[i] = -r[i];\n                }\n            }\n\n            // Compute beta.\n            final double deltaOld = delta;\n            final double[] newSteepestDescent = preconditioner.precondition(point, r);\n            delta = 0;\n            for (int i = 0; i < n; ++i) {\n                delta += r[i] * newSteepestDescent[i];\n            }\n\n            final double beta;\n            switch (updateFormula) {\n            case FLETCHER_REEVES:\n                beta = delta / deltaOld;\n                break;\n            case POLAK_RIBIERE:\n                double deltaMid = 0;\n                for (int i = 0; i < r.length; ++i) {\n                    deltaMid += r[i] * steepestDescent[i];\n                }\n                beta = (delta - deltaMid) / deltaOld;\n                break;\n            default:\n                // Should never happen.\n                throw new MathInternalError();\n            }\n            steepestDescent = newSteepestDescent;\n\n            // Compute conjugate search direction.\n            if (iter % n == 0 ||\n                beta < 0) {\n                // Break conjugation: reset search direction.\n                searchDirection = steepestDescent.clone();\n            } else {\n                // Compute new conjugate search direction.\n                for (int i = 0; i < n; ++i) {\n                    searchDirection[i] = steepestDescent[i] + beta * searchDirection[i];\n                }\n            }\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n        final double[] point = getStartPoint();\n        final GoalType goal = getGoalType();\n        final int n = point.length;\n        double[] r = computeObjectiveGradient(point);\n        if (goal == GoalType.MINIMIZE) {\n            for (int i = 0; i < n; i++) {\n                r[i] = -r[i];\n            }\n        }\n\n        // Initial search direction.\n        double[] steepestDescent = preconditioner.precondition(point, r);\n        double[] searchDirection = steepestDescent.clone();\n\n        double delta = 0;\n        for (int i = 0; i < n; ++i) {\n            delta += r[i] * searchDirection[i];\n        }\n\n        PointValuePair current = null;\n        int iter = 0;\n        int maxEval = getMaxEvaluations();\n        while (true) {\n            ++iter;\n\n            final double objective = computeObjectiveValue(point);\n            PointValuePair previous = current;\n            current = new PointValuePair(point.clone(), objective);\n            if (previous != null) {\n                if (checker.converged(iter, previous, current)) {\n                    // We have found an optimum.\n                    return current;\n                }\n            }\n\n            // Find the optimal step in the search direction.\n            final UnivariateFunction lsf = new LineSearchFunction(point, searchDirection);\n            final double uB = findUpperBound(lsf, 0, initialStep);\n            final double step = solver.solve(maxEval, lsf, 0, uB, 1e-15);\n            maxEval -= solver.getEvaluations(); // Subtract used up evaluations.\n\n            // Validate new point.\n            for (int i = 0; i < point.length; ++i) {\n                point[i] += step * searchDirection[i];\n            }\n\n            r = computeObjectiveGradient(point);\n            if (goal == GoalType.MINIMIZE) {\n                for (int i = 0; i < n; ++i) {\n                    r[i] = -r[i];\n                }\n            }\n\n            // Compute beta.\n            final double deltaOld = delta;\n            final double[] newSteepestDescent = preconditioner.precondition(point, r);\n            delta = 0;\n            for (int i = 0; i < n; ++i) {\n                delta += r[i] * newSteepestDescent[i];\n            }\n\n            final double beta;\n            switch (updateFormula) {\n            case FLETCHER_REEVES:\n                beta = delta / deltaOld;\n                break;\n            case POLAK_RIBIERE:\n                double deltaMid = 0;\n                for (int i = 0; i < r.length; ++i) {\n                    deltaMid += r[i] * steepestDescent[i];\n                }\n                beta = (delta - deltaMid) / deltaOld;\n                break;\n            default:\n                // Should never happen.\n                throw new MathInternalError();\n            }\n            steepestDescent = newSteepestDescent;\n\n            // Compute conjugate search direction.\n            if (iter % n == 0 ||\n                beta < 0) {\n                // Break conjugation: reset search direction.\n                searchDirection = steepestDescent.clone();\n            } else {\n                // Compute new conjugate search direction.\n                for (int i = 0; i < n; ++i) {\n                    searchDirection[i] = steepestDescent[i] + beta * searchDirection[i];\n                }\n            }\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n        final double[] point = getStartPoint();\n        final GoalType goal = getGoalType();\n        final int n = point.length;\n        double[] r = computeObjectiveGradient(point);\n        if (goal == GoalType.MINIMIZE) {\n            for (int i = 0; i < n; i++) {\n                r[i] = -r[i];\n            }\n        }\n\n        // Initial search direction.\n        double[] steepestDescent = preconditioner.precondition(point, r);\n        double[] searchDirection = steepestDescent.clone();\n\n        double delta = 0;\n        for (int i = 0; i < n; ++i) {\n            delta += r[i] * searchDirection[i];\n        }\n\n        PointValuePair current = null;\n        int iter = 0;\n        int maxEval = getMaxEvaluations();\n        while (true) {\n            ++iter;\n\n            final double objective = computeObjectiveValue(point);\n            PointValuePair previous = current;\n            current = new PointValuePair(point.clone(), objective);\n            if (previous != null) {\n                if (checker.converged(iter, previous, current)) {\n                    // We have found an optimum.\n                    return current;\n                }\n            }\n\n            // Find the optimal step in the search direction.\n            final UnivariateFunction lsf = new LineSearchFunction(point, searchDirection);\n            final double uB = findUpperBound(lsf, 0, initialStep);\n            // XXX Last parameters is set to a value close to zero in order to\n            // work around the divergence problem in the \"testCircleFitting\"\n            // unit test (see MATH-439).\n            final double step = solver.solve(maxEval, lsf, 0, uB, 1e-15);\n            maxEval -= solver.getEvaluations(); // Subtract used up evaluations.\n\n            // Validate new point.\n            for (int i = 0; i < point.length; ++i) {\n                point[i] += step * searchDirection[i];\n            }\n\n            r = computeObjectiveGradient(point);\n            if (goal == GoalType.MINIMIZE) {\n                for (int i = 0; i < n; ++i) {\n                    r[i] = -r[i];\n                }\n            }\n\n            // Compute beta.\n            final double deltaOld = delta;\n            final double[] newSteepestDescent = preconditioner.precondition(point, r);\n            delta = 0;\n            for (int i = 0; i < n; ++i) {\n                delta += r[i] * newSteepestDescent[i];\n            }\n\n            final double beta;\n            switch (updateFormula) {\n            case FLETCHER_REEVES:\n                beta = delta / deltaOld;\n                break;\n            case POLAK_RIBIERE:\n                double deltaMid = 0;\n                for (int i = 0; i < r.length; ++i) {\n                    deltaMid += r[i] * steepestDescent[i];\n                }\n                beta = (delta - deltaMid) / deltaOld;\n                break;\n            default:\n                // Should never happen.\n                throw new MathInternalError();\n            }\n            steepestDescent = newSteepestDescent;\n\n            // Compute conjugate search direction.\n            if (iter % n == 0 ||\n                beta < 0) {\n                // Break conjugation: reset search direction.\n                searchDirection = steepestDescent.clone();\n            } else {\n                // Compute new conjugate search direction.\n                for (int i = 0; i < n; ++i) {\n                    searchDirection[i] = steepestDescent[i] + beta * searchDirection[i];\n                }\n            }\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n        final double[] point = getStartPoint();\n        final GoalType goal = getGoalType();\n        final int n = point.length;\n        double[] r = computeObjectiveGradient(point);\n        if (goal == GoalType.MINIMIZE) {\n            for (int i = 0; i < n; i++) {\n                r[i] = -r[i];\n            }\n        }\n\n        // Initial search direction.\n        double[] steepestDescent = preconditioner.precondition(point, r);\n        double[] searchDirection = steepestDescent.clone();\n\n        double delta = 0;\n        for (int i = 0; i < n; ++i) {\n            delta += r[i] * steepestDescent[i];\n        }\n\n        PointValuePair current = null;\n        int iter = 0;\n        int maxEval = getMaxEvaluations();\n        while (true) {\n            ++iter;\n\n            final double objective = computeObjectiveValue(point);\n            PointValuePair previous = current;\n            current = new PointValuePair(point.clone(), objective); // Clone point to ensure checkpointing.\n            if (previous != null) {\n                if (checker.converged(iter, previous, current)) {\n                    // We have found an optimum.\n                    return current;\n                }\n            }\n\n            // Find the optimal step in the search direction.\n            final UnivariateFunction lsf = new LineSearchFunction(point, searchDirection);\n            final double uB = findUpperBound(lsf, 0, initialStep);\n            // XXX Last parameters is set to a value close to zero in order to\n            // work around the divergence problem in the \"testCircleFitting\"\n            // unit test (see MATH-439).\n            final double step = solver.solve(maxEval, lsf, 0, uB, 1e-15);\n            maxEval -= solver.getEvaluations(); // Subtract used up evaluations.\n\n            // Validate new point.\n            for (int i = 0; i < point.length; ++i) {\n                point[i] += step * searchDirection[i];\n            }\n\n            r = computeObjectiveGradient(point);\n            if (goal == GoalType.MINIMIZE) {\n                for (int i = 0; i < n; ++i) {\n                    r[i] = -r[i];\n                }\n            }\n\n            // Compute beta.\n            final double deltaOld = delta;\n            final double[] newSteepestDescent = preconditioner.precondition(point, r);\n            delta = 0;\n            for (int i = 0; i < n; ++i) {\n                delta += r[i] * newSteepestDescent[i];\n            }\n\n            final double beta;\n            switch (updateFormula) {\n            case FLETCHER_REEVES:\n                beta = delta / deltaOld;\n                break;\n            case POLAK_RIBIERE:\n                double deltaMid = 0;\n                for (int i = 0; i < r.length; ++i) {\n                    deltaMid += r[i] * steepestDescent[i];\n                }\n                beta = (delta - deltaMid) / deltaOld;\n                break;\n            default:\n                // Should never happen.\n                throw new MathInternalError();\n            }\n            steepestDescent = newSteepestDescent;\n\n            // Compute conjugate search direction.\n            if (iter % n == 0 ||\n                beta < 0) {\n                // Break conjugation: reset search direction.\n                searchDirection = steepestDescent.clone();\n            } else {\n                // Compute new conjugate search direction.\n                for (int i = 0; i < n; ++i) {\n                    searchDirection[i] = steepestDescent[i] + beta * searchDirection[i];\n                }\n            }\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n        final double[] point = getStartPoint();\n        final GoalType goal = getGoalType();\n        final int n = point.length;\n        double[] r = computeObjectiveGradient(point);\n        if (goal == GoalType.MINIMIZE) {\n            for (int i = 0; i < n; i++) {\n                r[i] = -r[i];\n            }\n        }\n\n        // Initial search direction.\n        double[] steepestDescent = preconditioner.precondition(point, r);\n        double[] searchDirection = steepestDescent.clone();\n\n        double delta = 0;\n        for (int i = 0; i < n; ++i) {\n            delta += r[i] * searchDirection[i];\n        }\n\n        PointValuePair current = null;\n        int iter = 0;\n        int maxEval = getMaxEvaluations();\n        while (true) {\n            ++iter;\n\n            final double objective = computeObjectiveValue(point);\n            PointValuePair previous = current;\n            current = new PointValuePair(point.clone(), objective);\n            if (previous != null) {\n                if (checker.converged(iter, previous, current)) {\n                    // We have found an optimum.\n                    return current;\n                }\n            }\n\n            // Find the optimal step in the search direction.\n            final UnivariateFunction lsf = new LineSearchFunction(point, searchDirection);\n            final double uB = findUpperBound(lsf, 0, initialStep);\n            // XXX Last parameters is set to a value close to zero in order to\n            // work around the divergence problem in the \"testCircleFitting\"\n            // unit test (see MATH-439).\n            final double step = solver.solve(maxEval, lsf, 0, uB, 1e-15);\n            maxEval -= solver.getEvaluations(); // Subtract used up evaluations.\n\n            // Validate new point.\n            for (int i = 0; i < point.length; ++i) {\n                point[i] += step * searchDirection[i];\n            }\n\n            r = computeObjectiveGradient(point);\n            if (goal == GoalType.MINIMIZE) {\n                for (int i = 0; i < n; ++i) {\n                    r[i] = -r[i];\n                }\n            }\n\n            // Compute beta.\n            final double deltaOld = delta;\n            final double[] newSteepestDescent = preconditioner.precondition(point, r);\n            delta = 0;\n            for (int i = 0; i < n; ++i) {\n                delta += r[i] * newSteepestDescent[i];\n            }\n\n            final double beta;\n            switch (updateFormula) {\n            case FLETCHER_REEVES:\n                beta = delta / deltaOld;\n                break;\n            case POLAK_RIBIERE:\n                double deltaMid = 0;\n                for (int i = 0; i < r.length; ++i) {\n                    deltaMid += r[i] * steepestDescent[i];\n                }\n                beta = (delta - deltaMid) / deltaOld;\n                break;\n            default:\n                // Should never happen.\n                throw new MathInternalError();\n            }\n            steepestDescent = newSteepestDescent;\n\n            // Compute conjugate search direction.\n            if (iter % n == 0 ||\n                beta < 0) {\n                // Break conjugation: reset search direction.\n                searchDirection = steepestDescent.clone();\n            } else {\n                // Compute new conjugate search direction.\n                for (int i = 0; i < n; ++i) {\n                    searchDirection[i] = steepestDescent[i] + beta * searchDirection[i];\n                }\n            }\n        }\n    }\n"
                        ],
                        "plausible_patches": [
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n        final double[] point = getStartPoint();\n        final GoalType goal = getGoalType();\n        final int n = point.length;\n        double[] r = computeObjectiveGradient(point);\n        if (goal == GoalType.MINIMIZE) {\n            for (int i = 0; i < n; i++) {\n                r[i] = -r[i];\n            }\n        }\n\n        // Initial search direction.\n        double[] steepestDescent = preconditioner.precondition(point, r);\n        double[] searchDirection = steepestDescent.clone();\n\n        double delta = 0;\n        for (int i = 0; i < n; ++i) {\n            delta += r[i] * steepestDescent[i];\n        }\n\n        PointValuePair current = null;\n        int iter = 0;\n        int maxEval = getMaxEvaluations();\n        while (true) {\n            ++iter;\n\n            final double objective = computeObjectiveValue(point);\n            PointValuePair previous = current;\n            current = new PointValuePair(point, objective);\n            if (previous != null) {\n                if (checker.converged(iter, previous, current)) {\n                    // We have found an optimum.\n                    return current;\n                }\n            }\n\n            // Find the optimal step in the search direction.\n            final UnivariateFunction lsf = new LineSearchFunction(point, searchDirection);\n            final double uB = findUpperBound(lsf, 0, initialStep);\n            final double step = solver.solve(maxEval, lsf, 0, uB, 1e-15);\n            maxEval -= solver.getEvaluations(); // Subtract used up evaluations.\n\n            // Validate new point.\n            for (int i = 0; i < point.length; ++i) {\n                point[i] += step * searchDirection[i];\n            }\n\n            r = computeObjectiveGradient(point);\n            if (goal == GoalType.MINIMIZE) {\n                for (int i = 0; i < n; ++i) {\n                    r[i] = -r[i];\n                }\n            }\n\n            // Compute beta.\n            final double deltaOld = delta;\n            double deltaMid = 0;\n            for (int i = 0; i < n; ++i) {\n                deltaMid += r[i] * steepestDescent[i];\n            }\n            final double[] newSteepestDescent = preconditioner.precondition(point, r);\n            delta = 0;\n            for (int i = 0; i < n; ++i) {\n                delta += r[i] * newSteepestDescent[i];\n            }\n\n            final double beta;\n            switch (updateFormula) {\n            case FLETCHER_REEVES:\n                beta = delta / deltaOld;\n                break;\n            case POLAK_RIBIERE:\n                beta = (delta - deltaMid) / deltaOld;\n                break;\n            default:\n                throw new MathInternalError();\n            }\n            steepestDescent = newSteepestDescent;\n\n            // Compute conjugate search direction.\n            if (iter % n == 0 ||\n                beta < 0) {\n                // Break conjugation: reset search direction.\n                searchDirection = steepestDescent.clone();\n            } else {\n                // Compute new conjugate search direction.\n                for (int i = 0; i < n; ++i) {\n                    searchDirection[i] = steepestDescent[i] + beta * searchDirection[i];\n                }\n            }\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n        final double[] point = getStartPoint();\n        final GoalType goal = getGoalType();\n        final int n = point.length;\n        double[] r = computeObjectiveGradient(point);\n        if (goal == GoalType.MINIMIZE) {\n            for (int i = 0; i < n; i++) {\n                r[i] = -r[i];\n            }\n        }\n\n        // Initial search direction.\n        double[] steepestDescent = preconditioner.precondition(point, r);\n        double[] searchDirection = steepestDescent.clone();\n\n        double delta = 0;\n        for (int i = 0; i < n; ++i) {\n            delta += r[i] * searchDirection[i];\n        }\n\n        PointValuePair current = null;\n        int iter = 0;\n        int maxEval = getMaxEvaluations();\n        while (true) {\n            ++iter;\n\n            final double objective = computeObjectiveValue(point);\n            PointValuePair previous = current;\n            current = new PointValuePair(point.clone(), objective); // Fix 1: clone the point to ensure immutability\n            if (previous != null) {\n                if (checker.converged(iter, previous, current)) {\n                    // We have found an optimum.\n                    return current;\n                }\n            }\n\n            // Find the optimal step in the search direction.\n            final UnivariateFunction lsf = new LineSearchFunction(point, searchDirection);\n            final double uB = findUpperBound(lsf, 0, initialStep);\n            final double step = solver.solve(maxEval, lsf, 0, uB, 1e-15);\n            maxEval -= solver.getEvaluations(); // Subtract used up evaluations.\n\n            // Validate new point.\n            for (int i = 0; i < point.length; ++i) {\n                point[i] += step * searchDirection[i];\n            }\n\n            r = computeObjectiveGradient(point);\n            if (goal == GoalType.MINIMIZE) {\n                for (int i = 0; i < n; ++i) {\n                    r[i] = -r[i];\n                }\n            }\n\n            // Compute beta.\n            final double deltaOld = delta;\n            final double[] newSteepestDescent = preconditioner.precondition(point, r);\n            delta = 0;\n            for (int i = 0; i < n; ++i) {\n                delta += r[i] * newSteepestDescent[i];\n            }\n\n            final double beta;\n            switch (updateFormula) {\n            case FLETCHER_REEVES:\n                beta = delta / deltaOld;\n                break;\n            case POLAK_RIBIERE:\n                double deltaMid = 0;\n                for (int i = 0; i < r.length; ++i) {\n                    deltaMid += r[i] * steepestDescent[i];\n                }\n                beta = (delta - deltaMid) / deltaOld;\n                break;\n            default:\n                // Should never happen.\n                throw new MathInternalError();\n            }\n            steepestDescent = newSteepestDescent;\n\n            // Compute conjugate search direction.\n            if (iter % n == 0 ||\n                beta < 0) {\n                // Break conjugation: reset search direction.\n                searchDirection = steepestDescent.clone();\n            } else {\n                // Compute new conjugate search direction.\n                for (int i = 0; i < n; ++i) {\n                    searchDirection[i] = steepestDescent[i] + beta * searchDirection[i];\n                }\n            }\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n        final double[] point = getStartPoint();\n        final GoalType goal = getGoalType();\n        final int n = point.length;\n        double[] r = computeObjectiveGradient(point);\n        if (goal == GoalType.MINIMIZE) {\n            for (int i = 0; i < n; i++) {\n                r[i] = -r[i];\n            }\n        }\n\n        // Initial search direction.\n        double[] steepestDescent = preconditioner.precondition(point, r);\n        double[] searchDirection = steepestDescent.clone();\n\n        double delta = 0;\n        for (int i = 0; i < n; ++i) {\n            delta += r[i] * searchDirection[i];\n        }\n\n        PointValuePair current = null;\n        int iter = 0;\n        int maxEval = getMaxEvaluations();\n        while (true) {\n            ++iter;\n\n            final double objective = computeObjectiveValue(point);\n            PointValuePair previous = current;\n            current = new PointValuePair(point.clone(), objective);\n            if (previous != null) {\n                if (checker.converged(iter, previous, current)) {\n                    // We have found an optimum.\n                    return current;\n                }\n            }\n\n            // Find the optimal step in the search direction.\n            final UnivariateFunction lsf = new LineSearchFunction(point, searchDirection);\n            final double uB = findUpperBound(lsf, 0, initialStep);\n            // XXX Last parameters is set to a value close to zero in order to\n            // work around the divergence problem in the \"testCircleFitting\"\n            // unit test (see MATH-439).\n            final double step = solver.solve(maxEval, lsf, 0, uB, 1e-15);\n            maxEval -= solver.getEvaluations(); // Subtract used up evaluations.\n\n            // Validate new point.\n            for (int i = 0; i < point.length; ++i) {\n                point[i] += step * searchDirection[i];\n            }\n\n            r = computeObjectiveGradient(point);\n            if (goal == GoalType.MINIMIZE) {\n                for (int i = 0; i < n; ++i) {\n                    r[i] = -r[i];\n                }\n            }\n\n            // Compute beta.\n            final double deltaOld = delta;\n            final double[] newSteepestDescent = preconditioner.precondition(point, r);\n            delta = 0;\n            for (int i = 0; i < n; ++i) {\n                delta += r[i] * newSteepestDescent[i];\n            }\n\n            final double beta;\n            switch (updateFormula) {\n            case FLETCHER_REEVES:\n                beta = delta / deltaOld;\n                break;\n            case POLAK_RIBIERE:\n                double deltaMid = 0;\n                for (int i = 0; i < r.length; ++i) {\n                    deltaMid += r[i] * steepestDescent[i];\n                }\n                beta = (delta - deltaMid) / deltaOld;\n                break;\n            default:\n                // Should never happen.\n                throw new MathInternalError();\n            }\n            steepestDescent = newSteepestDescent;\n\n            // Compute conjugate search direction.\n            if (iter % n == 0 ||\n                beta < 0 || Double.isNaN(beta) || Double.isInfinite(beta)) {\n                // Break conjugation: reset search direction.\n                searchDirection = steepestDescent.clone();\n            } else {\n                // Compute new conjugate search direction.\n                for (int i = 0; i < n; ++i) {\n                    searchDirection[i] = steepestDescent[i] + beta * searchDirection[i];\n                }\n            }\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n        final double[] point = getStartPoint();\n        final GoalType goal = getGoalType();\n        final int n = point.length;\n        double[] r = computeObjectiveGradient(point);\n        if (goal == GoalType.MINIMIZE) {\n            for (int i = 0; i < n; i++) {\n                r[i] = -r[i];\n            }\n        }\n\n        // Initial search direction.\n        double[] steepestDescent = preconditioner.precondition(point, r);\n        double[] searchDirection = steepestDescent.clone();\n\n        double delta = 0;\n        for (int i = 0; i < n; ++i) {\n            delta += r[i] * steepestDescent[i]; // Fixed: use steepestDescent instead of searchDirection\n        }\n\n        PointValuePair current = null;\n        int iter = 0;\n        int maxEval = getMaxEvaluations();\n        while (true) {\n            ++iter;\n\n            final double objective = computeObjectiveValue(point);\n            PointValuePair previous = current;\n            current = new PointValuePair(point.clone(), objective); // Fixed: use clone to prevent mutation issues\n            if (previous != null) {\n                if (checker.converged(iter, previous, current)) {\n                    // We have found an optimum.\n                    return current;\n                }\n            }\n\n            // Find the optimal step in the search direction.\n            final UnivariateFunction lsf = new LineSearchFunction(point, searchDirection);\n            final double uB = findUpperBound(lsf, 0, initialStep);\n            // XXX Last parameters is set to a value close to zero in order to\n            // work around the divergence problem in the \"testCircleFitting\"\n            // unit test (see MATH-439).\n            final double step = solver.solve(maxEval, lsf, 0, uB, 1e-15);\n            maxEval -= solver.getEvaluations(); // Subtract used up evaluations.\n\n            // Validate new point.\n            for (int i = 0; i < point.length; ++i) {\n                point[i] += step * searchDirection[i];\n            }\n\n            r = computeObjectiveGradient(point);\n            if (goal == GoalType.MINIMIZE) {\n                for (int i = 0; i < n; ++i) {\n                    r[i] = -r[i];\n                }\n            }\n\n            // Compute beta.\n            final double deltaOld = delta;\n            final double[] newSteepestDescent = preconditioner.precondition(point, r);\n            delta = 0;\n            for (int i = 0; i < n; ++i) {\n                delta += r[i] * newSteepestDescent[i];\n            }\n\n            final double beta;\n            switch (updateFormula) {\n            case FLETCHER_REEVES:\n                beta = delta / deltaOld;\n                break;\n            case POLAK_RIBIERE:\n                double deltaMid = 0;\n                for (int i = 0; i < r.length; ++i) {\n                    deltaMid += r[i] * steepestDescent[i];\n                }\n                beta = (delta - deltaMid) / deltaOld;\n                break;\n            default:\n                // Should never happen.\n                throw new MathInternalError();\n            }\n            steepestDescent = newSteepestDescent;\n\n            // Compute conjugate search direction.\n            if (iter % n == 0 ||\n                beta < 0) {\n                // Break conjugation: reset search direction.\n                searchDirection = steepestDescent.clone();\n            } else {\n                // Compute new conjugate search direction.\n                for (int i = 0; i < n; ++i) {\n                    searchDirection[i] = steepestDescent[i] + beta * searchDirection[i];\n                }\n            }\n        }\n    }\n"
                        ]
                    }
                }
            },
            {
                "path": "src/main/java/org/apache/commons/math3/optim/nonlinear/scalar/noderiv/CMAESOptimizer.java",
                "start_loc": 367,
                "end_loc": 515,
                "buggy_function": "    @Override\n    protected PointValuePair doOptimize() {\n         // -------------------- Initialization --------------------------------\n        isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n        final FitnessFunction fitfun = new FitnessFunction();\n        final double[] guess = getStartPoint();\n        // number of objective variables/problem dimension\n        dimension = guess.length;\n        initializeCMA(guess);\n        iterations = 0;\n        double bestValue = fitfun.value(guess);\n        push(fitnessHistory, bestValue);\n        PointValuePair optimum\n            = new PointValuePair(getStartPoint(),\n                                 isMinimize ? bestValue : -bestValue);\n        PointValuePair lastResult = null;\n\n        // -------------------- Generation Loop --------------------------------\n\n        generationLoop:\n        for (iterations = 1; iterations <= maxIterations; iterations++) {\n\n            // Generate and evaluate lambda offspring\n            final RealMatrix arz = randn1(dimension, lambda);\n            final RealMatrix arx = zeros(dimension, lambda);\n            final double[] fitness = new double[lambda];\n            // generate random offspring\n            for (int k = 0; k < lambda; k++) {\n                RealMatrix arxk = null;\n                for (int i = 0; i < checkFeasableCount + 1; i++) {\n                    if (diagonalOnly <= 0) {\n                        arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k))\n                                         .scalarMultiply(sigma)); // m + sig * Normal(0,C)\n                    } else {\n                        arxk = xmean.add(times(diagD,arz.getColumnMatrix(k))\n                                         .scalarMultiply(sigma));\n                    }\n                    if (i >= checkFeasableCount ||\n                        fitfun.isFeasible(arxk.getColumn(0))) {\n                        break;\n                    }\n                    // regenerate random arguments for row\n                    arz.setColumn(k, randn(dimension));\n                }\n                copyColumn(arxk, 0, arx, k);\n                try {\n                    fitness[k] = fitfun.value(arx.getColumn(k)); // compute fitness\n                } catch (TooManyEvaluationsException e) {\n                    break generationLoop;\n                }\n            }\n            // Sort by fitness and compute weighted mean into xmean\n            final int[] arindex = sortedIndices(fitness);\n            // Calculate new xmean, this is selection and recombination\n            final RealMatrix xold = xmean; // for speed up of Eq. (2) and (3)\n            final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n            xmean = bestArx.multiply(weights);\n            final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n            final RealMatrix zmean = bestArz.multiply(weights);\n            final boolean hsig = updateEvolutionPaths(zmean, xold);\n            if (diagonalOnly <= 0) {\n                updateCovariance(hsig, bestArx, arz, arindex, xold);\n            } else {\n                updateCovarianceDiagonalOnly(hsig, bestArz);\n            }\n            // Adapt step size sigma - Eq. (5)\n            sigma *= Math.exp(Math.min(1, (normps/chiN - 1) * cs / damps));\n            final double bestFitness = fitness[arindex[0]];\n            final double worstFitness = fitness[arindex[arindex.length - 1]];\n            if (bestValue > bestFitness) {\n                bestValue = bestFitness;\n                lastResult = optimum;\n                optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)),\n                                             isMinimize ? bestFitness : -bestFitness);\n                if (getConvergenceChecker() != null &&\n                    lastResult != null) {\n                    if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n                        break generationLoop;\n                    }\n                }\n            }\n            // handle termination criteria\n            // Break, if fitness is good enough\n            if (stopFitness != 0) { // only if stopFitness is defined\n                if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n                    break generationLoop;\n                }\n            }\n            final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n            final double[] pcCol = pc.getColumn(0);\n            for (int i = 0; i < dimension; i++) {\n                if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n                    break;\n                }\n                if (i >= dimension - 1) {\n                    break generationLoop;\n                }\n            }\n            for (int i = 0; i < dimension; i++) {\n                if (sigma * sqrtDiagC[i] > stopTolUpX) {\n                    break generationLoop;\n                }\n            }\n            final double historyBest = min(fitnessHistory);\n            final double historyWorst = max(fitnessHistory);\n            if (iterations > 2 &&\n                Math.max(historyWorst, worstFitness) -\n                Math.min(historyBest, bestFitness) < stopTolFun) {\n                break generationLoop;\n            }\n            if (iterations > fitnessHistory.length &&\n                historyWorst - historyBest < stopTolHistFun) {\n                break generationLoop;\n            }\n            // condition number of the covariance matrix exceeds 1e14\n            if (max(diagD) / min(diagD) > 1e7) {\n                break generationLoop;\n            }\n            // user defined termination\n            if (getConvergenceChecker() != null) {\n                final PointValuePair current\n                    = new PointValuePair(bestArx.getColumn(0),\n                                         isMinimize ? bestFitness : -bestFitness);\n                if (lastResult != null &&\n                    getConvergenceChecker().converged(iterations, current, lastResult)) {\n                    break generationLoop;\n                    }\n                lastResult = current;\n            }\n            // Adjust step size in case of equal function values (flat fitness)\n            if (bestValue == fitness[arindex[(int)(0.1+lambda/4.)]]) {\n                sigma = sigma * Math.exp(0.2 + cs / damps);\n            }\n            if (iterations > 2 && Math.max(historyWorst, bestFitness) -\n                Math.min(historyBest, bestFitness) == 0) {\n                sigma = sigma * Math.exp(0.2 + cs / damps);\n            }\n            // store best in history\n            push(fitnessHistory,bestFitness);\n            fitfun.setValueRange(worstFitness-bestFitness);\n            if (generateStatistics) {\n                statisticsSigmaHistory.add(sigma);\n                statisticsFitnessHistory.add(bestFitness);\n                statisticsMeanHistory.add(xmean.transpose());\n                statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n            }\n        }\n        return optimum;\n    }\n",
                "fixed_function": "    @Override\n    protected PointValuePair doOptimize() {\n         // -------------------- Initialization --------------------------------\n        isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n        final FitnessFunction fitfun = new FitnessFunction();\n        final double[] guess = getStartPoint();\n        // number of objective variables/problem dimension\n        dimension = guess.length;\n        initializeCMA(guess);\n        iterations = 0;\n        double bestValue = fitfun.value(guess);\n        push(fitnessHistory, bestValue);\n        PointValuePair optimum\n            = new PointValuePair(getStartPoint(),\n                                 isMinimize ? bestValue : -bestValue);\n        PointValuePair lastResult = null;\n\n        // -------------------- Generation Loop --------------------------------\n\n        generationLoop:\n        for (iterations = 1; iterations <= maxIterations; iterations++) {\n            incrementIterationCount();\n\n            // Generate and evaluate lambda offspring\n            final RealMatrix arz = randn1(dimension, lambda);\n            final RealMatrix arx = zeros(dimension, lambda);\n            final double[] fitness = new double[lambda];\n            // generate random offspring\n            for (int k = 0; k < lambda; k++) {\n                RealMatrix arxk = null;\n                for (int i = 0; i < checkFeasableCount + 1; i++) {\n                    if (diagonalOnly <= 0) {\n                        arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k))\n                                         .scalarMultiply(sigma)); // m + sig * Normal(0,C)\n                    } else {\n                        arxk = xmean.add(times(diagD,arz.getColumnMatrix(k))\n                                         .scalarMultiply(sigma));\n                    }\n                    if (i >= checkFeasableCount ||\n                        fitfun.isFeasible(arxk.getColumn(0))) {\n                        break;\n                    }\n                    // regenerate random arguments for row\n                    arz.setColumn(k, randn(dimension));\n                }\n                copyColumn(arxk, 0, arx, k);\n                try {\n                    fitness[k] = fitfun.value(arx.getColumn(k)); // compute fitness\n                } catch (TooManyEvaluationsException e) {\n                    break generationLoop;\n                }\n            }\n            // Sort by fitness and compute weighted mean into xmean\n            final int[] arindex = sortedIndices(fitness);\n            // Calculate new xmean, this is selection and recombination\n            final RealMatrix xold = xmean; // for speed up of Eq. (2) and (3)\n            final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n            xmean = bestArx.multiply(weights);\n            final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n            final RealMatrix zmean = bestArz.multiply(weights);\n            final boolean hsig = updateEvolutionPaths(zmean, xold);\n            if (diagonalOnly <= 0) {\n                updateCovariance(hsig, bestArx, arz, arindex, xold);\n            } else {\n                updateCovarianceDiagonalOnly(hsig, bestArz);\n            }\n            // Adapt step size sigma - Eq. (5)\n            sigma *= Math.exp(Math.min(1, (normps/chiN - 1) * cs / damps));\n            final double bestFitness = fitness[arindex[0]];\n            final double worstFitness = fitness[arindex[arindex.length - 1]];\n            if (bestValue > bestFitness) {\n                bestValue = bestFitness;\n                lastResult = optimum;\n                optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)),\n                                             isMinimize ? bestFitness : -bestFitness);\n                if (getConvergenceChecker() != null &&\n                    lastResult != null) {\n                    if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n                        break generationLoop;\n                    }\n                }\n            }\n            // handle termination criteria\n            // Break, if fitness is good enough\n            if (stopFitness != 0) { // only if stopFitness is defined\n                if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n                    break generationLoop;\n                }\n            }\n            final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n            final double[] pcCol = pc.getColumn(0);\n            for (int i = 0; i < dimension; i++) {\n                if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n                    break;\n                }\n                if (i >= dimension - 1) {\n                    break generationLoop;\n                }\n            }\n            for (int i = 0; i < dimension; i++) {\n                if (sigma * sqrtDiagC[i] > stopTolUpX) {\n                    break generationLoop;\n                }\n            }\n            final double historyBest = min(fitnessHistory);\n            final double historyWorst = max(fitnessHistory);\n            if (iterations > 2 &&\n                Math.max(historyWorst, worstFitness) -\n                Math.min(historyBest, bestFitness) < stopTolFun) {\n                break generationLoop;\n            }\n            if (iterations > fitnessHistory.length &&\n                historyWorst - historyBest < stopTolHistFun) {\n                break generationLoop;\n            }\n            // condition number of the covariance matrix exceeds 1e14\n            if (max(diagD) / min(diagD) > 1e7) {\n                break generationLoop;\n            }\n            // user defined termination\n            if (getConvergenceChecker() != null) {\n                final PointValuePair current\n                    = new PointValuePair(bestArx.getColumn(0),\n                                         isMinimize ? bestFitness : -bestFitness);\n                if (lastResult != null &&\n                    getConvergenceChecker().converged(iterations, current, lastResult)) {\n                    break generationLoop;\n                    }\n                lastResult = current;\n            }\n            // Adjust step size in case of equal function values (flat fitness)\n            if (bestValue == fitness[arindex[(int)(0.1+lambda/4.)]]) {\n                sigma = sigma * Math.exp(0.2 + cs / damps);\n            }\n            if (iterations > 2 && Math.max(historyWorst, bestFitness) -\n                Math.min(historyBest, bestFitness) == 0) {\n                sigma = sigma * Math.exp(0.2 + cs / damps);\n            }\n            // store best in history\n            push(fitnessHistory,bestFitness);\n            fitfun.setValueRange(worstFitness-bestFitness);\n            if (generateStatistics) {\n                statisticsSigmaHistory.add(sigma);\n                statisticsFitnessHistory.add(bestFitness);\n                statisticsMeanHistory.add(xmean.transpose());\n                statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n            }\n        }\n        return optimum;\n    }\n",
                "comment": "    /** {@inheritDoc} */\n",
                "normalized_body": [
                    "@Override protected PointValuePair doOptimize() { isMinimize = getGoalType().equals(GoalType.MINIMIZE); final FitnessFunction fitfun = new FitnessFunction(); final double[] guess = getStartPoint(); dimension = guess.length; initializeCMA(guess); iterations = 0; double bestValue = fitfun.value(guess); push(fitnessHistory, bestValue); PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue); PointValuePair lastResult = null; generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) { final RealMatrix arz = randn1(dimension, lambda); final RealMatrix arx = zeros(dimension, lambda); final double[] fitness = new double[lambda]; for (int k = 0; k < lambda; k++) { RealMatrix arxk = null; for (int i = 0; i < checkFeasableCount + 1; i++) { if (diagonalOnly <= 0) { arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(sigma)); } else { arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma)); } if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) { break; } arz.setColumn(k, randn(dimension)); } copyColumn(arxk, 0, arx, k); try { fitness[k] = fitfun.value(arx.getColumn(k)); } catch (TooManyEvaluationsException e) { break generationLoop; } } final int[] arindex = sortedIndices(fitness); final RealMatrix xold = xmean; final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu)); xmean = bestArx.multiply(weights); final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu)); final RealMatrix zmean = bestArz.multiply(weights); final boolean hsig = updateEvolutionPaths(zmean, xold); if (diagonalOnly <= 0) { updateCovariance(hsig, bestArx, arz, arindex, xold); } else { updateCovarianceDiagonalOnly(hsig, bestArz); } sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps)); final double bestFitness = fitness[arindex[0]]; final double worstFitness = fitness[arindex[arindex.length - 1]]; if (bestValue > bestFitness) { bestValue = bestFitness; lastResult = optimum; optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness); if (getConvergenceChecker() != null && lastResult != null) { if (getConvergenceChecker().converged(iterations, optimum, lastResult)) { break generationLoop; } } } if (stopFitness != 0) { if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) { break generationLoop; } } final double[] sqrtDiagC = sqrt(diagC).getColumn(0); final double[] pcCol = pc.getColumn(0); for (int i = 0; i < dimension; i++) { if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) { break; } if (i >= dimension - 1) { break generationLoop; } } for (int i = 0; i < dimension; i++) { if (sigma * sqrtDiagC[i] > stopTolUpX) { break generationLoop; } } final double historyBest = min(fitnessHistory); final double historyWorst = max(fitnessHistory); if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) { break generationLoop; } if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) { break generationLoop; } if (max(diagD) / min(diagD) > 1e7) { break generationLoop; } if (getConvergenceChecker() != null) { final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness); if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) { break generationLoop; } lastResult = current; } if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) { sigma = sigma * Math.exp(0.2 + cs / damps); } if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) { sigma = sigma * Math.exp(0.2 + cs / damps); } push(fitnessHistory, bestFitness); fitfun.setValueRange(worstFitness - bestFitness); if (generateStatistics) { statisticsSigmaHistory.add(sigma); statisticsFitnessHistory.add(bestFitness); statisticsMeanHistory.add(xmean.transpose()); statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5)); } } return optimum; }"
                ],
                "top_similar_methods": [
                    [
                        1.0000000794728596,
                        "@Override protected PointValuePair doOptimize() { isMinimize = getGoalType().equals(GoalType.MINIMIZE); final FitnessFunction fitfun = new FitnessFunction(); final double[] guess = getStartPoint(); dimension = guess.length; initializeCMA(guess); iterations = 0; double bestValue = fitfun.value(guess); push(fitnessHistory, bestValue); PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue); PointValuePair lastResult = null; generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) { final RealMatrix arz = randn1(dimension, lambda); final RealMatrix arx = zeros(dimension, lambda); final double[] fitness = new double[lambda]; for (int k = 0; k < lambda; k++) { RealMatrix arxk = null; for (int i = 0; i < checkFeasableCount + 1; i++) { if (diagonalOnly <= 0) { arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(sigma)); } else { arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma)); } if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) { break; } arz.setColumn(k, randn(dimension)); } copyColumn(arxk, 0, arx, k); try { fitness[k] = fitfun.value(arx.getColumn(k)); } catch (TooManyEvaluationsException e) { break generationLoop; } } final int[] arindex = sortedIndices(fitness); final RealMatrix xold = xmean; final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu)); xmean = bestArx.multiply(weights); final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu)); final RealMatrix zmean = bestArz.multiply(weights); final boolean hsig = updateEvolutionPaths(zmean, xold); if (diagonalOnly <= 0) { updateCovariance(hsig, bestArx, arz, arindex, xold); } else { updateCovarianceDiagonalOnly(hsig, bestArz); } sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps)); final double bestFitness = fitness[arindex[0]]; final double worstFitness = fitness[arindex[arindex.length - 1]]; if (bestValue > bestFitness) { bestValue = bestFitness; lastResult = optimum; optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness); if (getConvergenceChecker() != null && lastResult != null) { if (getConvergenceChecker().converged(iterations, optimum, lastResult)) { break generationLoop; } } } if (stopFitness != 0) { if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) { break generationLoop; } } final double[] sqrtDiagC = sqrt(diagC).getColumn(0); final double[] pcCol = pc.getColumn(0); for (int i = 0; i < dimension; i++) { if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) { break; } if (i >= dimension - 1) { break generationLoop; } } for (int i = 0; i < dimension; i++) { if (sigma * sqrtDiagC[i] > stopTolUpX) { break generationLoop; } } final double historyBest = min(fitnessHistory); final double historyWorst = max(fitnessHistory); if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) { break generationLoop; } if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) { break generationLoop; } if (max(diagD) / min(diagD) > 1e7) { break generationLoop; } if (getConvergenceChecker() != null) { final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness); if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) { break generationLoop; } lastResult = current; } if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) { sigma = sigma * Math.exp(0.2 + cs / damps); } if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) { sigma = sigma * Math.exp(0.2 + cs / damps); } push(fitnessHistory, bestFitness); fitfun.setValueRange(worstFitness - bestFitness); if (generateStatistics) { statisticsSigmaHistory.add(sigma); statisticsFitnessHistory.add(bestFitness); statisticsMeanHistory.add(xmean.transpose()); statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5)); } } return optimum; }"
                    ],
                    [
                        0.9997056722640991,
                        "@Override protected PointValuePair doOptimize() { checkParameters(); isMinimize = getGoalType().equals(GoalType.MINIMIZE); final FitnessFunction fitfun = new FitnessFunction(); final double[] guess = getStartPoint(); dimension = guess.length; initializeCMA(guess); iterations = 0; double bestValue = fitfun.value(guess); push(fitnessHistory, bestValue); PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue); PointValuePair lastResult = null; generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) { final RealMatrix arz = randn1(dimension, lambda); final RealMatrix arx = zeros(dimension, lambda); final double[] fitness = new double[lambda]; for (int k = 0; k < lambda; k++) { RealMatrix arxk = null; for (int i = 0; i < checkFeasableCount + 1; i++) { if (diagonalOnly <= 0) { arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(sigma)); } else { arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma)); } if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) { break; } arz.setColumn(k, randn(dimension)); } copyColumn(arxk, 0, arx, k); try { fitness[k] = fitfun.value(arx.getColumn(k)); } catch (TooManyEvaluationsException e) { break generationLoop; } } final int[] arindex = sortedIndices(fitness); final RealMatrix xold = xmean; final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu)); xmean = bestArx.multiply(weights); final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu)); final RealMatrix zmean = bestArz.multiply(weights); final boolean hsig = updateEvolutionPaths(zmean, xold); if (diagonalOnly <= 0) { updateCovariance(hsig, bestArx, arz, arindex, xold); } else { updateCovarianceDiagonalOnly(hsig, bestArz); } sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps)); final double bestFitness = fitness[arindex[0]]; final double worstFitness = fitness[arindex[arindex.length - 1]]; if (bestValue > bestFitness) { bestValue = bestFitness; lastResult = optimum; optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness); if (getConvergenceChecker() != null && lastResult != null) { if (getConvergenceChecker().converged(iterations, optimum, lastResult)) { break generationLoop; } } } if (stopFitness != 0) { if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) { break generationLoop; } } final double[] sqrtDiagC = sqrt(diagC).getColumn(0); final double[] pcCol = pc.getColumn(0); for (int i = 0; i < dimension; i++) { if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) { break; } if (i >= dimension - 1) { break generationLoop; } } for (int i = 0; i < dimension; i++) { if (sigma * sqrtDiagC[i] > stopTolUpX) { break generationLoop; } } final double historyBest = min(fitnessHistory); final double historyWorst = max(fitnessHistory); if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) { break generationLoop; } if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) { break generationLoop; } if (max(diagD) / min(diagD) > 1e7) { break generationLoop; } if (getConvergenceChecker() != null) { final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness); if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) { break generationLoop; } lastResult = current; } if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) { sigma = sigma * Math.exp(0.2 + cs / damps); } if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) { sigma = sigma * Math.exp(0.2 + cs / damps); } push(fitnessHistory, bestFitness); fitfun.setValueRange(worstFitness - bestFitness); if (generateStatistics) { statisticsSigmaHistory.add(sigma); statisticsFitnessHistory.add(bestFitness); statisticsMeanHistory.add(xmean.transpose()); statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5)); } } return optimum; }"
                    ],
                    [
                        0.991172711054484,
                        "@Override public void integrate(final ExpandableStatefulODE equations, final double t) throws NumberIsTooSmallException, DimensionMismatchException, MaxCountExceededException, NoBracketingException { sanityChecks(equations, t); setEquations(equations); final boolean forward = t > equations.getTime(); final double[] y0 = equations.getCompleteState(); final double[] y = y0.clone(); final double[] yDot0 = new double[y.length]; final double[] y1 = new double[y.length]; final double[] yTmp = new double[y.length]; final double[] yTmpDot = new double[y.length]; final double[][] diagonal = new double[sequence.length - 1][]; final double[][] y1Diag = new double[sequence.length - 1][]; for (int k = 0; k < sequence.length - 1; ++k) { diagonal[k] = new double[y.length]; y1Diag[k] = new double[y.length]; } final double[][][] fk = new double[sequence.length][][]; for (int k = 0; k < sequence.length; ++k) { fk[k] = new double[sequence[k] + 1][]; fk[k][0] = yDot0; for (int l = 0; l < sequence[k]; ++l) { fk[k][l + 1] = new double[y0.length]; } } if (y != y0) { System.arraycopy(y0, 0, y, 0, y0.length); } final double[] yDot1 = new double[y0.length]; final double[][] yMidDots = new double[1 + 2 * sequence.length][y0.length]; final double[] scale = new double[mainSetDimension]; rescale(y, y, scale); final double tol = (vecRelativeTolerance == null) ? scalRelativeTolerance : vecRelativeTolerance[0]; final double log10R = FastMath.log10(FastMath.max(1.0e-10, tol)); int targetIter = FastMath.max(1, FastMath.min(sequence.length - 2, (int) FastMath.floor(0.5 - 0.6 * log10R))); final AbstractStepInterpolator interpolator = new GraggBulirschStoerStepInterpolator(y, yDot0, y1, yDot1, yMidDots, forward, equations.getPrimaryMapper(), equations.getSecondaryMappers()); interpolator.storeTime(equations.getTime()); stepStart = equations.getTime(); double hNew = 0; double maxError = Double.MAX_VALUE; boolean previousRejected = false; boolean firstTime = true; boolean newStep = true; boolean firstStepAlreadyComputed = false; initIntegration(equations.getTime(), y0, t); costPerTimeUnit[0] = 0; isLastStep = false; do { double error; boolean reject = false; if (newStep) { interpolator.shift(); if (!firstStepAlreadyComputed) { computeDerivatives(stepStart, y, yDot0); } if (firstTime) { hNew = initializeStep(forward, 2 * targetIter + 1, scale, stepStart, y, yDot0, yTmp, yTmpDot); } newStep = false; } stepSize = hNew; if ((forward && (stepStart + stepSize > t)) || ((!forward) && (stepStart + stepSize < t))) { stepSize = t - stepStart; } final double nextT = stepStart + stepSize; isLastStep = forward ? (nextT >= t) : (nextT <= t); int k = -1; for (boolean loop = true; loop; ) { ++k; if (!tryStep(stepStart, y, stepSize, k, scale, fk[k], (k == 0) ? yMidDots[0] : diagonal[k - 1], (k == 0) ? y1 : y1Diag[k - 1], yTmp)) { hNew = FastMath.abs(filterStep(stepSize * stabilityReduction, forward, false)); reject = true; loop = false; } else { if (k > 0) { extrapolate(0, k, y1Diag, y1); rescale(y, y1, scale); error = 0; for (int j = 0; j < mainSetDimension; ++j) { final double e = FastMath.abs(y1[j] - y1Diag[0][j]) / scale[j]; error += e * e; } error = FastMath.sqrt(error / mainSetDimension); if ((error > 1.0e15) || ((k > 1) && (error > maxError))) { hNew = FastMath.abs(filterStep(stepSize * stabilityReduction, forward, false)); reject = true; loop = false; } else { maxError = FastMath.max(4 * error, 1.0); final double exp = 1.0 / (2 * k + 1); double fac = stepControl2 / FastMath.pow(error / stepControl1, exp); final double pow = FastMath.pow(stepControl3, exp); fac = FastMath.max(pow / stepControl4, FastMath.min(1 / pow, fac)); optimalStep[k] = FastMath.abs(filterStep(stepSize * fac, forward, true)); costPerTimeUnit[k] = costPerStep[k] / optimalStep[k]; switch(k - targetIter) { case -1: if ((targetIter > 1) && !previousRejected) { if (error <= 1.0) { loop = false; } else { final double ratio = ((double) sequence[targetIter] * sequence[targetIter + 1]) / (sequence[0] * sequence[0]); if (error > ratio * ratio) { reject = true; loop = false; targetIter = k; if ((targetIter > 1) && (costPerTimeUnit[targetIter - 1] < orderControl1 * costPerTimeUnit[targetIter])) { --targetIter; } hNew = optimalStep[targetIter]; } } } break; case 0: if (error <= 1.0) { loop = false; } else { final double ratio = ((double) sequence[k + 1]) / sequence[0]; if (error > ratio * ratio) { reject = true; loop = false; if ((targetIter > 1) && (costPerTimeUnit[targetIter - 1] < orderControl1 * costPerTimeUnit[targetIter])) { --targetIter; } hNew = optimalStep[targetIter]; } } break; case 1: if (error > 1.0) { reject = true; if ((targetIter > 1) && (costPerTimeUnit[targetIter - 1] < orderControl1 * costPerTimeUnit[targetIter])) { --targetIter; } hNew = optimalStep[targetIter]; } loop = false; break; default: if ((firstTime || isLastStep) && (error <= 1.0)) { loop = false; } break; } } } } } if (!reject) { computeDerivatives(stepStart + stepSize, y1, yDot1); } double hInt = getMaxStep(); if (!reject) { for (int j = 1; j <= k; ++j) { extrapolate(0, j, diagonal, yMidDots[0]); } final int mu = 2 * k - mudif + 3; for (int l = 0; l < mu; ++l) { final int l2 = l / 2; double factor = FastMath.pow(0.5 * sequence[l2], l); int middleIndex = fk[l2].length / 2; for (int i = 0; i < y0.length; ++i) { yMidDots[l + 1][i] = factor * fk[l2][middleIndex + l][i]; } for (int j = 1; j <= k - l2; ++j) { factor = FastMath.pow(0.5 * sequence[j + l2], l); middleIndex = fk[l2 + j].length / 2; for (int i = 0; i < y0.length; ++i) { diagonal[j - 1][i] = factor * fk[l2 + j][middleIndex + l][i]; } extrapolate(l2, j, diagonal, yMidDots[l + 1]); } for (int i = 0; i < y0.length; ++i) { yMidDots[l + 1][i] *= stepSize; } for (int j = (l + 1) / 2; j <= k; ++j) { for (int m = fk[j].length - 1; m >= 2 * (l + 1); --m) { for (int i = 0; i < y0.length; ++i) { fk[j][m][i] -= fk[j][m - 2][i]; } } } } if (mu >= 0) { final GraggBulirschStoerStepInterpolator gbsInterpolator = (GraggBulirschStoerStepInterpolator) interpolator; gbsInterpolator.computeCoefficients(mu, stepSize); if (useInterpolationError) { final double interpError = gbsInterpolator.estimateError(scale); hInt = FastMath.abs(stepSize / FastMath.max(FastMath.pow(interpError, 1.0 / (mu + 4)), 0.01)); if (interpError > 10.0) { hNew = hInt; reject = true; } } } } if (!reject) { interpolator.storeTime(stepStart + stepSize); stepStart = acceptStep(interpolator, y1, yDot1, t); interpolator.storeTime(stepStart); System.arraycopy(y1, 0, y, 0, y0.length); System.arraycopy(yDot1, 0, yDot0, 0, y0.length); firstStepAlreadyComputed = true; int optimalIter; if (k == 1) { optimalIter = 2; if (previousRejected) { optimalIter = 1; } } else if (k <= targetIter) { optimalIter = k; if (costPerTimeUnit[k - 1] < orderControl1 * costPerTimeUnit[k]) { optimalIter = k - 1; } else if (costPerTimeUnit[k] < orderControl2 * costPerTimeUnit[k - 1]) { optimalIter = FastMath.min(k + 1, sequence.length - 2); } } else { optimalIter = k - 1; if ((k > 2) && (costPerTimeUnit[k - 2] < orderControl1 * costPerTimeUnit[k - 1])) { optimalIter = k - 2; } if (costPerTimeUnit[k] < orderControl2 * costPerTimeUnit[optimalIter]) { optimalIter = FastMath.min(k, sequence.length - 2); } } if (previousRejected) { targetIter = FastMath.min(optimalIter, k); hNew = FastMath.min(FastMath.abs(stepSize), optimalStep[targetIter]); } else { if (optimalIter <= k) { hNew = optimalStep[optimalIter]; } else { if ((k < targetIter) && (costPerTimeUnit[k] < orderControl2 * costPerTimeUnit[k - 1])) { hNew = filterStep(optimalStep[k] * costPerStep[optimalIter + 1] / costPerStep[k], forward, false); } else { hNew = filterStep(optimalStep[k] * costPerStep[optimalIter] / costPerStep[k], forward, false); } } targetIter = optimalIter; } newStep = true; } hNew = FastMath.min(hNew, hInt); if (!forward) { hNew = -hNew; } firstTime = false; if (reject) { isLastStep = false; previousRejected = true; } else { previousRejected = false; } } while (!isLastStep); equations.setTime(stepStart); equations.setCompleteState(y); resetInternalState(); }"
                    ],
                    [
                        0.9909623463948568,
                        "@Override public void integrate(final ExpandableStatefulODE equations, final double t) throws NumberIsTooSmallException, DimensionMismatchException, MaxCountExceededException, NoBracketingException { sanityChecks(equations, t); setEquations(equations); final boolean forward = t > equations.getTime(); final double[] y0 = equations.getCompleteState(); final double[] y = y0.clone(); final double[] yDot = new double[y.length]; final NordsieckStepInterpolator interpolator = new NordsieckStepInterpolator(); interpolator.reinitialize(y, forward, equations.getPrimaryMapper(), equations.getSecondaryMappers()); initIntegration(equations.getTime(), y0, t); start(equations.getTime(), y, t); interpolator.reinitialize(stepStart, stepSize, scaled, nordsieck); interpolator.storeTime(stepStart); final int lastRow = nordsieck.getRowDimension() - 1; double hNew = stepSize; interpolator.rescale(hNew); isLastStep = false; do { double error = 10; while (error >= 1.0) { stepSize = hNew; error = 0; for (int i = 0; i < mainSetDimension; ++i) { final double yScale = FastMath.abs(y[i]); final double tol = (vecAbsoluteTolerance == null) ? (scalAbsoluteTolerance + scalRelativeTolerance * yScale) : (vecAbsoluteTolerance[i] + vecRelativeTolerance[i] * yScale); final double ratio = nordsieck.getEntry(lastRow, i) / tol; error += ratio * ratio; } error = FastMath.sqrt(error / mainSetDimension); if (error >= 1.0) { final double factor = computeStepGrowShrinkFactor(error); hNew = filterStep(stepSize * factor, forward, false); interpolator.rescale(hNew); } } final double stepEnd = stepStart + stepSize; interpolator.shift(); interpolator.setInterpolatedTime(stepEnd); System.arraycopy(interpolator.getInterpolatedState(), 0, y, 0, y0.length); computeDerivatives(stepEnd, y, yDot); final double[] predictedScaled = new double[y0.length]; for (int j = 0; j < y0.length; ++j) { predictedScaled[j] = stepSize * yDot[j]; } final Array2DRowRealMatrix nordsieckTmp = updateHighOrderDerivativesPhase1(nordsieck); updateHighOrderDerivativesPhase2(scaled, predictedScaled, nordsieckTmp); interpolator.reinitialize(stepEnd, stepSize, predictedScaled, nordsieckTmp); interpolator.storeTime(stepEnd); stepStart = acceptStep(interpolator, y, yDot, t); scaled = predictedScaled; nordsieck = nordsieckTmp; interpolator.reinitialize(stepEnd, stepSize, scaled, nordsieck); if (!isLastStep) { interpolator.storeTime(stepStart); if (resetOccurred) { start(stepStart, y, t); interpolator.reinitialize(stepStart, stepSize, scaled, nordsieck); } final double factor = computeStepGrowShrinkFactor(error); final double scaledH = stepSize * factor; final double nextT = stepStart + scaledH; final boolean nextIsLast = forward ? (nextT >= t) : (nextT <= t); hNew = filterStep(scaledH, forward, nextIsLast); final double filteredNextT = stepStart + hNew; final boolean filteredNextIsLast = forward ? (filteredNextT >= t) : (filteredNextT <= t); if (filteredNextIsLast) { hNew = t - stepStart; } interpolator.rescale(hNew); } } while (!isLastStep); equations.setTime(stepStart); equations.setCompleteState(y); resetInternalState(); }"
                    ],
                    [
                        0.9895931084950765,
                        "private double[] trsbox(double delta, ArrayRealVector gnew, ArrayRealVector xbdi, ArrayRealVector s, ArrayRealVector hs, ArrayRealVector hred) { printMethod(); final int n = currentBest.getDimension(); final int npt = numberOfInterpolationPoints; double dsq = Double.NaN; double crvmin = Double.NaN; double ds; int iu; double dhd, dhs, cth, shs, sth, ssq, beta = 0, sdec, blen; int iact = -1; int nact = 0; double angt = 0, qred; int isav; double temp = 0, xsav = 0, xsum = 0, angbd = 0, dredg = 0, sredg = 0; int iterc; double resid = 0, delsq = 0, ggsav = 0, tempa = 0, tempb = 0, redmax = 0, dredsq = 0, redsav = 0, gredsq = 0, rednew = 0; int itcsav = 0; double rdprev = 0, rdnext = 0, stplen = 0, stepsq = 0; int itermax = 0; iterc = 0; nact = 0; for (int i = 0; i < n; i++) { xbdi.setEntry(i, ZERO); if (trustRegionCenterOffset.getEntry(i) <= lowerDifference.getEntry(i)) { if (gradientAtTrustRegionCenter.getEntry(i) >= ZERO) { xbdi.setEntry(i, MINUS_ONE); } } else if (trustRegionCenterOffset.getEntry(i) >= upperDifference.getEntry(i)) { if (gradientAtTrustRegionCenter.getEntry(i) <= ZERO) { xbdi.setEntry(i, ONE); } } if (xbdi.getEntry(i) != ZERO) { ++nact; } trialStepPoint.setEntry(i, ZERO); gnew.setEntry(i, gradientAtTrustRegionCenter.getEntry(i)); } delsq = delta * delta; qred = ZERO; crvmin = MINUS_ONE; int state = 20; for (; ; ) { switch(state) { case 20: { printState(20); beta = ZERO; } case 30: { printState(30); stepsq = ZERO; for (int i = 0; i < n; i++) { if (xbdi.getEntry(i) != ZERO) { s.setEntry(i, ZERO); } else if (beta == ZERO) { s.setEntry(i, -gnew.getEntry(i)); } else { s.setEntry(i, beta * s.getEntry(i) - gnew.getEntry(i)); } final double d1 = s.getEntry(i); stepsq += d1 * d1; } if (stepsq == ZERO) { state = 190; break; } if (beta == ZERO) { gredsq = stepsq; itermax = iterc + n - nact; } if (gredsq * delsq <= qred * 1e-4 * qred) { state = 190; break; } state = 210; break; } case 50: { printState(50); resid = delsq; ds = ZERO; shs = ZERO; for (int i = 0; i < n; i++) { if (xbdi.getEntry(i) == ZERO) { final double d1 = trialStepPoint.getEntry(i); resid -= d1 * d1; ds += s.getEntry(i) * trialStepPoint.getEntry(i); shs += s.getEntry(i) * hs.getEntry(i); } } if (resid <= ZERO) { state = 90; break; } temp = Math.sqrt(stepsq * resid + ds * ds); if (ds < ZERO) { blen = (temp - ds) / stepsq; } else { blen = resid / (temp + ds); } stplen = blen; if (shs > ZERO) { stplen = Math.min(blen, gredsq / shs); } iact = -1; for (int i = 0; i < n; i++) { if (s.getEntry(i) != ZERO) { xsum = trustRegionCenterOffset.getEntry(i) + trialStepPoint.getEntry(i); if (s.getEntry(i) > ZERO) { temp = (upperDifference.getEntry(i) - xsum) / s.getEntry(i); } else { temp = (lowerDifference.getEntry(i) - xsum) / s.getEntry(i); } if (temp < stplen) { stplen = temp; iact = i; } } } sdec = ZERO; if (stplen > ZERO) { ++iterc; temp = shs / stepsq; if (iact == -1 && temp > ZERO) { crvmin = Math.min(crvmin, temp); if (crvmin == MINUS_ONE) { crvmin = temp; } } ggsav = gredsq; gredsq = ZERO; for (int i = 0; i < n; i++) { gnew.setEntry(i, gnew.getEntry(i) + stplen * hs.getEntry(i)); if (xbdi.getEntry(i) == ZERO) { final double d1 = gnew.getEntry(i); gredsq += d1 * d1; } trialStepPoint.setEntry(i, trialStepPoint.getEntry(i) + stplen * s.getEntry(i)); } final double d1 = stplen * (ggsav - HALF * stplen * shs); sdec = Math.max(d1, ZERO); qred += sdec; } if (iact >= 0) { ++nact; xbdi.setEntry(iact, ONE); if (s.getEntry(iact) < ZERO) { xbdi.setEntry(iact, MINUS_ONE); } final double d1 = trialStepPoint.getEntry(iact); delsq -= d1 * d1; if (delsq <= ZERO) { state = 190; break; } state = 20; break; } if (stplen < blen) { if (iterc == itermax) { state = 190; break; } if (sdec <= qred * .01) { state = 190; break; } beta = gredsq / ggsav; state = 30; break; } } case 90: { printState(90); crvmin = ZERO; } case 100: { printState(100); if (nact >= n - 1) { state = 190; break; } dredsq = ZERO; dredg = ZERO; gredsq = ZERO; for (int i = 0; i < n; i++) { if (xbdi.getEntry(i) == ZERO) { double d1 = trialStepPoint.getEntry(i); dredsq += d1 * d1; dredg += trialStepPoint.getEntry(i) * gnew.getEntry(i); d1 = gnew.getEntry(i); gredsq += d1 * d1; s.setEntry(i, trialStepPoint.getEntry(i)); } else { s.setEntry(i, ZERO); } } itcsav = iterc; state = 210; break; } case 120: { printState(120); ++iterc; temp = gredsq * dredsq - dredg * dredg; if (temp <= qred * 1e-4 * qred) { state = 190; break; } temp = Math.sqrt(temp); for (int i = 0; i < n; i++) { if (xbdi.getEntry(i) == ZERO) { s.setEntry(i, (dredg * trialStepPoint.getEntry(i) - dredsq * gnew.getEntry(i)) / temp); } else { s.setEntry(i, ZERO); } } sredg = -temp; angbd = ONE; iact = -1; for (int i = 0; i < n; i++) { if (xbdi.getEntry(i) == ZERO) { tempa = trustRegionCenterOffset.getEntry(i) + trialStepPoint.getEntry(i) - lowerDifference.getEntry(i); tempb = upperDifference.getEntry(i) - trustRegionCenterOffset.getEntry(i) - trialStepPoint.getEntry(i); if (tempa <= ZERO) { ++nact; xbdi.setEntry(i, MINUS_ONE); state = 100; break; } else if (tempb <= ZERO) { ++nact; xbdi.setEntry(i, ONE); state = 100; break; } double d1 = trialStepPoint.getEntry(i); double d2 = s.getEntry(i); ssq = d1 * d1 + d2 * d2; d1 = trustRegionCenterOffset.getEntry(i) - lowerDifference.getEntry(i); temp = ssq - d1 * d1; if (temp > ZERO) { temp = Math.sqrt(temp) - s.getEntry(i); if (angbd * temp > tempa) { angbd = tempa / temp; iact = i; xsav = MINUS_ONE; } } d1 = upperDifference.getEntry(i) - trustRegionCenterOffset.getEntry(i); temp = ssq - d1 * d1; if (temp > ZERO) { temp = Math.sqrt(temp) + s.getEntry(i); if (angbd * temp > tempb) { angbd = tempb / temp; iact = i; xsav = ONE; } } } } state = 210; break; } case 150: { printState(150); shs = ZERO; dhs = ZERO; dhd = ZERO; for (int i = 0; i < n; i++) { if (xbdi.getEntry(i) == ZERO) { shs += s.getEntry(i) * hs.getEntry(i); dhs += trialStepPoint.getEntry(i) * hs.getEntry(i); dhd += trialStepPoint.getEntry(i) * hred.getEntry(i); } } redmax = ZERO; isav = -1; redsav = ZERO; iu = (int) (angbd * 17. + 3.1); for (int i = 0; i < iu; i++) { angt = angbd * i / iu; sth = (angt + angt) / (ONE + angt * angt); temp = shs + angt * (angt * dhd - dhs - dhs); rednew = sth * (angt * dredg - sredg - HALF * sth * temp); if (rednew > redmax) { redmax = rednew; isav = i; rdprev = redsav; } else if (i == isav + 1) { rdnext = rednew; } redsav = rednew; } if (isav < 0) { state = 190; break; } if (isav < iu) { temp = (rdnext - rdprev) / (redmax + redmax - rdprev - rdnext); angt = angbd * (isav + HALF * temp) / iu; } cth = (ONE - angt * angt) / (ONE + angt * angt); sth = (angt + angt) / (ONE + angt * angt); temp = shs + angt * (angt * dhd - dhs - dhs); sdec = sth * (angt * dredg - sredg - HALF * sth * temp); if (sdec <= ZERO) { state = 190; break; } dredg = ZERO; gredsq = ZERO; for (int i = 0; i < n; i++) { gnew.setEntry(i, gnew.getEntry(i) + (cth - ONE) * hred.getEntry(i) + sth * hs.getEntry(i)); if (xbdi.getEntry(i) == ZERO) { trialStepPoint.setEntry(i, cth * trialStepPoint.getEntry(i) + sth * s.getEntry(i)); dredg += trialStepPoint.getEntry(i) * gnew.getEntry(i); final double d1 = gnew.getEntry(i); gredsq += d1 * d1; } hred.setEntry(i, cth * hred.getEntry(i) + sth * hs.getEntry(i)); } qred += sdec; if (iact >= 0 && isav == iu) { ++nact; xbdi.setEntry(iact, xsav); state = 100; break; } if (sdec > qred * .01) { state = 120; break; } } case 190: { printState(190); dsq = ZERO; for (int i = 0; i < n; i++) { final double min = Math.min(trustRegionCenterOffset.getEntry(i) + trialStepPoint.getEntry(i), upperDifference.getEntry(i)); newPoint.setEntry(i, Math.max(min, lowerDifference.getEntry(i))); if (xbdi.getEntry(i) == MINUS_ONE) { newPoint.setEntry(i, lowerDifference.getEntry(i)); } if (xbdi.getEntry(i) == ONE) { newPoint.setEntry(i, upperDifference.getEntry(i)); } trialStepPoint.setEntry(i, newPoint.getEntry(i) - trustRegionCenterOffset.getEntry(i)); final double d1 = trialStepPoint.getEntry(i); dsq += d1 * d1; } return new double[] { dsq, crvmin }; } case 210: { printState(210); int ih = 0; for (int j = 0; j < n; j++) { hs.setEntry(j, ZERO); for (int i = 0; i <= j; i++) { if (i < j) { hs.setEntry(j, hs.getEntry(j) + modelSecondDerivativesValues.getEntry(ih) * s.getEntry(i)); } hs.setEntry(i, hs.getEntry(i) + modelSecondDerivativesValues.getEntry(ih) * s.getEntry(j)); ih++; } } final RealVector tmp = interpolationPoints.operate(s).ebeMultiply(modelSecondDerivativesParameters); for (int k = 0; k < npt; k++) { if (modelSecondDerivativesParameters.getEntry(k) != ZERO) { for (int i = 0; i < n; i++) { hs.setEntry(i, hs.getEntry(i) + tmp.getEntry(k) * interpolationPoints.getEntry(k, i)); } } } if (crvmin != ZERO) { state = 50; break; } if (iterc > itcsav) { state = 150; break; } for (int i = 0; i < n; i++) { hred.setEntry(i, hs.getEntry(i)); } state = 120; break; } default: { throw new MathIllegalStateException(LocalizedFormats.SIMPLE_MESSAGE, \\\"trsbox\\\"); } } } }"
                    ],
                    [
                        0.9895931084950765,
                        "private double[] trsbox(double delta, ArrayRealVector gnew, ArrayRealVector xbdi, ArrayRealVector s, ArrayRealVector hs, ArrayRealVector hred) { printMethod(); final int n = currentBest.getDimension(); final int npt = numberOfInterpolationPoints; double dsq = Double.NaN; double crvmin = Double.NaN; double ds; int iu; double dhd, dhs, cth, shs, sth, ssq, beta = 0, sdec, blen; int iact = -1; int nact = 0; double angt = 0, qred; int isav; double temp = 0, xsav = 0, xsum = 0, angbd = 0, dredg = 0, sredg = 0; int iterc; double resid = 0, delsq = 0, ggsav = 0, tempa = 0, tempb = 0, redmax = 0, dredsq = 0, redsav = 0, gredsq = 0, rednew = 0; int itcsav = 0; double rdprev = 0, rdnext = 0, stplen = 0, stepsq = 0; int itermax = 0; iterc = 0; nact = 0; for (int i = 0; i < n; i++) { xbdi.setEntry(i, ZERO); if (trustRegionCenterOffset.getEntry(i) <= lowerDifference.getEntry(i)) { if (gradientAtTrustRegionCenter.getEntry(i) >= ZERO) { xbdi.setEntry(i, MINUS_ONE); } } else if (trustRegionCenterOffset.getEntry(i) >= upperDifference.getEntry(i)) { if (gradientAtTrustRegionCenter.getEntry(i) <= ZERO) { xbdi.setEntry(i, ONE); } } if (xbdi.getEntry(i) != ZERO) { ++nact; } trialStepPoint.setEntry(i, ZERO); gnew.setEntry(i, gradientAtTrustRegionCenter.getEntry(i)); } delsq = delta * delta; qred = ZERO; crvmin = MINUS_ONE; int state = 20; for (; ; ) { switch(state) { case 20: { printState(20); beta = ZERO; } case 30: { printState(30); stepsq = ZERO; for (int i = 0; i < n; i++) { if (xbdi.getEntry(i) != ZERO) { s.setEntry(i, ZERO); } else if (beta == ZERO) { s.setEntry(i, -gnew.getEntry(i)); } else { s.setEntry(i, beta * s.getEntry(i) - gnew.getEntry(i)); } final double d1 = s.getEntry(i); stepsq += d1 * d1; } if (stepsq == ZERO) { state = 190; break; } if (beta == ZERO) { gredsq = stepsq; itermax = iterc + n - nact; } if (gredsq * delsq <= qred * 1e-4 * qred) { state = 190; break; } state = 210; break; } case 50: { printState(50); resid = delsq; ds = ZERO; shs = ZERO; for (int i = 0; i < n; i++) { if (xbdi.getEntry(i) == ZERO) { final double d1 = trialStepPoint.getEntry(i); resid -= d1 * d1; ds += s.getEntry(i) * trialStepPoint.getEntry(i); shs += s.getEntry(i) * hs.getEntry(i); } } if (resid <= ZERO) { state = 90; break; } temp = Math.sqrt(stepsq * resid + ds * ds); if (ds < ZERO) { blen = (temp - ds) / stepsq; } else { blen = resid / (temp + ds); } stplen = blen; if (shs > ZERO) { stplen = Math.min(blen, gredsq / shs); } iact = -1; for (int i = 0; i < n; i++) { if (s.getEntry(i) != ZERO) { xsum = trustRegionCenterOffset.getEntry(i) + trialStepPoint.getEntry(i); if (s.getEntry(i) > ZERO) { temp = (upperDifference.getEntry(i) - xsum) / s.getEntry(i); } else { temp = (lowerDifference.getEntry(i) - xsum) / s.getEntry(i); } if (temp < stplen) { stplen = temp; iact = i; } } } sdec = ZERO; if (stplen > ZERO) { ++iterc; temp = shs / stepsq; if (iact == -1 && temp > ZERO) { crvmin = Math.min(crvmin, temp); if (crvmin == MINUS_ONE) { crvmin = temp; } } ggsav = gredsq; gredsq = ZERO; for (int i = 0; i < n; i++) { gnew.setEntry(i, gnew.getEntry(i) + stplen * hs.getEntry(i)); if (xbdi.getEntry(i) == ZERO) { final double d1 = gnew.getEntry(i); gredsq += d1 * d1; } trialStepPoint.setEntry(i, trialStepPoint.getEntry(i) + stplen * s.getEntry(i)); } final double d1 = stplen * (ggsav - HALF * stplen * shs); sdec = Math.max(d1, ZERO); qred += sdec; } if (iact >= 0) { ++nact; xbdi.setEntry(iact, ONE); if (s.getEntry(iact) < ZERO) { xbdi.setEntry(iact, MINUS_ONE); } final double d1 = trialStepPoint.getEntry(iact); delsq -= d1 * d1; if (delsq <= ZERO) { state = 190; break; } state = 20; break; } if (stplen < blen) { if (iterc == itermax) { state = 190; break; } if (sdec <= qred * .01) { state = 190; break; } beta = gredsq / ggsav; state = 30; break; } } case 90: { printState(90); crvmin = ZERO; } case 100: { printState(100); if (nact >= n - 1) { state = 190; break; } dredsq = ZERO; dredg = ZERO; gredsq = ZERO; for (int i = 0; i < n; i++) { if (xbdi.getEntry(i) == ZERO) { double d1 = trialStepPoint.getEntry(i); dredsq += d1 * d1; dredg += trialStepPoint.getEntry(i) * gnew.getEntry(i); d1 = gnew.getEntry(i); gredsq += d1 * d1; s.setEntry(i, trialStepPoint.getEntry(i)); } else { s.setEntry(i, ZERO); } } itcsav = iterc; state = 210; break; } case 120: { printState(120); ++iterc; temp = gredsq * dredsq - dredg * dredg; if (temp <= qred * 1e-4 * qred) { state = 190; break; } temp = Math.sqrt(temp); for (int i = 0; i < n; i++) { if (xbdi.getEntry(i) == ZERO) { s.setEntry(i, (dredg * trialStepPoint.getEntry(i) - dredsq * gnew.getEntry(i)) / temp); } else { s.setEntry(i, ZERO); } } sredg = -temp; angbd = ONE; iact = -1; for (int i = 0; i < n; i++) { if (xbdi.getEntry(i) == ZERO) { tempa = trustRegionCenterOffset.getEntry(i) + trialStepPoint.getEntry(i) - lowerDifference.getEntry(i); tempb = upperDifference.getEntry(i) - trustRegionCenterOffset.getEntry(i) - trialStepPoint.getEntry(i); if (tempa <= ZERO) { ++nact; xbdi.setEntry(i, MINUS_ONE); state = 100; break; } else if (tempb <= ZERO) { ++nact; xbdi.setEntry(i, ONE); state = 100; break; } double d1 = trialStepPoint.getEntry(i); double d2 = s.getEntry(i); ssq = d1 * d1 + d2 * d2; d1 = trustRegionCenterOffset.getEntry(i) - lowerDifference.getEntry(i); temp = ssq - d1 * d1; if (temp > ZERO) { temp = Math.sqrt(temp) - s.getEntry(i); if (angbd * temp > tempa) { angbd = tempa / temp; iact = i; xsav = MINUS_ONE; } } d1 = upperDifference.getEntry(i) - trustRegionCenterOffset.getEntry(i); temp = ssq - d1 * d1; if (temp > ZERO) { temp = Math.sqrt(temp) + s.getEntry(i); if (angbd * temp > tempb) { angbd = tempb / temp; iact = i; xsav = ONE; } } } } state = 210; break; } case 150: { printState(150); shs = ZERO; dhs = ZERO; dhd = ZERO; for (int i = 0; i < n; i++) { if (xbdi.getEntry(i) == ZERO) { shs += s.getEntry(i) * hs.getEntry(i); dhs += trialStepPoint.getEntry(i) * hs.getEntry(i); dhd += trialStepPoint.getEntry(i) * hred.getEntry(i); } } redmax = ZERO; isav = -1; redsav = ZERO; iu = (int) (angbd * 17. + 3.1); for (int i = 0; i < iu; i++) { angt = angbd * i / iu; sth = (angt + angt) / (ONE + angt * angt); temp = shs + angt * (angt * dhd - dhs - dhs); rednew = sth * (angt * dredg - sredg - HALF * sth * temp); if (rednew > redmax) { redmax = rednew; isav = i; rdprev = redsav; } else if (i == isav + 1) { rdnext = rednew; } redsav = rednew; } if (isav < 0) { state = 190; break; } if (isav < iu) { temp = (rdnext - rdprev) / (redmax + redmax - rdprev - rdnext); angt = angbd * (isav + HALF * temp) / iu; } cth = (ONE - angt * angt) / (ONE + angt * angt); sth = (angt + angt) / (ONE + angt * angt); temp = shs + angt * (angt * dhd - dhs - dhs); sdec = sth * (angt * dredg - sredg - HALF * sth * temp); if (sdec <= ZERO) { state = 190; break; } dredg = ZERO; gredsq = ZERO; for (int i = 0; i < n; i++) { gnew.setEntry(i, gnew.getEntry(i) + (cth - ONE) * hred.getEntry(i) + sth * hs.getEntry(i)); if (xbdi.getEntry(i) == ZERO) { trialStepPoint.setEntry(i, cth * trialStepPoint.getEntry(i) + sth * s.getEntry(i)); dredg += trialStepPoint.getEntry(i) * gnew.getEntry(i); final double d1 = gnew.getEntry(i); gredsq += d1 * d1; } hred.setEntry(i, cth * hred.getEntry(i) + sth * hs.getEntry(i)); } qred += sdec; if (iact >= 0 && isav == iu) { ++nact; xbdi.setEntry(iact, xsav); state = 100; break; } if (sdec > qred * .01) { state = 120; break; } } case 190: { printState(190); dsq = ZERO; for (int i = 0; i < n; i++) { final double min = Math.min(trustRegionCenterOffset.getEntry(i) + trialStepPoint.getEntry(i), upperDifference.getEntry(i)); newPoint.setEntry(i, Math.max(min, lowerDifference.getEntry(i))); if (xbdi.getEntry(i) == MINUS_ONE) { newPoint.setEntry(i, lowerDifference.getEntry(i)); } if (xbdi.getEntry(i) == ONE) { newPoint.setEntry(i, upperDifference.getEntry(i)); } trialStepPoint.setEntry(i, newPoint.getEntry(i) - trustRegionCenterOffset.getEntry(i)); final double d1 = trialStepPoint.getEntry(i); dsq += d1 * d1; } return new double[] { dsq, crvmin }; } case 210: { printState(210); int ih = 0; for (int j = 0; j < n; j++) { hs.setEntry(j, ZERO); for (int i = 0; i <= j; i++) { if (i < j) { hs.setEntry(j, hs.getEntry(j) + modelSecondDerivativesValues.getEntry(ih) * s.getEntry(i)); } hs.setEntry(i, hs.getEntry(i) + modelSecondDerivativesValues.getEntry(ih) * s.getEntry(j)); ih++; } } final RealVector tmp = interpolationPoints.operate(s).ebeMultiply(modelSecondDerivativesParameters); for (int k = 0; k < npt; k++) { if (modelSecondDerivativesParameters.getEntry(k) != ZERO) { for (int i = 0; i < n; i++) { hs.setEntry(i, hs.getEntry(i) + tmp.getEntry(k) * interpolationPoints.getEntry(k, i)); } } } if (crvmin != ZERO) { state = 50; break; } if (iterc > itcsav) { state = 150; break; } for (int i = 0; i < n; i++) { hred.setEntry(i, hs.getEntry(i)); } state = 120; break; } default: { throw new MathIllegalStateException(LocalizedFormats.SIMPLE_MESSAGE, \\\"trsbox\\\"); } } } }"
                    ],
                    [
                        0.9890182415644327,
                        "@Test public void testUnpreconditionedResidual() { final int n = 10; final int maxIterations = n; final RealLinearOperator a = new HilbertMatrix(n); final ConjugateGradient solver; solver = new ConjugateGradient(maxIterations, 1E-15, true); final RealVector r = new ArrayRealVector(n); final RealVector x = new ArrayRealVector(n); final IterationListener listener = new IterationListener() { public void terminationPerformed(final IterationEvent e) { } public void iterationStarted(final IterationEvent e) { } public void iterationPerformed(final IterationEvent e) { final IterativeLinearSolverEvent evt; evt = (IterativeLinearSolverEvent) e; RealVector v = evt.getResidual(); r.setSubVector(0, v); v = evt.getSolution(); x.setSubVector(0, v); } public void initializationPerformed(final IterationEvent e) { } }; solver.getIterationManager().addIterationListener(listener); final RealVector b = new ArrayRealVector(n); for (int j = 0; j < n; j++) { b.set(0.); b.setEntry(j, 1.); boolean caught = false; try { solver.solve(a, b); } catch (MaxCountExceededException e) { caught = true; final RealVector y = a.operate(x); for (int i = 0; i < n; i++) { final double actual = b.getEntry(i) - y.getEntry(i); final double expected = r.getEntry(i); final double delta = 1E-6 * Math.abs(expected); final String msg = String.format(\\\"column %d, residual %d\\\", i, j); Assert.assertEquals(msg, expected, actual, delta); } } Assert.assertTrue(\\\"MaxCountExceededException should have been caught\\\", caught); } }"
                    ],
                    [
                        0.9886561433474222,
                        "public Dfp solve(final int maxEval, final UnivariateDfpFunction f, final Dfp min, final Dfp max, final Dfp startValue, final AllowedSolution allowedSolution) throws NullArgumentException, NoBracketingException { MathUtils.checkNotNull(f); evaluations.setMaximalCount(maxEval); evaluations.resetCount(); Dfp zero = startValue.getZero(); Dfp nan = zero.newInstance((byte) 1, Dfp.QNAN); final Dfp[] x = new Dfp[maximalOrder + 1]; final Dfp[] y = new Dfp[maximalOrder + 1]; x[0] = min; x[1] = startValue; x[2] = max; evaluations.incrementCount(); y[1] = f.value(x[1]); if (y[1].isZero()) { return x[1]; } evaluations.incrementCount(); y[0] = f.value(x[0]); if (y[0].isZero()) { return x[0]; } int nbPoints; int signChangeIndex; if (y[0].multiply(y[1]).negativeOrNull()) { nbPoints = 2; signChangeIndex = 1; } else { evaluations.incrementCount(); y[2] = f.value(x[2]); if (y[2].isZero()) { return x[2]; } if (y[1].multiply(y[2]).negativeOrNull()) { nbPoints = 3; signChangeIndex = 2; } else { throw new NoBracketingException(x[0].toDouble(), x[2].toDouble(), y[0].toDouble(), y[2].toDouble()); } } final Dfp[] tmpX = new Dfp[x.length]; Dfp xA = x[signChangeIndex - 1]; Dfp yA = y[signChangeIndex - 1]; Dfp absXA = xA.abs(); Dfp absYA = yA.abs(); int agingA = 0; Dfp xB = x[signChangeIndex]; Dfp yB = y[signChangeIndex]; Dfp absXB = xB.abs(); Dfp absYB = yB.abs(); int agingB = 0; while (true) { Dfp maxX = absXA.lessThan(absXB) ? absXB : absXA; Dfp maxY = absYA.lessThan(absYB) ? absYB : absYA; final Dfp xTol = absoluteAccuracy.add(relativeAccuracy.multiply(maxX)); if (xB.subtract(xA).subtract(xTol).negativeOrNull() || maxY.lessThan(functionValueAccuracy)) { switch(allowedSolution) { case ANY_SIDE: return absYA.lessThan(absYB) ? xA : xB; case LEFT_SIDE: return xA; case RIGHT_SIDE: return xB; case BELOW_SIDE: return yA.lessThan(zero) ? xA : xB; case ABOVE_SIDE: return yA.lessThan(zero) ? xB : xA; default: throw new MathInternalError(null); } } Dfp targetY; if (agingA >= MAXIMAL_AGING) { targetY = yB.divide(16).negate(); } else if (agingB >= MAXIMAL_AGING) { targetY = yA.divide(16).negate(); } else { targetY = zero; } Dfp nextX; int start = 0; int end = nbPoints; do { System.arraycopy(x, start, tmpX, start, end - start); nextX = guessX(targetY, tmpX, y, start, end); if (!(nextX.greaterThan(xA) && nextX.lessThan(xB))) { if (signChangeIndex - start >= end - signChangeIndex) { ++start; } else { --end; } nextX = nan; } } while (nextX.isNaN() && (end - start > 1)); if (nextX.isNaN()) { nextX = xA.add(xB.subtract(xA).divide(2)); start = signChangeIndex - 1; end = signChangeIndex; } evaluations.incrementCount(); final Dfp nextY = f.value(nextX); if (nextY.isZero()) { return nextX; } if ((nbPoints > 2) && (end - start != nbPoints)) { nbPoints = end - start; System.arraycopy(x, start, x, 0, nbPoints); System.arraycopy(y, start, y, 0, nbPoints); signChangeIndex -= start; } else if (nbPoints == x.length) { nbPoints--; if (signChangeIndex >= (x.length + 1) / 2) { System.arraycopy(x, 1, x, 0, nbPoints); System.arraycopy(y, 1, y, 0, nbPoints); --signChangeIndex; } } System.arraycopy(x, signChangeIndex, x, signChangeIndex + 1, nbPoints - signChangeIndex); x[signChangeIndex] = nextX; System.arraycopy(y, signChangeIndex, y, signChangeIndex + 1, nbPoints - signChangeIndex); y[signChangeIndex] = nextY; ++nbPoints; if (nextY.multiply(yA).negativeOrNull()) { xB = nextX; yB = nextY; absYB = yB.abs(); ++agingA; agingB = 0; } else { xA = nextX; yA = nextY; absYA = yA.abs(); agingA = 0; ++agingB; signChangeIndex++; } } }"
                    ],
                    [
                        0.9885096947352091,
                        "private void prelim(double[] lowerBound, double[] upperBound) { printMethod(); final int n = currentBest.getDimension(); final int npt = numberOfInterpolationPoints; final int ndim = bMatrix.getRowDimension(); final double rhosq = initialTrustRegionRadius * initialTrustRegionRadius; final double recip = 1d / rhosq; final int np = n + 1; for (int j = 0; j < n; j++) { originShift.setEntry(j, currentBest.getEntry(j)); for (int k = 0; k < npt; k++) { interpolationPoints.setEntry(k, j, ZERO); } for (int i = 0; i < ndim; i++) { bMatrix.setEntry(i, j, ZERO); } } for (int i = 0, max = n * np / 2; i < max; i++) { modelSecondDerivativesValues.setEntry(i, ZERO); } for (int k = 0; k < npt; k++) { modelSecondDerivativesParameters.setEntry(k, ZERO); for (int j = 0, max = npt - np; j < max; j++) { zMatrix.setEntry(k, j, ZERO); } } int ipt = 0; int jpt = 0; double fbeg = Double.NaN; do { final int nfm = getEvaluations(); final int nfx = nfm - n; final int nfmm = nfm - 1; final int nfxm = nfx - 1; double stepa = 0; double stepb = 0; if (nfm <= 2 * n) { if (nfm >= 1 && nfm <= n) { stepa = initialTrustRegionRadius; if (upperDifference.getEntry(nfmm) == ZERO) { stepa = -stepa; } interpolationPoints.setEntry(nfm, nfmm, stepa); } else if (nfm > n) { stepa = interpolationPoints.getEntry(nfx, nfxm); stepb = -initialTrustRegionRadius; if (lowerDifference.getEntry(nfxm) == ZERO) { stepb = Math.min(TWO * initialTrustRegionRadius, upperDifference.getEntry(nfxm)); } if (upperDifference.getEntry(nfxm) == ZERO) { stepb = Math.max(-TWO * initialTrustRegionRadius, lowerDifference.getEntry(nfxm)); } interpolationPoints.setEntry(nfm, nfxm, stepb); } } else { final int tmp1 = (nfm - np) / n; jpt = nfm - tmp1 * n - n; ipt = jpt + tmp1; if (ipt > n) { final int tmp2 = jpt; jpt = ipt - n; ipt = tmp2; } final int iptMinus1 = ipt - 1; final int jptMinus1 = jpt - 1; interpolationPoints.setEntry(nfm, iptMinus1, interpolationPoints.getEntry(ipt, iptMinus1)); interpolationPoints.setEntry(nfm, jptMinus1, interpolationPoints.getEntry(jpt, jptMinus1)); } for (int j = 0; j < n; j++) { currentBest.setEntry(j, Math.min(Math.max(lowerBound[j], originShift.getEntry(j) + interpolationPoints.getEntry(nfm, j)), upperBound[j])); if (interpolationPoints.getEntry(nfm, j) == lowerDifference.getEntry(j)) { currentBest.setEntry(j, lowerBound[j]); } if (interpolationPoints.getEntry(nfm, j) == upperDifference.getEntry(j)) { currentBest.setEntry(j, upperBound[j]); } } final double objectiveValue = computeObjectiveValue(currentBest.toArray()); final double f = isMinimize ? objectiveValue : -objectiveValue; final int numEval = getEvaluations(); fAtInterpolationPoints.setEntry(nfm, f); if (numEval == 1) { fbeg = f; trustRegionCenterInterpolationPointIndex = 0; } else if (f < fAtInterpolationPoints.getEntry(trustRegionCenterInterpolationPointIndex)) { trustRegionCenterInterpolationPointIndex = nfm; } if (numEval <= 2 * n + 1) { if (numEval >= 2 && numEval <= n + 1) { gradientAtTrustRegionCenter.setEntry(nfmm, (f - fbeg) / stepa); if (npt < numEval + n) { final double oneOverStepA = ONE / stepa; bMatrix.setEntry(0, nfmm, -oneOverStepA); bMatrix.setEntry(nfm, nfmm, oneOverStepA); bMatrix.setEntry(npt + nfmm, nfmm, -HALF * rhosq); } } else if (numEval >= n + 2) { final int ih = nfx * (nfx + 1) / 2 - 1; final double tmp = (f - fbeg) / stepb; final double diff = stepb - stepa; modelSecondDerivativesValues.setEntry(ih, TWO * (tmp - gradientAtTrustRegionCenter.getEntry(nfxm)) / diff); gradientAtTrustRegionCenter.setEntry(nfxm, (gradientAtTrustRegionCenter.getEntry(nfxm) * stepb - tmp * stepa) / diff); if (stepa * stepb < ZERO) { if (f < fAtInterpolationPoints.getEntry(nfm - n)) { fAtInterpolationPoints.setEntry(nfm, fAtInterpolationPoints.getEntry(nfm - n)); fAtInterpolationPoints.setEntry(nfm - n, f); if (trustRegionCenterInterpolationPointIndex == nfm) { trustRegionCenterInterpolationPointIndex = nfm - n; } interpolationPoints.setEntry(nfm - n, nfxm, stepb); interpolationPoints.setEntry(nfm, nfxm, stepa); } } bMatrix.setEntry(0, nfxm, -(stepa + stepb) / (stepa * stepb)); bMatrix.setEntry(nfm, nfxm, -HALF / interpolationPoints.getEntry(nfm - n, nfxm)); bMatrix.setEntry(nfm - n, nfxm, -bMatrix.getEntry(0, nfxm) - bMatrix.getEntry(nfm, nfxm)); zMatrix.setEntry(0, nfxm, Math.sqrt(TWO) / (stepa * stepb)); zMatrix.setEntry(nfm, nfxm, Math.sqrt(HALF) / rhosq); zMatrix.setEntry(nfm - n, nfxm, -zMatrix.getEntry(0, nfxm) - zMatrix.getEntry(nfm, nfxm)); } } else { zMatrix.setEntry(0, nfxm, recip); zMatrix.setEntry(nfm, nfxm, recip); zMatrix.setEntry(ipt, nfxm, -recip); zMatrix.setEntry(jpt, nfxm, -recip); final int ih = ipt * (ipt - 1) / 2 + jpt - 1; final double tmp = interpolationPoints.getEntry(nfm, ipt - 1) * interpolationPoints.getEntry(nfm, jpt - 1); modelSecondDerivativesValues.setEntry(ih, (fbeg - fAtInterpolationPoints.getEntry(ipt) - fAtInterpolationPoints.getEntry(jpt) + f) / tmp); } } while (getEvaluations() < npt); }"
                    ],
                    [
                        0.9885096947352091,
                        "private void prelim(double[] lowerBound, double[] upperBound) { printMethod(); final int n = currentBest.getDimension(); final int npt = numberOfInterpolationPoints; final int ndim = bMatrix.getRowDimension(); final double rhosq = initialTrustRegionRadius * initialTrustRegionRadius; final double recip = 1d / rhosq; final int np = n + 1; for (int j = 0; j < n; j++) { originShift.setEntry(j, currentBest.getEntry(j)); for (int k = 0; k < npt; k++) { interpolationPoints.setEntry(k, j, ZERO); } for (int i = 0; i < ndim; i++) { bMatrix.setEntry(i, j, ZERO); } } for (int i = 0, max = n * np / 2; i < max; i++) { modelSecondDerivativesValues.setEntry(i, ZERO); } for (int k = 0; k < npt; k++) { modelSecondDerivativesParameters.setEntry(k, ZERO); for (int j = 0, max = npt - np; j < max; j++) { zMatrix.setEntry(k, j, ZERO); } } int ipt = 0; int jpt = 0; double fbeg = Double.NaN; do { final int nfm = getEvaluations(); final int nfx = nfm - n; final int nfmm = nfm - 1; final int nfxm = nfx - 1; double stepa = 0; double stepb = 0; if (nfm <= 2 * n) { if (nfm >= 1 && nfm <= n) { stepa = initialTrustRegionRadius; if (upperDifference.getEntry(nfmm) == ZERO) { stepa = -stepa; } interpolationPoints.setEntry(nfm, nfmm, stepa); } else if (nfm > n) { stepa = interpolationPoints.getEntry(nfx, nfxm); stepb = -initialTrustRegionRadius; if (lowerDifference.getEntry(nfxm) == ZERO) { stepb = Math.min(TWO * initialTrustRegionRadius, upperDifference.getEntry(nfxm)); } if (upperDifference.getEntry(nfxm) == ZERO) { stepb = Math.max(-TWO * initialTrustRegionRadius, lowerDifference.getEntry(nfxm)); } interpolationPoints.setEntry(nfm, nfxm, stepb); } } else { final int tmp1 = (nfm - np) / n; jpt = nfm - tmp1 * n - n; ipt = jpt + tmp1; if (ipt > n) { final int tmp2 = jpt; jpt = ipt - n; ipt = tmp2; } final int iptMinus1 = ipt - 1; final int jptMinus1 = jpt - 1; interpolationPoints.setEntry(nfm, iptMinus1, interpolationPoints.getEntry(ipt, iptMinus1)); interpolationPoints.setEntry(nfm, jptMinus1, interpolationPoints.getEntry(jpt, jptMinus1)); } for (int j = 0; j < n; j++) { currentBest.setEntry(j, Math.min(Math.max(lowerBound[j], originShift.getEntry(j) + interpolationPoints.getEntry(nfm, j)), upperBound[j])); if (interpolationPoints.getEntry(nfm, j) == lowerDifference.getEntry(j)) { currentBest.setEntry(j, lowerBound[j]); } if (interpolationPoints.getEntry(nfm, j) == upperDifference.getEntry(j)) { currentBest.setEntry(j, upperBound[j]); } } final double objectiveValue = computeObjectiveValue(currentBest.toArray()); final double f = isMinimize ? objectiveValue : -objectiveValue; final int numEval = getEvaluations(); fAtInterpolationPoints.setEntry(nfm, f); if (numEval == 1) { fbeg = f; trustRegionCenterInterpolationPointIndex = 0; } else if (f < fAtInterpolationPoints.getEntry(trustRegionCenterInterpolationPointIndex)) { trustRegionCenterInterpolationPointIndex = nfm; } if (numEval <= 2 * n + 1) { if (numEval >= 2 && numEval <= n + 1) { gradientAtTrustRegionCenter.setEntry(nfmm, (f - fbeg) / stepa); if (npt < numEval + n) { final double oneOverStepA = ONE / stepa; bMatrix.setEntry(0, nfmm, -oneOverStepA); bMatrix.setEntry(nfm, nfmm, oneOverStepA); bMatrix.setEntry(npt + nfmm, nfmm, -HALF * rhosq); } } else if (numEval >= n + 2) { final int ih = nfx * (nfx + 1) / 2 - 1; final double tmp = (f - fbeg) / stepb; final double diff = stepb - stepa; modelSecondDerivativesValues.setEntry(ih, TWO * (tmp - gradientAtTrustRegionCenter.getEntry(nfxm)) / diff); gradientAtTrustRegionCenter.setEntry(nfxm, (gradientAtTrustRegionCenter.getEntry(nfxm) * stepb - tmp * stepa) / diff); if (stepa * stepb < ZERO) { if (f < fAtInterpolationPoints.getEntry(nfm - n)) { fAtInterpolationPoints.setEntry(nfm, fAtInterpolationPoints.getEntry(nfm - n)); fAtInterpolationPoints.setEntry(nfm - n, f); if (trustRegionCenterInterpolationPointIndex == nfm) { trustRegionCenterInterpolationPointIndex = nfm - n; } interpolationPoints.setEntry(nfm - n, nfxm, stepb); interpolationPoints.setEntry(nfm, nfxm, stepa); } } bMatrix.setEntry(0, nfxm, -(stepa + stepb) / (stepa * stepb)); bMatrix.setEntry(nfm, nfxm, -HALF / interpolationPoints.getEntry(nfm - n, nfxm)); bMatrix.setEntry(nfm - n, nfxm, -bMatrix.getEntry(0, nfxm) - bMatrix.getEntry(nfm, nfxm)); zMatrix.setEntry(0, nfxm, Math.sqrt(TWO) / (stepa * stepb)); zMatrix.setEntry(nfm, nfxm, Math.sqrt(HALF) / rhosq); zMatrix.setEntry(nfm - n, nfxm, -zMatrix.getEntry(0, nfxm) - zMatrix.getEntry(nfm, nfxm)); } } else { zMatrix.setEntry(0, nfxm, recip); zMatrix.setEntry(nfm, nfxm, recip); zMatrix.setEntry(ipt, nfxm, -recip); zMatrix.setEntry(jpt, nfxm, -recip); final int ih = ipt * (ipt - 1) / 2 + jpt - 1; final double tmp = interpolationPoints.getEntry(nfm, ipt - 1) * interpolationPoints.getEntry(nfm, jpt - 1); modelSecondDerivativesValues.setEntry(ih, (fbeg - fAtInterpolationPoints.getEntry(ipt) - fAtInterpolationPoints.getEntry(jpt) + f) / tmp); } } while (getEvaluations() < npt); }"
                    ],
                    [
                        0.9883519411087036,
                        "@Override public void integrate(final ExpandableStatefulODE equations, final double t) throws NumberIsTooSmallException, DimensionMismatchException, MaxCountExceededException, NoBracketingException { sanityChecks(equations, t); setEquations(equations); final boolean forward = t > equations.getTime(); final double[] y0 = equations.getCompleteState(); final double[] y = y0.clone(); final int stages = c.length + 1; final double[][] yDotK = new double[stages][y.length]; final double[] yTmp = y0.clone(); final double[] yDotTmp = new double[y.length]; final RungeKuttaStepInterpolator interpolator = (RungeKuttaStepInterpolator) prototype.copy(); interpolator.reinitialize(this, yTmp, yDotK, forward, equations.getPrimaryMapper(), equations.getSecondaryMappers()); interpolator.storeTime(equations.getTime()); stepStart = equations.getTime(); double hNew = 0; boolean firstTime = true; initIntegration(equations.getTime(), y0, t); isLastStep = false; do { interpolator.shift(); double error = 10; while (error >= 1.0) { if (firstTime || !fsal) { computeDerivatives(stepStart, y, yDotK[0]); } if (firstTime) { final double[] scale = new double[mainSetDimension]; if (vecAbsoluteTolerance == null) { for (int i = 0; i < scale.length; ++i) { scale[i] = scalAbsoluteTolerance + scalRelativeTolerance * FastMath.abs(y[i]); } } else { for (int i = 0; i < scale.length; ++i) { scale[i] = vecAbsoluteTolerance[i] + vecRelativeTolerance[i] * FastMath.abs(y[i]); } } hNew = initializeStep(forward, getOrder(), scale, stepStart, y, yDotK[0], yTmp, yDotK[1]); firstTime = false; } stepSize = hNew; if (forward) { if (stepStart + stepSize >= t) { stepSize = t - stepStart; } } else { if (stepStart + stepSize <= t) { stepSize = t - stepStart; } } for (int k = 1; k < stages; ++k) { for (int j = 0; j < y0.length; ++j) { double sum = a[k - 1][0] * yDotK[0][j]; for (int l = 1; l < k; ++l) { sum += a[k - 1][l] * yDotK[l][j]; } yTmp[j] = y[j] + stepSize * sum; } computeDerivatives(stepStart + c[k - 1] * stepSize, yTmp, yDotK[k]); } for (int j = 0; j < y0.length; ++j) { double sum = b[0] * yDotK[0][j]; for (int l = 1; l < stages; ++l) { sum += b[l] * yDotK[l][j]; } yTmp[j] = y[j] + stepSize * sum; } error = estimateError(yDotK, y, yTmp, stepSize); if (error >= 1.0) { final double factor = FastMath.min(maxGrowth, FastMath.max(minReduction, safety * FastMath.pow(error, exp))); hNew = filterStep(stepSize * factor, forward, false); } } interpolator.storeTime(stepStart + stepSize); System.arraycopy(yTmp, 0, y, 0, y0.length); System.arraycopy(yDotK[stages - 1], 0, yDotTmp, 0, y0.length); stepStart = acceptStep(interpolator, y, yDotTmp, t); System.arraycopy(y, 0, yTmp, 0, y.length); if (!isLastStep) { interpolator.storeTime(stepStart); if (fsal) { System.arraycopy(yDotTmp, 0, yDotK[0], 0, y0.length); } final double factor = FastMath.min(maxGrowth, FastMath.max(minReduction, safety * FastMath.pow(error, exp))); final double scaledH = stepSize * factor; final double nextT = stepStart + scaledH; final boolean nextIsLast = forward ? (nextT >= t) : (nextT <= t); hNew = filterStep(scaledH, forward, nextIsLast); final double filteredNextT = stepStart + hNew; final boolean filteredNextIsLast = forward ? (filteredNextT >= t) : (filteredNextT <= t); if (filteredNextIsLast) { hNew = t - stepStart; } } } while (!isLastStep); equations.setTime(stepStart); equations.setCompleteState(y); resetInternalState(); }"
                    ],
                    [
                        0.9881410598754883,
                        "@Test public void testDerivativesWithInverseFunction() { double[] epsilon = new double[] { 1.0e-20, 4.0e-16, 3.0e-15, 2.0e-11, 3.0e-9, 1.0e-6 }; final double lo = 2; final double hi = 3; final Logit f = new Logit(lo, hi); final Sigmoid g = new Sigmoid(lo, hi); RandomGenerator random = new Well1024a(0x96885e9c1f81cea5l); final UnivariateDifferentiableFunction id = FunctionUtils.compose((UnivariateDifferentiableFunction) g, (UnivariateDifferentiableFunction) f); for (int maxOrder = 0; maxOrder < 6; ++maxOrder) { double max = 0; for (int i = 0; i < 10; i++) { final double x = lo + random.nextDouble() * (hi - lo); final DerivativeStructure dsX = new DerivativeStructure(1, maxOrder, 0, x); max = FastMath.max(max, FastMath.abs(dsX.getPartialDerivative(maxOrder) - id.value(dsX).getPartialDerivative(maxOrder))); Assert.assertEquals(dsX.getPartialDerivative(maxOrder), id.value(dsX).getPartialDerivative(maxOrder), epsilon[maxOrder]); } final DerivativeStructure dsLo = new DerivativeStructure(1, maxOrder, 0, lo); if (maxOrder == 0) { Assert.assertTrue(Double.isInfinite(f.value(dsLo).getPartialDerivative(maxOrder))); Assert.assertEquals(lo, id.value(dsLo).getPartialDerivative(maxOrder), epsilon[maxOrder]); } else if (maxOrder == 1) { Assert.assertTrue(Double.isInfinite(f.value(dsLo).getPartialDerivative(maxOrder))); Assert.assertTrue(Double.isNaN(id.value(dsLo).getPartialDerivative(maxOrder))); } else { Assert.assertTrue(Double.isNaN(f.value(dsLo).getPartialDerivative(maxOrder))); Assert.assertTrue(Double.isNaN(id.value(dsLo).getPartialDerivative(maxOrder))); } final DerivativeStructure dsHi = new DerivativeStructure(1, maxOrder, 0, hi); if (maxOrder == 0) { Assert.assertTrue(Double.isInfinite(f.value(dsHi).getPartialDerivative(maxOrder))); Assert.assertEquals(hi, id.value(dsHi).getPartialDerivative(maxOrder), epsilon[maxOrder]); } else if (maxOrder == 1) { Assert.assertTrue(Double.isInfinite(f.value(dsHi).getPartialDerivative(maxOrder))); Assert.assertTrue(Double.isNaN(id.value(dsHi).getPartialDerivative(maxOrder))); } else { Assert.assertTrue(Double.isNaN(f.value(dsHi).getPartialDerivative(maxOrder))); Assert.assertTrue(Double.isNaN(id.value(dsHi).getPartialDerivative(maxOrder))); } } }"
                    ]
                ],
                "directly_related_tests": [
                    "org.apache.commons.math3.genetics.FixedElapsedTimeTest::testIsSatisfied"
                ],
                "generation": {
                    "1": {
                        "prompt": "You are an expert Java developer. Below is a buggy method from a large Java project. This method causes one or more test failures.\nYour task is to fix the bug in this method. Use the provided test failure(s) to guide your reasoning.\n\n---\n\n## Buggy Method\n\n```java\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n         // -------------------- Initialization --------------------------------\n        isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n        final FitnessFunction fitfun = new FitnessFunction();\n        final double[] guess = getStartPoint();\n        // number of objective variables/problem dimension\n        dimension = guess.length;\n        initializeCMA(guess);\n        iterations = 0;\n        double bestValue = fitfun.value(guess);\n        push(fitnessHistory, bestValue);\n        PointValuePair optimum\n            = new PointValuePair(getStartPoint(),\n                                 isMinimize ? bestValue : -bestValue);\n        PointValuePair lastResult = null;\n\n        // -------------------- Generation Loop --------------------------------\n\n        generationLoop:\n        for (iterations = 1; iterations <= maxIterations; iterations++) {\n\n            // Generate and evaluate lambda offspring\n            final RealMatrix arz = randn1(dimension, lambda);\n            final RealMatrix arx = zeros(dimension, lambda);\n            final double[] fitness = new double[lambda];\n            // generate random offspring\n            for (int k = 0; k < lambda; k++) {\n                RealMatrix arxk = null;\n                for (int i = 0; i < checkFeasableCount + 1; i++) {\n                    if (diagonalOnly <= 0) {\n                        arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k))\n                                         .scalarMultiply(sigma)); // m + sig * Normal(0,C)\n                    } else {\n                        arxk = xmean.add(times(diagD,arz.getColumnMatrix(k))\n                                         .scalarMultiply(sigma));\n                    }\n                    if (i >= checkFeasableCount ||\n                        fitfun.isFeasible(arxk.getColumn(0))) {\n                        break;\n                    }\n                    // regenerate random arguments for row\n                    arz.setColumn(k, randn(dimension));\n                }\n                copyColumn(arxk, 0, arx, k);\n                try {\n                    fitness[k] = fitfun.value(arx.getColumn(k)); // compute fitness\n                } catch (TooManyEvaluationsException e) {\n                    break generationLoop;\n                }\n            }\n            // Sort by fitness and compute weighted mean into xmean\n            final int[] arindex = sortedIndices(fitness);\n            // Calculate new xmean, this is selection and recombination\n            final RealMatrix xold = xmean; // for speed up of Eq. (2) and (3)\n            final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n            xmean = bestArx.multiply(weights);\n            final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n            final RealMatrix zmean = bestArz.multiply(weights);\n            final boolean hsig = updateEvolutionPaths(zmean, xold);\n            if (diagonalOnly <= 0) {\n                updateCovariance(hsig, bestArx, arz, arindex, xold);\n            } else {\n                updateCovarianceDiagonalOnly(hsig, bestArz);\n            }\n            // Adapt step size sigma - Eq. (5)\n            sigma *= Math.exp(Math.min(1, (normps/chiN - 1) * cs / damps));\n            final double bestFitness = fitness[arindex[0]];\n            final double worstFitness = fitness[arindex[arindex.length - 1]];\n            if (bestValue > bestFitness) {\n                bestValue = bestFitness;\n                lastResult = optimum;\n                optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)),\n                                             isMinimize ? bestFitness : -bestFitness);\n                if (getConvergenceChecker() != null &&\n                    lastResult != null) {\n                    if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n                        break generationLoop;\n                    }\n                }\n            }\n            // handle termination criteria\n            // Break, if fitness is good enough\n            if (stopFitness != 0) { // only if stopFitness is defined\n                if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n                    break generationLoop;\n                }\n            }\n            final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n            final double[] pcCol = pc.getColumn(0);\n            for (int i = 0; i < dimension; i++) {\n                if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n                    break;\n                }\n                if (i >= dimension - 1) {\n                    break generationLoop;\n                }\n            }\n            for (int i = 0; i < dimension; i++) {\n                if (sigma * sqrtDiagC[i] > stopTolUpX) {\n                    break generationLoop;\n                }\n            }\n            final double historyBest = min(fitnessHistory);\n            final double historyWorst = max(fitnessHistory);\n            if (iterations > 2 &&\n                Math.max(historyWorst, worstFitness) -\n                Math.min(historyBest, bestFitness) < stopTolFun) {\n                break generationLoop;\n            }\n            if (iterations > fitnessHistory.length &&\n                historyWorst - historyBest < stopTolHistFun) {\n                break generationLoop;\n            }\n            // condition number of the covariance matrix exceeds 1e14\n            if (max(diagD) / min(diagD) > 1e7) {\n                break generationLoop;\n            }\n            // user defined termination\n            if (getConvergenceChecker() != null) {\n                final PointValuePair current\n                    = new PointValuePair(bestArx.getColumn(0),\n                                         isMinimize ? bestFitness : -bestFitness);\n                if (lastResult != null &&\n                    getConvergenceChecker().converged(iterations, current, lastResult)) {\n                    break generationLoop;\n                    }\n                lastResult = current;\n            }\n            // Adjust step size in case of equal function values (flat fitness)\n            if (bestValue == fitness[arindex[(int)(0.1+lambda/4.)]]) {\n                sigma = sigma * Math.exp(0.2 + cs / damps);\n            }\n            if (iterations > 2 && Math.max(historyWorst, bestFitness) -\n                Math.min(historyBest, bestFitness) == 0) {\n                sigma = sigma * Math.exp(0.2 + cs / damps);\n            }\n            // store best in history\n            push(fitnessHistory,bestFitness);\n            fitfun.setValueRange(worstFitness-bestFitness);\n            if (generateStatistics) {\n                statisticsSigmaHistory.add(sigma);\n                statisticsFitnessHistory.add(bestFitness);\n                statisticsMeanHistory.add(xmean.transpose());\n                statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n            }\n        }\n        return optimum;\n    }\n\n```\n---\n\n## Failing Test Case(s)\n\n\n---\n\n## Your Goal\n\nFix the buggy method. Return only the fixed Java method. Do not include explanations or other text.\n",
                        "patches": [
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n         // -------------------- Initialization --------------------------------\n        isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n        final FitnessFunction fitfun = new FitnessFunction();\n        final double[] guess = getStartPoint();\n        // number of objective variables/problem dimension\n        dimension = guess.length;\n        initializeCMA(guess);\n        iterations = 0;\n        double bestValue = fitfun.value(guess);\n        push(fitnessHistory, bestValue);\n        PointValuePair optimum\n            = new PointValuePair(getStartPoint(),\n                                 isMinimize ? bestValue : -bestValue);\n        PointValuePair lastResult = null;\n\n        // -------------------- Generation Loop --------------------------------\n\n        generationLoop:\n        for (iterations = 1; iterations <= maxIterations; iterations++) {\n\n            // Generate and evaluate lambda offspring\n            final RealMatrix arz = randn1(dimension, lambda);\n            final RealMatrix arx = zeros(dimension, lambda);\n            final double[] fitness = new double[lambda];\n            // generate random offspring\n            for (int k = 0; k < lambda; k++) {\n                RealMatrix arxk = null;\n                for (int i = 0; i < checkFeasableCount + 1; i++) {\n                    if (diagonalOnly <= 0) {\n                        arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k))\n                                         .scalarMultiply(sigma)); // m + sig * Normal(0,C)\n                    } else {\n                        arxk = xmean.add(times(diagD,arz.getColumnMatrix(k))\n                                         .scalarMultiply(sigma));\n                    }\n                    if (i >= checkFeasableCount ||\n                        fitfun.isFeasible(arxk.getColumn(0))) {\n                        break;\n                    }\n                    // regenerate random arguments for row\n                    arz.setColumn(k, randn(dimension));\n                }\n                copyColumn(arxk, 0, arx, k);\n                try {\n                    fitness[k] = fitfun.value(arx.getColumn(k)); // compute fitness\n                } catch (TooManyEvaluationsException e) {\n                    break generationLoop;\n                }\n            }\n            // Sort by fitness and compute weighted mean into xmean\n            final int[] arindex = sortedIndices(fitness);\n            // Calculate new xmean, this is selection and recombination\n            final RealMatrix xold = xmean; // for speed up of Eq. (2) and (3)\n            final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n            xmean = bestArx.multiply(weights);\n            final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n            final RealMatrix zmean = bestArz.multiply(weights);\n            final boolean hsig = updateEvolutionPaths(zmean, xold);\n            if (diagonalOnly <= 0) {\n                updateCovariance(hsig, bestArx, arz, arindex, xold);\n            } else {\n                updateCovarianceDiagonalOnly(hsig, bestArz);\n            }\n            // Adapt step size sigma - Eq. (5)\n            sigma *= Math.exp(Math.min(1, (normps/chiN - 1) * cs / damps));\n            final double bestFitness = fitness[arindex[0]];\n            final double worstFitness = fitness[arindex[arindex.length - 1]];\n            if (bestValue > bestFitness) {\n                bestValue = bestFitness;\n                lastResult = optimum;\n                optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)),\n                                             isMinimize ? bestFitness : -bestFitness);\n                if (getConvergenceChecker() != null &&\n                    lastResult != null) {\n                    if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n                        break generationLoop;\n                    }\n                }\n            }\n            // handle termination criteria\n            // Break, if fitness is good enough\n            if (stopFitness != 0) { // only if stopFitness is defined\n                if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n                    break generationLoop;\n                }\n            }\n            final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n            final double[] pcCol = pc.getColumn(0);\n            for (int i = 0; i < dimension; i++) {\n                if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n                    break;\n                }\n                if (i >= dimension - 1) {\n                    break generationLoop;\n                }\n            }\n            for (int i = 0; i < dimension; i++) {\n                if (sigma * sqrtDiagC[i] > stopTolUpX) {\n                    break generationLoop;\n                }\n            }\n            final double historyBest = min(fitnessHistory);\n            final double historyWorst = max(fitnessHistory);\n            if (iterations > 2 &&\n                Math.max(historyWorst, worstFitness) -\n                Math.min(historyBest, bestFitness) < stopTolFun) {\n                break generationLoop;\n            }\n            if (iterations > fitnessHistory.length &&\n                historyWorst - historyBest < stopTolHistFun) {\n                break generationLoop;\n            }\n            // condition number of the covariance matrix exceeds 1e7\n            if (max(diagD) / min(diagD) > 1e7) {\n                break generationLoop;\n            }\n            // user defined termination\n            if (getConvergenceChecker() != null) {\n                final PointValuePair current\n                    = new PointValuePair(bestArx.getColumn(0),\n                                         isMinimize ? bestFitness : -bestFitness);\n                if (lastResult != null &&\n                    getConvergenceChecker().converged(iterations, current, lastResult)) {\n                    break generationLoop;\n                    }\n                lastResult = current;\n            }\n            // Adjust step size in case of equal function values (flat fitness)\n            if (bestValue == fitness[arindex[(int)(0.1+lambda/4.)]]) {\n                sigma = sigma * Math.exp(0.2 + cs / damps);\n            }\n            if (iterations > 2 && Math.max(historyWorst, bestFitness) -\n                Math.min(historyBest, bestFitness) == 0) {\n                sigma = sigma * Math.exp(0.2 + cs / damps);\n            }\n            // store best in history\n            push(fitnessHistory,bestFitness);\n            fitfun.setValueRange(worstFitness-bestFitness);\n            if (generateStatistics) {\n                statisticsSigmaHistory.add(sigma);\n                statisticsFitnessHistory.add(bestFitness);\n                statisticsMeanHistory.add(xmean.transpose());\n                statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n            }\n        }\n        return optimum;\n    }\n",
                            "\n@Override\nprotected PointValuePair doOptimize() {\n    // -------------------- Initialization --------------------------------\n    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n    final FitnessFunction fitfun = new FitnessFunction();\n    final double[] guess = getStartPoint();\n    // number of objective variables/problem dimension\n    dimension = guess.length;\n    initializeCMA(guess);\n    iterations = 0;\n    double bestValue = fitfun.value(guess);\n    push(fitnessHistory, bestValue);\n    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n\n    // -------------------- Generation Loop --------------------------------\n    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n        // Generate and evaluate lambda offspring\n        final RealMatrix arz = randn1(dimension, lambda);\n        final RealMatrix arx = zeros(dimension, lambda);\n        final double[] fitness = new double[lambda];\n        // generate random offspring\n        for (int k = 0; k < lambda; k++) {\n            RealMatrix arxk = null;\n            for (int i = 0; i < checkFeasableCount + 1; i++) {\n                if (diagonalOnly <= 0) {\n                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(sigma)); // m + sig * Normal(0,C)\n                } else {\n                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n                }\n                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n                    break;\n                }\n                // regenerate random arguments for row\n                arz.setColumn(k, randn(dimension));\n            }\n            copyColumn(arxk, 0, arx, k);\n            try {\n                fitness[k] = fitfun.value(arx.getColumn(k)); // compute fitness\n            } catch (TooManyEvaluationsException e) {\n                break generationLoop;\n            }\n        }\n        // Sort by fitness and compute weighted mean into xmean\n        final int[] arindex = sortedIndices(fitness);\n        // Calculate new xmean, this is selection and recombination\n        final RealMatrix xold = xmean; // for speed up of Eq. (2) and (3)\n        final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n        xmean = bestArx.multiply(weights);\n        final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n        final RealMatrix zmean = bestArz.multiply(weights);\n        final boolean hsig = updateEvolutionPaths(zmean, xold);\n        if (diagonalOnly <= 0) {\n            updateCovariance(hsig, bestArx, arz, arindex, xold);\n        } else {\n            updateCovarianceDiagonalOnly(hsig, bestArz);\n        }\n        // Adapt step size sigma - Eq. (5)\n        sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps));\n        final double bestFitness = fitness[arindex[0]];\n        final double worstFitness = fitness[arindex[arindex.length - 1]];\n\n        if (bestValue > bestFitness) {\n            bestValue = bestFitness;\n            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n        }\n\n        // Check for convergence\n        if (getConvergenceChecker() != null) {\n            if (iterations > 1 && getConvergenceChecker().converged(iterations, optimum, new PointValuePair(fitfun.repair(xold.getColumn(0)), isMinimize ? bestValue : -bestValue))) {\n                break generationLoop;\n            }\n        }\n        \n        // handle termination criteria\n        // Break if fitness is good enough\n        if (stopFitness != 0) { // only if stopFitness is defined\n            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n                break generationLoop;\n            }\n        }\n        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n        final double[] pcCol = pc.getColumn(0);\n        for (int i = 0; i < dimension; i++) {\n            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n                break;\n            }\n            if (i >= dimension - 1) {\n                break generationLoop;\n            }\n        }\n        for (int i = 0; i < dimension; i++) {\n            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n                break generationLoop;\n            }\n        }\n        final double historyBest = min(fitnessHistory);\n        final double historyWorst = max(fitnessHistory);\n        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n            break generationLoop;\n        }\n        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n            break generationLoop;\n        }\n        // condition number of the covariance matrix exceeds 1e14\n        if (max(diagD) / min(diagD) > 1e7) {\n            break generationLoop;\n        }\n        // Adjust step size in case of equal function values (flat fitness)\n        if (bestValue == fitness[arindex[(int)(0.1 + lambda / 4.)]]) {\n            sigma = sigma * Math.exp(0.2 + cs / damps);\n        }\n        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n            sigma = sigma * Math.exp(0.2 + cs / damps);\n        }\n        // store best in history\n        push(fitnessHistory, bestFitness);\n        fitfun.setValueRange(worstFitness - bestFitness);\n        if (generateStatistics) {\n            statisticsSigmaHistory.add(sigma);\n            statisticsFitnessHistory.add(bestFitness);\n            statisticsMeanHistory.add(xmean.transpose());\n            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n        }\n    }\n    return optimum;\n}\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n         // -------------------- Initialization --------------------------------\n        isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n        final FitnessFunction fitfun = new FitnessFunction();\n        final double[] guess = getStartPoint();\n        // number of objective variables/problem dimension\n        dimension = guess.length;\n        initializeCMA(guess);\n        iterations = 0;\n        double bestValue = fitfun.value(guess);\n        push(fitnessHistory, bestValue);\n        PointValuePair optimum\n            = new PointValuePair(getStartPoint(),\n                                 isMinimize ? bestValue : -bestValue);\n        PointValuePair lastResult = null;\n\n        // -------------------- Generation Loop --------------------------------\n\n        generationLoop:\n        for (iterations = 1; iterations <= maxIterations; iterations++) {\n\n            // Generate and evaluate lambda offspring\n            final RealMatrix arz = randn1(dimension, lambda);\n            final RealMatrix arx = zeros(dimension, lambda);\n            final double[] fitness = new double[lambda];\n            // generate random offspring\n            for (int k = 0; k < lambda; k++) {\n                RealMatrix arxk = null;\n                for (int i = 0; i < checkFeasableCount + 1; i++) {\n                    if (diagonalOnly <= 0) {\n                        arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k))\n                                         .scalarMultiply(sigma)); // m + sig * Normal(0,C)\n                    } else {\n                        arxk = xmean.add(times(diagD,arz.getColumnMatrix(k))\n                                         .scalarMultiply(sigma));\n                    }\n                    if (i >= checkFeasableCount ||\n                        fitfun.isFeasible(arxk.getColumn(0))) {\n                        break;\n                    }\n                    // regenerate random arguments for row\n                    arz.setColumn(k, randn(dimension));\n                }\n                copyColumn(arxk, 0, arx, k);\n                try {\n                    fitness[k] = fitfun.value(arx.getColumn(k)); // compute fitness\n                } catch (TooManyEvaluationsException e) {\n                    break generationLoop;\n                }\n            }\n            // Sort by fitness and compute weighted mean into xmean\n            final int[] arindex = sortedIndices(fitness);\n            // Calculate new xmean, this is selection and recombination\n            final RealMatrix xold = xmean; // for speed up of Eq. (2) and (3)\n            final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n            xmean = bestArx.multiply(weights);\n            final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n            final RealMatrix zmean = bestArz.multiply(weights);\n            final boolean hsig = updateEvolutionPaths(zmean, xold);\n            if (diagonalOnly <= 0) {\n                updateCovariance(hsig, bestArx, arz, arindex, xold);\n            } else {\n                updateCovarianceDiagonalOnly(hsig, bestArz);\n            }\n            // Adapt step size sigma - Eq. (5)\n            sigma *= Math.exp(Math.min(1, (normps/chiN - 1) * cs / damps));\n            final double bestFitness = fitness[arindex[0]];\n            final double worstFitness = fitness[arindex[arindex.length - 1]];\n            if (bestValue > bestFitness) {\n                bestValue = bestFitness;\n                lastResult = optimum;\n                optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)),\n                                             isMinimize ? bestFitness : -bestFitness);\n                if (getConvergenceChecker() != null &&\n                    lastResult != null) {\n                    if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n                        break generationLoop;\n                    }\n                }\n            }\n            // handle termination criteria\n            // Break, if fitness is good enough\n            if (stopFitness != 0) { // only if stopFitness is defined\n                if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n                    break generationLoop;\n                }\n            }\n            final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n            final double[] pcCol = pc.getColumn(0);\n            for (int i = 0; i < dimension; i++) {\n                if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n                    break;\n                }\n                if (i >= dimension - 1) {\n                    break generationLoop;\n                }\n            }\n            for (int i = 0; i < dimension; i++) {\n                if (sigma * sqrtDiagC[i] > stopTolUpX) {\n                    break generationLoop;\n                }\n            }\n            final double historyBest = min(fitnessHistory);\n            final double historyWorst = max(fitnessHistory);\n            if (iterations > 2 &&\n                Math.max(historyWorst, worstFitness) -\n                Math.min(historyBest, bestFitness) < stopTolFun) {\n                break generationLoop;\n            }\n            if (iterations > fitnessHistory.length &&\n                historyWorst - historyBest < stopTolHistFun) {\n                break generationLoop;\n            }\n            // condition number of the covariance matrix exceeds 1e14\n            if (max(diagD) / min(diagD) > 1e7) {\n                break generationLoop;\n            }\n            // user defined termination\n            if (getConvergenceChecker() != null) {\n                final PointValuePair current\n                    = new PointValuePair(bestArx.getColumn(0),\n                                         isMinimize ? bestFitness : -bestFitness);\n                if (lastResult != null &&\n                    getConvergenceChecker().converged(iterations, current, lastResult)) {\n                    break generationLoop;\n                    }\n                lastResult = current;\n            }\n            // Adjust step size in case of equal function values (flat fitness)\n            if (bestValue == fitness[arindex[(int)(0.1+lambda/4.)]]) {\n                sigma = sigma * Math.exp(0.2 + cs / damps);\n            }\n            if (iterations > 2 && Math.max(historyWorst, bestFitness) -\n                Math.min(historyBest, bestFitness) == 0) {\n                sigma = sigma * Math.exp(0.2 + cs / damps);\n            }\n            // store best in history\n            push(fitnessHistory,bestFitness);\n            fitfun.setValueRange(worstFitness-bestFitness);\n            if (generateStatistics) {\n                statisticsSigmaHistory.add(sigma);\n                statisticsFitnessHistory.add(bestFitness);\n                statisticsMeanHistory.add(xmean.transpose());\n                statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n            }\n        }\n        return optimum;\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n         // -------------------- Initialization --------------------------------\n        isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n        final FitnessFunction fitfun = new FitnessFunction();\n        final double[] guess = getStartPoint();\n        // number of objective variables/problem dimension\n        dimension = guess.length;\n        initializeCMA(guess);\n        iterations = 0;\n        double bestValue = fitfun.value(guess);\n        push(fitnessHistory, bestValue);\n        PointValuePair optimum\n            = new PointValuePair(getStartPoint(),\n                                 isMinimize ? bestValue : -bestValue);\n        PointValuePair lastResult = null;\n\n        // -------------------- Generation Loop --------------------------------\n\n        generationLoop:\n        for (iterations = 1; iterations <= maxIterations; iterations++) {\n\n            // Generate and evaluate lambda offspring\n            final RealMatrix arz = randn1(dimension, lambda);\n            final RealMatrix arx = zeros(dimension, lambda);\n            final double[] fitness = new double[lambda];\n            // generate random offspring\n            for (int k = 0; k < lambda; k++) {\n                RealMatrix arxk = null;\n                for (int i = 0; i < checkFeasableCount + 1; i++) {\n                    if (diagonalOnly <= 0) {\n                        arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k))\n                                         .scalarMultiply(sigma)); // m + sig * Normal(0,C)\n                    } else {\n                        arxk = xmean.add(times(diagD,arz.getColumnMatrix(k))\n                                         .scalarMultiply(sigma));\n                    }\n                    if (i >= checkFeasableCount ||\n                        fitfun.isFeasible(arxk.getColumn(0))) {\n                        break;\n                    }\n                    // regenerate random arguments for row\n                    arz.setColumn(k, randn(dimension));\n                }\n                copyColumn(arxk, 0, arx, k);\n                try {\n                    fitness[k] = fitfun.value(arx.getColumn(k)); // compute fitness\n                } catch (TooManyEvaluationsException e) {\n                    break generationLoop;\n                }\n            }\n            // Sort by fitness and compute weighted mean into xmean\n            final int[] arindex = sortedIndices(fitness);\n            // Calculate new xmean, this is selection and recombination\n            final RealMatrix xold = xmean; // for speed up of Eq. (2) and (3)\n            final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n            xmean = bestArx.multiply(weights);\n            final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n            final RealMatrix zmean = bestArz.multiply(weights);\n            final boolean hsig = updateEvolutionPaths(zmean, xold);\n            if (diagonalOnly <= 0) {\n                updateCovariance(hsig, bestArx, arz, arindex, xold);\n            } else {\n                updateCovarianceDiagonalOnly(hsig, bestArz);\n            }\n            // Adapt step size sigma - Eq. (5)\n            sigma *= Math.exp(Math.min(1, (normps/chiN - 1) * cs / damps));\n            final double bestFitness = fitness[arindex[0]];\n            final double worstFitness = fitness[arindex[arindex.length - 1]];\n            if (bestValue > bestFitness) {\n                bestValue = bestFitness;\n                lastResult = optimum;\n                optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)),\n                                             isMinimize ? bestFitness : -bestFitness);\n                if (getConvergenceChecker() != null &&\n                    lastResult != null) {\n                    if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n                        break generationLoop;\n                    }\n                }\n            }\n            // handle termination criteria\n            // Break, if fitness is good enough\n            if (stopFitness != 0) { // only if stopFitness is defined\n                if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n                    break generationLoop;\n                }\n            }\n            final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n            final double[] pcCol = pc.getColumn(0);\n            boolean breakLoop = false;\n            for (int i = 0; i < dimension; i++) {\n                if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n                    breakLoop = true;\n                    break;\n                }\n            }\n            if (!breakLoop) {\n                for (int i = 0; i < dimension; i++) {\n                    if (sigma * sqrtDiagC[i] > stopTolUpX) {\n                        breakLoop = true;\n                        break;\n                    }\n                }\n            }\n            if (breakLoop) {\n                break generationLoop;\n            }\n            final double historyBest = min(fitnessHistory);\n            final double historyWorst = max(fitnessHistory);\n            if (iterations > 2 &&\n                Math.max(historyWorst, worstFitness) -\n                Math.min(historyBest, bestFitness) < stopTolFun) {\n                break generationLoop;\n            }\n            if (iterations > fitnessHistory.length &&\n                historyWorst - historyBest < stopTolHistFun) {\n                break generationLoop;\n            }\n            // condition number of the covariance matrix exceeds 1e14\n            if (max(diagD) / min(diagD) > 1e7) {\n                break generationLoop;\n            }\n            // user defined termination\n            if (getConvergenceChecker() != null) {\n                final PointValuePair current\n                    = new PointValuePair(bestArx.getColumn(0),\n                                         isMinimize ? bestFitness : -bestFitness);\n                if (lastResult != null &&\n                    getConvergenceChecker().converged(iterations, current, lastResult)) {\n                    break generationLoop;\n                    }\n                lastResult = current;\n            }\n            // Adjust step size in case of equal function values (flat fitness)\n            if (bestValue == fitness[arindex[(int)(0.1+lambda/4.)]]) {\n                sigma = sigma * Math.exp(0.2 + cs / damps);\n            }\n            if (iterations > 2 && Math.max(historyWorst, bestFitness) -\n                Math.min(historyBest, bestFitness) == 0) {\n                sigma = sigma * Math.exp(0.2 + cs / damps);\n            }\n            // store best in history\n            push(fitnessHistory,bestFitness);\n            fitfun.setValueRange(worstFitness-bestFitness);\n            if (generateStatistics) {\n                statisticsSigmaHistory.add(sigma);\n                statisticsFitnessHistory.add(bestFitness);\n                statisticsMeanHistory.add(xmean.transpose());\n                statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n            }\n        }\n        return optimum;\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        // -------------------- Initialization --------------------------------\n        isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n        final FitnessFunction fitfun = new FitnessFunction();\n        final double[] guess = getStartPoint();\n        // number of objective variables/problem dimension\n        dimension = guess.length;\n        initializeCMA(guess);\n        iterations = 0;\n        double bestValue = fitfun.value(guess);\n        push(fitnessHistory, bestValue);\n        PointValuePair optimum\n            = new PointValuePair(getStartPoint(),\n                                 isMinimize ? bestValue : -bestValue);\n        PointValuePair lastResult = null;\n\n        // -------------------- Generation Loop --------------------------------\n\n        generationLoop:\n        for (iterations = 1; iterations <= maxIterations; iterations++) {\n\n            // Generate and evaluate lambda offspring\n            final RealMatrix arz = randn1(dimension, lambda);\n            final RealMatrix arx = zeros(dimension, lambda);\n            final double[] fitness = new double[lambda];\n            // generate random offspring\n            for (int k = 0; k < lambda; k++) {\n                RealMatrix arxk = null;\n                for (int i = 0; i < checkFeasableCount + 1; i++) {\n                    if (diagonalOnly <= 0) {\n                        arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k))\n                                         .scalarMultiply(sigma)); // m + sig * Normal(0,C)\n                    } else {\n                        arxk = xmean.add(times(diagD,arz.getColumnMatrix(k))\n                                         .scalarMultiply(sigma));\n                    }\n                    if (i >= checkFeasableCount ||\n                        fitfun.isFeasible(arxk.getColumn(0))) {\n                        break;\n                    }\n                    // regenerate random arguments for row\n                    arz.setColumn(k, randn(dimension));\n                }\n                copyColumn(arxk, 0, arx, k);\n                try {\n                    fitness[k] = fitfun.value(arx.getColumn(k)); // compute fitness\n                } catch (TooManyEvaluationsException e) {\n                    break generationLoop;\n                }\n            }\n            // Sort by fitness and compute weighted mean into xmean\n            final int[] arindex = sortedIndices(fitness);\n            // Calculate new xmean, this is selection and recombination\n            final RealMatrix xold = xmean; // for speed up of Eq. (2) and (3)\n            final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n            xmean = bestArx.multiply(weights);\n            final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n            final RealMatrix zmean = bestArz.multiply(weights);\n            final boolean hsig = updateEvolutionPaths(zmean, xold);\n            if (diagonalOnly <= 0) {\n                updateCovariance(hsig, bestArx, arz, arindex, xold);\n            } else {\n                updateCovarianceDiagonalOnly(hsig, bestArz);\n            }\n            // Adapt step size sigma - Eq. (5)\n            sigma *= Math.exp(Math.min(1, (normps/chiN - 1) * cs / damps));\n            final double bestFitness = fitness[arindex[0]];\n            final double worstFitness = fitness[arindex[arindex.length - 1]];\n            if (bestValue > bestFitness) {\n                bestValue = bestFitness;\n                lastResult = optimum;\n                optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)),\n                                             isMinimize ? bestFitness : -bestFitness);\n                if (getConvergenceChecker() != null &&\n                    lastResult != null) {\n                    if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n                        break generationLoop;\n                    }\n                }\n            }\n            // handle termination criteria\n            // Break, if fitness is good enough\n            if (stopFitness != 0) { // only if stopFitness is defined\n                if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n                    break generationLoop;\n                }\n            }\n            final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n            final double[] pcCol = pc.getColumn(0);\n            for (int i = 0; i < dimension; i++) {\n                if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n                    break;\n                }\n                if (i >= dimension - 1) {\n                    break generationLoop;\n                }\n            }\n            for (int i = 0; i < dimension; i++) {\n                if (sigma * sqrtDiagC[i] > stopTolUpX) {\n                    break generationLoop;\n                }\n            }\n            final double historyBest = min(fitnessHistory);\n            final double historyWorst = max(fitnessHistory);\n            if (iterations > 2 &&\n                Math.max(historyWorst, worstFitness) -\n                Math.min(historyBest, bestFitness) < stopTolFun) {\n                break generationLoop;\n            }\n            if (iterations > fitnessHistory.length &&\n                historyWorst - historyBest < stopTolHistFun) {\n                break generationLoop;\n            }\n            // condition number of the covariance matrix exceeds 1e14\n            if (max(diagD) / min(diagD) > 1e7) {\n                break generationLoop;\n            }\n            // user defined termination\n            if (getConvergenceChecker() != null) {\n                final PointValuePair current\n                    = new PointValuePair(bestArx.getColumn(0),\n                                         isMinimize ? bestFitness : -bestFitness);\n                lastResult = current;\n                if (getConvergenceChecker().converged(iterations, current, optimum)) {\n                    break generationLoop;\n                }\n            }\n            // Adjust step size in case of equal function values (flat fitness)\n            if (bestValue == fitness[arindex[(int)(0.1+lambda/4.)]]) {\n                sigma = sigma * Math.exp(0.2 + cs / damps);\n            }\n            if (iterations > 2 && Math.max(historyWorst, bestFitness) -\n                Math.min(historyBest, bestFitness) == 0) {\n                sigma = sigma * Math.exp(0.2 + cs / damps);\n            }\n            // store best in history\n            push(fitnessHistory,bestFitness);\n            fitfun.setValueRange(worstFitness-bestFitness);\n            if (generateStatistics) {\n                statisticsSigmaHistory.add(sigma);\n                statisticsFitnessHistory.add(bestFitness);\n                statisticsMeanHistory.add(xmean.transpose());\n                statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n            }\n        }\n        return optimum;\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n         // -------------------- Initialization --------------------------------\n        isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n        final FitnessFunction fitfun = new FitnessFunction();\n        final double[] guess = getStartPoint();\n        // number of objective variables/problem dimension\n        dimension = guess.length;\n        initializeCMA(guess);\n        iterations = 0;\n        double bestValue = fitfun.value(guess);\n        push(fitnessHistory, bestValue);\n        PointValuePair optimum\n            = new PointValuePair(getStartPoint(),\n                                 isMinimize ? bestValue : -bestValue);\n        PointValuePair lastResult = null;\n\n        // -------------------- Generation Loop --------------------------------\n\n        generationLoop:\n        for (iterations = 1; iterations <= maxIterations; iterations++) {\n\n            // Generate and evaluate lambda offspring\n            final RealMatrix arz = randn1(dimension, lambda);\n            final RealMatrix arx = zeros(dimension, lambda);\n            final double[] fitness = new double[lambda];\n            // generate random offspring\n            for (int k = 0; k < lambda; k++) {\n                RealMatrix arxk = null;\n                for (int i = 0; i < checkFeasableCount + 1; i++) {\n                    if (diagonalOnly <= 0) {\n                        arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k))\n                                         .scalarMultiply(sigma)); // m + sig * Normal(0,C)\n                    } else {\n                        arxk = xmean.add(times(diagD,arz.getColumnMatrix(k))\n                                         .scalarMultiply(sigma));\n                    }\n                    if (i >= checkFeasableCount ||\n                        fitfun.isFeasible(arxk.getColumn(0))) {\n                        break;\n                    }\n                    // regenerate random arguments for row\n                    arz.setColumn(k, randn(dimension));\n                }\n                copyColumn(arxk, 0, arx, k);\n                try {\n                    fitness[k] = fitfun.value(arx.getColumn(k)); // compute fitness\n                } catch (TooManyEvaluationsException e) {\n                    break generationLoop;\n                }\n            }\n            // Sort by fitness and compute weighted mean into xmean\n            final int[] arindex = sortedIndices(fitness);\n            // Calculate new xmean, this is selection and recombination\n            final RealMatrix xold = xmean; // for speed up of Eq. (2) and (3)\n            final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n            xmean = bestArx.multiply(weights);\n            final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n            final RealMatrix zmean = bestArz.multiply(weights);\n            final boolean hsig = updateEvolutionPaths(zmean, xold);\n            if (diagonalOnly <= 0) {\n                updateCovariance(hsig, bestArx, arz, arindex, xold);\n            } else {\n                updateCovarianceDiagonalOnly(hsig, bestArz);\n            }\n            // Adapt step size sigma - Eq. (5)\n            sigma *= Math.exp(Math.min(1, (normps/chiN - 1) * cs / damps));\n            final double bestFitness = fitness[arindex[0]];\n            final double worstFitness = fitness[arindex[arindex.length - 1]];\n            if (bestValue > bestFitness) {\n                bestValue = bestFitness;\n                lastResult = optimum;\n                optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)),\n                                             isMinimize ? bestFitness : -bestFitness);\n\n                // Fix: Ensure lastResult is assigned before checking convergence\n                if (getConvergenceChecker() != null) {\n                    lastResult = optimum;\n                    if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n                        break generationLoop;\n                    }\n                }\n            }\n            // handle termination criteria\n            // Break, if fitness is good enough\n            if (stopFitness != 0) { // only if stopFitness is defined\n                if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n                    break generationLoop;\n                }\n            }\n            final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n            final double[] pcCol = pc.getColumn(0);\n            for (int i = 0; i < dimension; i++) {\n                if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n                    break;\n                }\n                if (i >= dimension - 1) {\n                    break generationLoop;\n                }\n            }\n            for (int i = 0; i < dimension; i++) {\n                if (sigma * sqrtDiagC[i] > stopTolUpX) {\n                    break generationLoop;\n                }\n            }\n            final double historyBest = min(fitnessHistory);\n            final double historyWorst = max(fitnessHistory);\n            if (iterations > 2 &&\n                Math.max(historyWorst, worstFitness) -\n                Math.min(historyBest, bestFitness) < stopTolFun) {\n                break generationLoop;\n            }\n            if (iterations > fitnessHistory.length &&\n                historyWorst - historyBest < stopTolHistFun) {\n                break generationLoop;\n            }\n            // condition number of the covariance matrix exceeds 1e14\n            if (max(diagD) / min(diagD) > 1e7) {\n                break generationLoop;\n            }\n            // user defined termination\n            if (getConvergenceChecker() != null) {\n                final PointValuePair current\n                    = new PointValuePair(bestArx.getColumn(0),\n                                         isMinimize ? bestFitness : -bestFitness);\n                if (lastResult != null &&\n                    getConvergenceChecker().converged(iterations, current, lastResult)) {\n                    break generationLoop;\n                    }\n                lastResult = current;\n            }\n            // Adjust step size in case of equal function values (flat fitness)\n            if (bestValue == fitness[arindex[(int)(0.1+lambda/4.)]]) {\n                sigma = sigma * Math.exp(0.2 + cs / damps);\n            }\n            if (iterations > 2 && Math.max(historyWorst, bestFitness) -\n                Math.min(historyBest, bestFitness) == 0) {\n                sigma = sigma * Math.exp(0.2 + cs / damps);\n            }\n            // store best in history\n            push(fitnessHistory,bestFitness);\n            fitfun.setValueRange(worstFitness-bestFitness);\n            if (generateStatistics) {\n                statisticsSigmaHistory.add(sigma);\n                statisticsFitnessHistory.add(bestFitness);\n                statisticsMeanHistory.add(xmean.transpose());\n                statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n            }\n        }\n        return optimum;\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n         // -------------------- Initialization --------------------------------\n        isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n        final FitnessFunction fitfun = new FitnessFunction();\n        final double[] guess = getStartPoint();\n        // number of objective variables/problem dimension\n        dimension = guess.length;\n        initializeCMA(guess);\n        iterations = 0;\n        double bestValue = fitfun.value(guess);\n        push(fitnessHistory, bestValue);\n        PointValuePair optimum\n            = new PointValuePair(getStartPoint(),\n                                 isMinimize ? bestValue : -bestValue);\n        PointValuePair lastResult = null;\n\n        // -------------------- Generation Loop --------------------------------\n\n        generationLoop:\n        for (iterations = 1; iterations <= maxIterations; iterations++) {\n\n            // Generate and evaluate lambda offspring\n            final RealMatrix arz = randn1(dimension, lambda);\n            final RealMatrix arx = zeros(dimension, lambda);\n            final double[] fitness = new double[lambda];\n            // generate random offspring\n            for (int k = 0; k < lambda; k++) {\n                RealMatrix arxk = null;\n                for (int i = 0; i < checkFeasableCount + 1; i++) {\n                    if (diagonalOnly <= 0) {\n                        arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k))\n                                         .scalarMultiply(sigma)); // m + sig * Normal(0,C)\n                    } else {\n                        arxk = xmean.add(times(diagD,arz.getColumnMatrix(k))\n                                         .scalarMultiply(sigma));\n                    }\n                    if (i >= checkFeasableCount ||\n                        fitfun.isFeasible(arxk.getColumn(0))) {\n                        break;\n                    }\n                    // regenerate random arguments for row\n                    arz.setColumn(k, randn(dimension));\n                }\n                copyColumn(arxk, 0, arx, k);\n                try {\n                    fitness[k] = fitfun.value(fitfun.repair(arx.getColumn(k))); // compute fitness on repaired solution\n                } catch (TooManyEvaluationsException e) {\n                    break generationLoop;\n                }\n            }\n            // Sort by fitness and compute weighted mean into xmean\n            final int[] arindex = sortedIndices(fitness);\n            // Calculate new xmean, this is selection and recombination\n            final RealMatrix xold = xmean; // for speed up of Eq. (2) and (3)\n            final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n            xmean = bestArx.multiply(weights);\n            final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n            final RealMatrix zmean = bestArz.multiply(weights);\n            final boolean hsig = updateEvolutionPaths(zmean, xold);\n            if (diagonalOnly <= 0) {\n                updateCovariance(hsig, bestArx, arz, arindex, xold);\n            } else {\n                updateCovarianceDiagonalOnly(hsig, bestArz);\n            }\n            // Adapt step size sigma - Eq. (5)\n            sigma *= Math.exp(Math.min(1, (normps/chiN - 1) * cs / damps));\n            final double bestFitness = fitness[arindex[0]];\n            final double worstFitness = fitness[arindex[arindex.length - 1]];\n            if (bestValue > bestFitness) {\n                bestValue = bestFitness;\n                lastResult = optimum;\n                optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)),\n                                             isMinimize ? bestFitness : -bestFitness);\n                if (getConvergenceChecker() != null &&\n                    lastResult != null) {\n                    if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n                        break generationLoop;\n                    }\n                }\n            }\n            // handle termination criteria\n            // Break, if fitness is good enough\n            if (stopFitness != 0) { // only if stopFitness is defined\n                if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n                    break generationLoop;\n                }\n            }\n            final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n            final double[] pcCol = pc.getColumn(0);\n            for (int i = 0; i < dimension; i++) {\n                if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n                    break;\n                }\n                if (i >= dimension - 1) {\n                    break generationLoop;\n                }\n            }\n            for (int i = 0; i < dimension; i++) {\n                if (sigma * sqrtDiagC[i] > stopTolUpX) {\n                    break generationLoop;\n                }\n            }\n            final double historyBest = min(fitnessHistory);\n            final double historyWorst = max(fitnessHistory);\n            if (iterations > 2 &&\n                Math.max(historyWorst, worstFitness) -\n                Math.min(historyBest, bestFitness) < stopTolFun) {\n                break generationLoop;\n            }\n            if (iterations > fitnessHistory.length &&\n                historyWorst - historyBest < stopTolHistFun) {\n                break generationLoop;\n            }\n            // condition number of the covariance matrix exceeds 1e14\n            if (max(diagD) / min(diagD) > 1e7) {\n                break generationLoop;\n            }\n            // user defined termination\n            if (getConvergenceChecker() != null) {\n                final PointValuePair current\n                    = new PointValuePair(bestArx.getColumn(0),\n                                         isMinimize ? bestFitness : -bestFitness);\n                if (lastResult != null &&\n                    getConvergenceChecker().converged(iterations, current, lastResult)) {\n                    break generationLoop;\n                    }\n                lastResult = current;\n            }\n            // Adjust step size in case of equal function values (flat fitness)\n            if (bestValue == fitness[arindex[(int)(0.1+lambda/4.)]]) {\n                sigma = sigma * Math.exp(0.2 + cs / damps);\n            }\n            if (iterations > 2 && Math.max(historyWorst, bestFitness) -\n                Math.min(historyBest, bestFitness) == 0) {\n                sigma = sigma * Math.exp(0.2 + cs / damps);\n            }\n            // store best in history\n            push(fitnessHistory,bestFitness);\n            fitfun.setValueRange(worstFitness-bestFitness);\n            if (generateStatistics) {\n                statisticsSigmaHistory.add(sigma);\n                statisticsFitnessHistory.add(bestFitness);\n                statisticsMeanHistory.add(xmean.transpose());\n                statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n            }\n        }\n        return optimum;\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n         // -------------------- Initialization --------------------------------\n        isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n        final FitnessFunction fitfun = new FitnessFunction();\n        final double[] guess = getStartPoint();\n        // number of objective variables/problem dimension\n        dimension = guess.length;\n        initializeCMA(guess);\n        iterations = 0;\n        double bestValue = fitfun.value(guess);\n        push(fitnessHistory, bestValue);\n        PointValuePair optimum\n            = new PointValuePair(getStartPoint(),\n                                 isMinimize ? bestValue : -bestValue);\n        PointValuePair lastResult = null;\n\n        // -------------------- Generation Loop --------------------------------\n\n        generationLoop:\n        for (iterations = 1; iterations <= maxIterations; iterations++) {\n\n            // Generate and evaluate lambda offspring\n            final RealMatrix arz = randn1(dimension, lambda);\n            final RealMatrix arx = zeros(dimension, lambda);\n            final double[] fitness = new double[lambda];\n            // generate random offspring\n            for (int k = 0; k < lambda; k++) {\n                RealMatrix arxk = null;\n                for (int i = 0; i < checkFeasableCount + 1; i++) {\n                    if (diagonalOnly <= 0) {\n                        arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k))\n                                         .scalarMultiply(sigma)); // m + sig * Normal(0,C)\n                    } else {\n                        arxk = xmean.add(times(diagD,arz.getColumnMatrix(k))\n                                         .scalarMultiply(sigma));\n                    }\n                    if (i >= checkFeasableCount ||\n                        fitfun.isFeasible(arxk.getColumn(0))) {\n                        break;\n                    }\n                    // regenerate random arguments for row\n                    arz.setColumn(k, randn(dimension));\n                }\n                copyColumn(arxk, 0, arx, k);\n                try {\n                    double potentialFitness = fitfun.value(arx.getColumn(k));\n                    if (!Double.isNaN(potentialFitness)) { // Ensure fitness is valid\n                        fitness[k] = potentialFitness; // compute fitness\n                    } else {\n                        fitness[k] = Double.POSITIVE_INFINITY; // Set to a worse value if invalid \n                    }\n                } catch (TooManyEvaluationsException e) {\n                    break generationLoop;\n                }\n            }\n            // Sort by fitness and compute weighted mean into xmean\n            final int[] arindex = sortedIndices(fitness);\n            // Calculate new xmean, this is selection and recombination\n            final RealMatrix xold = xmean; // for speed up of Eq. (2) and (3)\n            final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n            xmean = bestArx.multiply(weights);\n            final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n            final RealMatrix zmean = bestArz.multiply(weights);\n            final boolean hsig = updateEvolutionPaths(zmean, xold);\n            if (diagonalOnly <= 0) {\n                updateCovariance(hsig, bestArx, arz, arindex, xold);\n            } else {\n                updateCovarianceDiagonalOnly(hsig, bestArz);\n            }\n            // Adapt step size sigma - Eq. (5)\n            sigma *= Math.exp(Math.min(1, (normps/chiN - 1) * cs / damps));\n            final double bestFitness = fitness[arindex[0]];\n            final double worstFitness = fitness[arindex[arindex.length - 1]];\n            if (bestValue == 0 || bestValue > bestFitness) {\n                bestValue = bestFitness;\n                lastResult = optimum;\n                optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)),\n                                             isMinimize ? bestFitness : -bestFitness);\n                if (getConvergenceChecker() != null &&\n                    lastResult != null) {\n                    if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n                        break generationLoop;\n                    }\n                }\n            }\n            // handle termination criteria\n            // Break, if fitness is good enough\n            if (stopFitness != 0) { // only if stopFitness is defined\n                if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n                    break generationLoop;\n                }\n            }\n            final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n            final double[] pcCol = pc.getColumn(0);\n            for (int i = 0; i < dimension; i++) {\n                if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n                    break;\n                }\n                if (i >= dimension - 1) {\n                    break generationLoop;\n                }\n            }\n            for (int i = 0; i < dimension; i++) {\n                if (sigma * sqrtDiagC[i] > stopTolUpX) {\n                    break generationLoop;\n                }\n            }\n            final double historyBest = min(fitnessHistory);\n            final double historyWorst = max(fitnessHistory);\n            if (iterations > 2 &&\n                Math.max(historyWorst, worstFitness) -\n                Math.min(historyBest, bestFitness) < stopTolFun) {\n                break generationLoop;\n            }\n            if (iterations > fitnessHistory.length &&\n                historyWorst - historyBest < stopTolHistFun) {\n                break generationLoop;\n            }\n            // condition number of the covariance matrix exceeds 1e14\n            if (max(diagD) / min(diagD) > 1e7) {\n                break generationLoop;\n            }\n            // user defined termination\n            if (getConvergenceChecker() != null) {\n                final PointValuePair current\n                    = new PointValuePair(bestArx.getColumn(0),\n                                         isMinimize ? bestFitness : -bestFitness);\n                if (lastResult != null &&\n                    getConvergenceChecker().converged(iterations, current, lastResult)) {\n                    break generationLoop;\n                    }\n                lastResult = current;\n            }\n            // Adjust step size in case of equal function values (flat fitness)\n            if (bestValue == fitness[arindex[(int)(0.1+lambda/4.)]]) {\n                sigma = sigma * Math.exp(0.2 + cs / damps);\n            }\n            if (iterations > 2 && Math.max(historyWorst, bestFitness) -\n                Math.min(historyBest, bestFitness) == 0) {\n                sigma = sigma * Math.exp(0.2 + cs / damps);\n            }\n            // store best in history\n            push(fitnessHistory,bestFitness);\n            fitfun.setValueRange(worstFitness-bestFitness);\n            if (generateStatistics) {\n                statisticsSigmaHistory.add(sigma);\n                statisticsFitnessHistory.add(bestFitness);\n                statisticsMeanHistory.add(xmean.transpose());\n                statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n            }\n        }\n        return optimum;\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        // -------------------- Initialization --------------------------------\n        isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n        final FitnessFunction fitfun = new FitnessFunction();\n        final double[] guess = getStartPoint();\n        // number of objective variables/problem dimension\n        dimension = guess.length;\n        initializeCMA(guess);\n        iterations = 0;\n        double bestValue = fitfun.value(guess);\n        push(fitnessHistory, bestValue);\n        PointValuePair optimum\n            = new PointValuePair(getStartPoint(),\n                                 isMinimize ? bestValue : -bestValue);\n        PointValuePair lastResult = null;\n\n        // -------------------- Generation Loop --------------------------------\n\n        generationLoop:\n        for (iterations = 1; iterations <= maxIterations; iterations++) {\n\n            // Generate and evaluate lambda offspring\n            final RealMatrix arz = randn1(dimension, lambda);\n            final RealMatrix arx = zeros(dimension, lambda);\n            final double[] fitness = new double[lambda];\n            // generate random offspring\n            for (int k = 0; k < lambda; k++) {\n                RealMatrix arxk = null;\n                for (int i = 0; i < checkFeasableCount + 1; i++) {\n                    if (diagonalOnly <= 0) {\n                        arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k))\n                                         .scalarMultiply(sigma)); // m + sig * Normal(0,C)\n                    } else {\n                        arxk = xmean.add(times(diagD,arz.getColumnMatrix(k))\n                                         .scalarMultiply(sigma));\n                    }\n                    if (i >= checkFeasableCount ||\n                        fitfun.isFeasible(arxk.getColumn(0))) {\n                        break;\n                    }\n                    // regenerate random arguments for row\n                    arz.setColumn(k, randn(dimension));\n                }\n                copyColumn(arxk, 0, arx, k);\n                try {\n                    fitness[k] = fitfun.value(arx.getColumn(k)); // compute fitness\n                } catch (TooManyEvaluationsException e) {\n                    break generationLoop;\n                }\n            }\n            // Sort by fitness and compute weighted mean into xmean\n            final int[] arindex = sortedIndices(fitness);\n            // Calculate new xmean, this is selection and recombination\n            final RealMatrix xold = xmean; // for speed up of Eq. (2) and (3)\n            final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n            xmean = bestArx.multiply(weights);\n            final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n            final RealMatrix zmean = bestArz.multiply(weights);\n            final boolean hsig = updateEvolutionPaths(zmean, xold);\n            if (diagonalOnly <= 0) {\n                updateCovariance(hsig, bestArx, arz, arindex, xold);\n            } else {\n                updateCovarianceDiagonalOnly(hsig, bestArz);\n            }\n            // Adapt step size sigma - Eq. (5)\n            sigma *= Math.exp(Math.min(1, (normps/chiN - 1) * cs / damps));\n            final double bestFitness = fitness[arindex[0]];\n            final double worstFitness = fitness[arindex[arindex.length - 1]];\n            if (bestValue > bestFitness) {\n                lastResult = optimum;\n                bestValue = bestFitness;\n                optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)),\n                                             isMinimize ? bestFitness : -bestFitness);\n                if (getConvergenceChecker() != null &&\n                    lastResult != null) {\n                    if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n                        break generationLoop;\n                    }\n                }\n            }\n            // handle termination criteria\n            // Break, if fitness is good enough\n            if (stopFitness != 0) { // only if stopFitness is defined\n                if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n                    break generationLoop;\n                }\n            }\n            final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n            final double[] pcCol = pc.getColumn(0);\n            for (int i = 0; i < dimension; i++) {\n                if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n                    break;\n                }\n                if (i >= dimension - 1) {\n                    break generationLoop;\n                }\n            }\n            for (int i = 0; i < dimension; i++) {\n                if (sigma * sqrtDiagC[i] > stopTolUpX) {\n                    break generationLoop;\n                }\n            }\n            final double historyBest = min(fitnessHistory);\n            final double historyWorst = max(fitnessHistory);\n            if (iterations > 2 &&\n                Math.max(historyWorst, worstFitness) -\n                Math.min(historyBest, bestFitness) < stopTolFun) {\n                break generationLoop;\n            }\n            if (iterations > fitnessHistory.length &&\n                historyWorst - historyBest < stopTolHistFun) {\n                break generationLoop;\n            }\n            // condition number of the covariance matrix exceeds 1e14\n            if (max(diagD) / min(diagD) > 1e7) {\n                break generationLoop;\n            }\n            // user defined termination\n            if (getConvergenceChecker() != null) {\n                final PointValuePair current\n                    = new PointValuePair(bestArx.getColumn(0),\n                                         isMinimize ? bestFitness : -bestFitness);\n                if (lastResult != null &&\n                    getConvergenceChecker().converged(iterations, current, lastResult)) {\n                    break generationLoop;\n                }\n                lastResult = current;\n            }\n            // Adjust step size in case of equal function values (flat fitness)\n            if (bestValue == fitness[arindex[(int)(0.1+lambda/4.)]]) {\n                sigma = sigma * Math.exp(0.2 + cs / damps);\n            }\n            if (iterations > 2 && Math.max(historyWorst, bestFitness) -\n                Math.min(historyBest, bestFitness) == 0) {\n                sigma = sigma * Math.exp(0.2 + cs / damps);\n            }\n            // store best in history\n            push(fitnessHistory,bestFitness);\n            fitfun.setValueRange(worstFitness-bestFitness);\n            if (generateStatistics) {\n                statisticsSigmaHistory.add(sigma);\n                statisticsFitnessHistory.add(bestFitness);\n                statisticsMeanHistory.add(xmean.transpose());\n                statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n            }\n        }\n        return optimum;\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        // -------------------- Initialization --------------------------------\n        isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n        final FitnessFunction fitfun = new FitnessFunction();\n        final double[] guess = getStartPoint();\n        // number of objective variables/problem dimension\n        dimension = guess.length;\n        initializeCMA(guess);\n        iterations = 0;\n        double bestValue = fitfun.value(guess);\n        push(fitnessHistory, bestValue);\n        PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n\n        // -------------------- Generation Loop --------------------------------\n\n        generationLoop:\n        for (iterations = 1; iterations <= maxIterations; iterations++) {\n\n            // Generate and evaluate lambda offspring\n            final RealMatrix arz = randn1(dimension, lambda);\n            final RealMatrix arx = zeros(dimension, lambda);\n            final double[] fitness = new double[lambda];\n            // generate random offspring\n            for (int k = 0; k < lambda; k++) {\n                RealMatrix arxk = null;\n                for (int i = 0; i < checkFeasableCount + 1; i++) {\n                    if (diagonalOnly <= 0) {\n                        arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k))\n                                        .scalarMultiply(sigma)); // m + sig * Normal(0,C)\n                    } else {\n                        arxk = xmean.add(times(diagD, arz.getColumnMatrix(k))\n                                        .scalarMultiply(sigma));\n                    }\n                    if (i >= checkFeasableCount ||\n                        fitfun.isFeasible(arxk.getColumn(0))) {\n                        break;\n                    }\n                    // regenerate random arguments for row\n                    arz.setColumn(k, randn(dimension));\n                }\n                copyColumn(arxk, 0, arx, k);\n                try {\n                    fitness[k] = fitfun.value(arx.getColumn(k)); // compute fitness\n                } catch (TooManyEvaluationsException e) {\n                    break generationLoop;\n                }\n            }\n            // Sort by fitness and compute weighted mean into xmean\n            final int[] arindex = sortedIndices(fitness);\n            // Calculate new xmean, this is selection and recombination\n            final RealMatrix xold = xmean; // for speed up of Eq. (2) and (3)\n            final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n            xmean = bestArx.multiply(weights);\n            final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n            final RealMatrix zmean = bestArz.multiply(weights);\n            final boolean hsig = updateEvolutionPaths(zmean, xold);\n            if (diagonalOnly <= 0) {\n                updateCovariance(hsig, bestArx, arz, arindex, xold);\n            } else {\n                updateCovarianceDiagonalOnly(hsig, bestArz);\n            }\n            // Adapt step size sigma - Eq. (5)\n            sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps));\n            final double bestFitness = fitness[arindex[0]];\n            final double worstFitness = fitness[arindex[arindex.length - 1]];\n            if (bestValue > bestFitness) {\n                bestValue = bestFitness;\n                optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n                if (getConvergenceChecker() != null) {\n                    if (getConvergenceChecker().converged(iterations, optimum, new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness))) {\n                        break generationLoop;\n                    }\n                }\n            }\n            // handle termination criteria\n            // Break, if fitness is good enough\n            if (stopFitness != 0) { // only if stopFitness is defined\n                if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n                    break generationLoop;\n                }\n            }\n            final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n            final double[] pcCol = pc.getColumn(0);\n            for (int i = 0; i < dimension; i++) {\n                if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n                    break;\n                }\n                if (i >= dimension - 1) {\n                    break generationLoop;\n                }\n            }\n            for (int i = 0; i < dimension; i++) {\n                if (sigma * sqrtDiagC[i] > stopTolUpX) {\n                    break generationLoop;\n                }\n            }\n            final double historyBest = min(fitnessHistory);\n            final double historyWorst = max(fitnessHistory);\n            if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n                break generationLoop;\n            }\n            if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n                break generationLoop;\n            }\n            // condition number of the covariance matrix exceeds 1e14\n            if (max(diagD) / min(diagD) > 1e7) {\n                break generationLoop;\n            }\n            // user defined termination\n            if (getConvergenceChecker() != null) {\n                final PointValuePair current = new PointValuePair(bestArx.getColumn(0),\n                                        isMinimize ? bestFitness : -bestFitness);\n                if (getConvergenceChecker().converged(iterations, current, optimum)) {\n                    break generationLoop;\n                }\n            }\n            // Adjust step size in case of equal function values (flat fitness)\n            if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n                sigma = sigma * Math.exp(0.2 + cs / damps);\n            }\n            if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n                sigma = sigma * Math.exp(0.2 + cs / damps);\n            }\n            // store best in history\n            push(fitnessHistory, bestFitness);\n            fitfun.setValueRange(worstFitness - bestFitness);\n            if (generateStatistics) {\n                statisticsSigmaHistory.add(sigma);\n                statisticsFitnessHistory.add(bestFitness);\n                statisticsMeanHistory.add(xmean.transpose());\n                statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n            }\n        }\n        return optimum;\n    }\n"
                        ],
                        "plausible_patches": [
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n         // -------------------- Initialization --------------------------------\n        isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n        final FitnessFunction fitfun = new FitnessFunction();\n        final double[] guess = getStartPoint();\n        // number of objective variables/problem dimension\n        dimension = guess.length;\n        initializeCMA(guess);\n        iterations = 0;\n        double bestValue = fitfun.value(guess);\n        push(fitnessHistory, bestValue);\n        PointValuePair optimum\n            = new PointValuePair(getStartPoint(),\n                                 isMinimize ? bestValue : -bestValue);\n        PointValuePair lastResult = null;\n\n        // -------------------- Generation Loop --------------------------------\n\n        generationLoop:\n        for (iterations = 1; iterations <= maxIterations; iterations++) {\n\n            // Generate and evaluate lambda offspring\n            final RealMatrix arz = randn1(dimension, lambda);\n            final RealMatrix arx = zeros(dimension, lambda);\n            final double[] fitness = new double[lambda];\n            // generate random offspring\n            for (int k = 0; k < lambda; k++) {\n                RealMatrix arxk = null;\n                for (int i = 0; i < checkFeasableCount + 1; i++) {\n                    if (diagonalOnly <= 0) {\n                        arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k))\n                                         .scalarMultiply(sigma)); // m + sig * Normal(0,C)\n                    } else {\n                        arxk = xmean.add(times(diagD,arz.getColumnMatrix(k))\n                                         .scalarMultiply(sigma));\n                    }\n                    if (i >= checkFeasableCount ||\n                        fitfun.isFeasible(arxk.getColumn(0))) {\n                        break;\n                    }\n                    // regenerate random arguments for row\n                    arz.setColumn(k, randn(dimension));\n                }\n                copyColumn(arxk, 0, arx, k);\n                try {\n                    fitness[k] = fitfun.value(arx.getColumn(k)); // compute fitness\n                } catch (TooManyEvaluationsException e) {\n                    break generationLoop;\n                }\n            }\n            // Sort by fitness and compute weighted mean into xmean\n            final int[] arindex = sortedIndices(fitness);\n            // Calculate new xmean, this is selection and recombination\n            final RealMatrix xold = xmean; // for speed up of Eq. (2) and (3)\n            final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n            xmean = bestArx.multiply(weights);\n            final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n            final RealMatrix zmean = bestArz.multiply(weights);\n            final boolean hsig = updateEvolutionPaths(zmean, xold);\n            if (diagonalOnly <= 0) {\n                updateCovariance(hsig, bestArx, arz, arindex, xold);\n            } else {\n                updateCovarianceDiagonalOnly(hsig, bestArz);\n            }\n            // Adapt step size sigma - Eq. (5)\n            sigma *= Math.exp(Math.min(1, (normps/chiN - 1) * cs / damps));\n            final double bestFitness = fitness[arindex[0]];\n            final double worstFitness = fitness[arindex[arindex.length - 1]];\n            if (bestValue > bestFitness) {\n                bestValue = bestFitness;\n                lastResult = optimum;\n                optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)),\n                                             isMinimize ? bestFitness : -bestFitness);\n                if (getConvergenceChecker() != null &&\n                    lastResult != null) {\n                    if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n                        break generationLoop;\n                    }\n                }\n            }\n            // handle termination criteria\n            // Break, if fitness is good enough\n            if (stopFitness != 0) { // only if stopFitness is defined\n                if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n                    break generationLoop;\n                }\n            }\n            final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n            final double[] pcCol = pc.getColumn(0);\n            for (int i = 0; i < dimension; i++) {\n                if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n                    break;\n                }\n                if (i >= dimension - 1) {\n                    break generationLoop;\n                }\n            }\n            for (int i = 0; i < dimension; i++) {\n                if (sigma * sqrtDiagC[i] > stopTolUpX) {\n                    break generationLoop;\n                }\n            }\n            final double historyBest = min(fitnessHistory);\n            final double historyWorst = max(fitnessHistory);\n            if (iterations > 2 &&\n                Math.max(historyWorst, worstFitness) -\n                Math.min(historyBest, bestFitness) < stopTolFun) {\n                break generationLoop;\n            }\n            if (iterations > fitnessHistory.length &&\n                historyWorst - historyBest < stopTolHistFun) {\n                break generationLoop;\n            }\n            // condition number of the covariance matrix exceeds 1e7\n            if (max(diagD) / min(diagD) > 1e7) {\n                break generationLoop;\n            }\n            // user defined termination\n            if (getConvergenceChecker() != null) {\n                final PointValuePair current\n                    = new PointValuePair(bestArx.getColumn(0),\n                                         isMinimize ? bestFitness : -bestFitness);\n                if (lastResult != null &&\n                    getConvergenceChecker().converged(iterations, current, lastResult)) {\n                    break generationLoop;\n                    }\n                lastResult = current;\n            }\n            // Adjust step size in case of equal function values (flat fitness)\n            if (bestValue == fitness[arindex[(int)(0.1+lambda/4.)]]) {\n                sigma = sigma * Math.exp(0.2 + cs / damps);\n            }\n            if (iterations > 2 && Math.max(historyWorst, bestFitness) -\n                Math.min(historyBest, bestFitness) == 0) {\n                sigma = sigma * Math.exp(0.2 + cs / damps);\n            }\n            // store best in history\n            push(fitnessHistory,bestFitness);\n            fitfun.setValueRange(worstFitness-bestFitness);\n            if (generateStatistics) {\n                statisticsSigmaHistory.add(sigma);\n                statisticsFitnessHistory.add(bestFitness);\n                statisticsMeanHistory.add(xmean.transpose());\n                statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n            }\n        }\n        return optimum;\n    }\n",
                            "\n@Override\nprotected PointValuePair doOptimize() {\n    // -------------------- Initialization --------------------------------\n    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n    final FitnessFunction fitfun = new FitnessFunction();\n    final double[] guess = getStartPoint();\n    // number of objective variables/problem dimension\n    dimension = guess.length;\n    initializeCMA(guess);\n    iterations = 0;\n    double bestValue = fitfun.value(guess);\n    push(fitnessHistory, bestValue);\n    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n\n    // -------------------- Generation Loop --------------------------------\n    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n        // Generate and evaluate lambda offspring\n        final RealMatrix arz = randn1(dimension, lambda);\n        final RealMatrix arx = zeros(dimension, lambda);\n        final double[] fitness = new double[lambda];\n        // generate random offspring\n        for (int k = 0; k < lambda; k++) {\n            RealMatrix arxk = null;\n            for (int i = 0; i < checkFeasableCount + 1; i++) {\n                if (diagonalOnly <= 0) {\n                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(sigma)); // m + sig * Normal(0,C)\n                } else {\n                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n                }\n                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n                    break;\n                }\n                // regenerate random arguments for row\n                arz.setColumn(k, randn(dimension));\n            }\n            copyColumn(arxk, 0, arx, k);\n            try {\n                fitness[k] = fitfun.value(arx.getColumn(k)); // compute fitness\n            } catch (TooManyEvaluationsException e) {\n                break generationLoop;\n            }\n        }\n        // Sort by fitness and compute weighted mean into xmean\n        final int[] arindex = sortedIndices(fitness);\n        // Calculate new xmean, this is selection and recombination\n        final RealMatrix xold = xmean; // for speed up of Eq. (2) and (3)\n        final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n        xmean = bestArx.multiply(weights);\n        final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n        final RealMatrix zmean = bestArz.multiply(weights);\n        final boolean hsig = updateEvolutionPaths(zmean, xold);\n        if (diagonalOnly <= 0) {\n            updateCovariance(hsig, bestArx, arz, arindex, xold);\n        } else {\n            updateCovarianceDiagonalOnly(hsig, bestArz);\n        }\n        // Adapt step size sigma - Eq. (5)\n        sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps));\n        final double bestFitness = fitness[arindex[0]];\n        final double worstFitness = fitness[arindex[arindex.length - 1]];\n\n        if (bestValue > bestFitness) {\n            bestValue = bestFitness;\n            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n        }\n\n        // Check for convergence\n        if (getConvergenceChecker() != null) {\n            if (iterations > 1 && getConvergenceChecker().converged(iterations, optimum, new PointValuePair(fitfun.repair(xold.getColumn(0)), isMinimize ? bestValue : -bestValue))) {\n                break generationLoop;\n            }\n        }\n        \n        // handle termination criteria\n        // Break if fitness is good enough\n        if (stopFitness != 0) { // only if stopFitness is defined\n            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n                break generationLoop;\n            }\n        }\n        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n        final double[] pcCol = pc.getColumn(0);\n        for (int i = 0; i < dimension; i++) {\n            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n                break;\n            }\n            if (i >= dimension - 1) {\n                break generationLoop;\n            }\n        }\n        for (int i = 0; i < dimension; i++) {\n            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n                break generationLoop;\n            }\n        }\n        final double historyBest = min(fitnessHistory);\n        final double historyWorst = max(fitnessHistory);\n        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n            break generationLoop;\n        }\n        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n            break generationLoop;\n        }\n        // condition number of the covariance matrix exceeds 1e14\n        if (max(diagD) / min(diagD) > 1e7) {\n            break generationLoop;\n        }\n        // Adjust step size in case of equal function values (flat fitness)\n        if (bestValue == fitness[arindex[(int)(0.1 + lambda / 4.)]]) {\n            sigma = sigma * Math.exp(0.2 + cs / damps);\n        }\n        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n            sigma = sigma * Math.exp(0.2 + cs / damps);\n        }\n        // store best in history\n        push(fitnessHistory, bestFitness);\n        fitfun.setValueRange(worstFitness - bestFitness);\n        if (generateStatistics) {\n            statisticsSigmaHistory.add(sigma);\n            statisticsFitnessHistory.add(bestFitness);\n            statisticsMeanHistory.add(xmean.transpose());\n            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n        }\n    }\n    return optimum;\n}\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n         // -------------------- Initialization --------------------------------\n        isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n        final FitnessFunction fitfun = new FitnessFunction();\n        final double[] guess = getStartPoint();\n        // number of objective variables/problem dimension\n        dimension = guess.length;\n        initializeCMA(guess);\n        iterations = 0;\n        double bestValue = fitfun.value(guess);\n        push(fitnessHistory, bestValue);\n        PointValuePair optimum\n            = new PointValuePair(getStartPoint(),\n                                 isMinimize ? bestValue : -bestValue);\n        PointValuePair lastResult = null;\n\n        // -------------------- Generation Loop --------------------------------\n\n        generationLoop:\n        for (iterations = 1; iterations <= maxIterations; iterations++) {\n\n            // Generate and evaluate lambda offspring\n            final RealMatrix arz = randn1(dimension, lambda);\n            final RealMatrix arx = zeros(dimension, lambda);\n            final double[] fitness = new double[lambda];\n            // generate random offspring\n            for (int k = 0; k < lambda; k++) {\n                RealMatrix arxk = null;\n                for (int i = 0; i < checkFeasableCount + 1; i++) {\n                    if (diagonalOnly <= 0) {\n                        arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k))\n                                         .scalarMultiply(sigma)); // m + sig * Normal(0,C)\n                    } else {\n                        arxk = xmean.add(times(diagD,arz.getColumnMatrix(k))\n                                         .scalarMultiply(sigma));\n                    }\n                    if (i >= checkFeasableCount ||\n                        fitfun.isFeasible(arxk.getColumn(0))) {\n                        break;\n                    }\n                    // regenerate random arguments for row\n                    arz.setColumn(k, randn(dimension));\n                }\n                copyColumn(arxk, 0, arx, k);\n                try {\n                    fitness[k] = fitfun.value(arx.getColumn(k)); // compute fitness\n                } catch (TooManyEvaluationsException e) {\n                    break generationLoop;\n                }\n            }\n            // Sort by fitness and compute weighted mean into xmean\n            final int[] arindex = sortedIndices(fitness);\n            // Calculate new xmean, this is selection and recombination\n            final RealMatrix xold = xmean; // for speed up of Eq. (2) and (3)\n            final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n            xmean = bestArx.multiply(weights);\n            final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n            final RealMatrix zmean = bestArz.multiply(weights);\n            final boolean hsig = updateEvolutionPaths(zmean, xold);\n            if (diagonalOnly <= 0) {\n                updateCovariance(hsig, bestArx, arz, arindex, xold);\n            } else {\n                updateCovarianceDiagonalOnly(hsig, bestArz);\n            }\n            // Adapt step size sigma - Eq. (5)\n            sigma *= Math.exp(Math.min(1, (normps/chiN - 1) * cs / damps));\n            final double bestFitness = fitness[arindex[0]];\n            final double worstFitness = fitness[arindex[arindex.length - 1]];\n            if (bestValue > bestFitness) {\n                bestValue = bestFitness;\n                lastResult = optimum;\n                optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)),\n                                             isMinimize ? bestFitness : -bestFitness);\n                if (getConvergenceChecker() != null &&\n                    lastResult != null) {\n                    if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n                        break generationLoop;\n                    }\n                }\n            }\n            // handle termination criteria\n            // Break, if fitness is good enough\n            if (stopFitness != 0) { // only if stopFitness is defined\n                if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n                    break generationLoop;\n                }\n            }\n            final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n            final double[] pcCol = pc.getColumn(0);\n            boolean breakLoop = false;\n            for (int i = 0; i < dimension; i++) {\n                if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n                    breakLoop = true;\n                    break;\n                }\n            }\n            if (!breakLoop) {\n                for (int i = 0; i < dimension; i++) {\n                    if (sigma * sqrtDiagC[i] > stopTolUpX) {\n                        breakLoop = true;\n                        break;\n                    }\n                }\n            }\n            if (breakLoop) {\n                break generationLoop;\n            }\n            final double historyBest = min(fitnessHistory);\n            final double historyWorst = max(fitnessHistory);\n            if (iterations > 2 &&\n                Math.max(historyWorst, worstFitness) -\n                Math.min(historyBest, bestFitness) < stopTolFun) {\n                break generationLoop;\n            }\n            if (iterations > fitnessHistory.length &&\n                historyWorst - historyBest < stopTolHistFun) {\n                break generationLoop;\n            }\n            // condition number of the covariance matrix exceeds 1e14\n            if (max(diagD) / min(diagD) > 1e7) {\n                break generationLoop;\n            }\n            // user defined termination\n            if (getConvergenceChecker() != null) {\n                final PointValuePair current\n                    = new PointValuePair(bestArx.getColumn(0),\n                                         isMinimize ? bestFitness : -bestFitness);\n                if (lastResult != null &&\n                    getConvergenceChecker().converged(iterations, current, lastResult)) {\n                    break generationLoop;\n                    }\n                lastResult = current;\n            }\n            // Adjust step size in case of equal function values (flat fitness)\n            if (bestValue == fitness[arindex[(int)(0.1+lambda/4.)]]) {\n                sigma = sigma * Math.exp(0.2 + cs / damps);\n            }\n            if (iterations > 2 && Math.max(historyWorst, bestFitness) -\n                Math.min(historyBest, bestFitness) == 0) {\n                sigma = sigma * Math.exp(0.2 + cs / damps);\n            }\n            // store best in history\n            push(fitnessHistory,bestFitness);\n            fitfun.setValueRange(worstFitness-bestFitness);\n            if (generateStatistics) {\n                statisticsSigmaHistory.add(sigma);\n                statisticsFitnessHistory.add(bestFitness);\n                statisticsMeanHistory.add(xmean.transpose());\n                statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n            }\n        }\n        return optimum;\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        // -------------------- Initialization --------------------------------\n        isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n        final FitnessFunction fitfun = new FitnessFunction();\n        final double[] guess = getStartPoint();\n        // number of objective variables/problem dimension\n        dimension = guess.length;\n        initializeCMA(guess);\n        iterations = 0;\n        double bestValue = fitfun.value(guess);\n        push(fitnessHistory, bestValue);\n        PointValuePair optimum\n            = new PointValuePair(getStartPoint(),\n                                 isMinimize ? bestValue : -bestValue);\n        PointValuePair lastResult = null;\n\n        // -------------------- Generation Loop --------------------------------\n\n        generationLoop:\n        for (iterations = 1; iterations <= maxIterations; iterations++) {\n\n            // Generate and evaluate lambda offspring\n            final RealMatrix arz = randn1(dimension, lambda);\n            final RealMatrix arx = zeros(dimension, lambda);\n            final double[] fitness = new double[lambda];\n            // generate random offspring\n            for (int k = 0; k < lambda; k++) {\n                RealMatrix arxk = null;\n                for (int i = 0; i < checkFeasableCount + 1; i++) {\n                    if (diagonalOnly <= 0) {\n                        arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k))\n                                         .scalarMultiply(sigma)); // m + sig * Normal(0,C)\n                    } else {\n                        arxk = xmean.add(times(diagD,arz.getColumnMatrix(k))\n                                         .scalarMultiply(sigma));\n                    }\n                    if (i >= checkFeasableCount ||\n                        fitfun.isFeasible(arxk.getColumn(0))) {\n                        break;\n                    }\n                    // regenerate random arguments for row\n                    arz.setColumn(k, randn(dimension));\n                }\n                copyColumn(arxk, 0, arx, k);\n                try {\n                    fitness[k] = fitfun.value(arx.getColumn(k)); // compute fitness\n                } catch (TooManyEvaluationsException e) {\n                    break generationLoop;\n                }\n            }\n            // Sort by fitness and compute weighted mean into xmean\n            final int[] arindex = sortedIndices(fitness);\n            // Calculate new xmean, this is selection and recombination\n            final RealMatrix xold = xmean; // for speed up of Eq. (2) and (3)\n            final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n            xmean = bestArx.multiply(weights);\n            final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n            final RealMatrix zmean = bestArz.multiply(weights);\n            final boolean hsig = updateEvolutionPaths(zmean, xold);\n            if (diagonalOnly <= 0) {\n                updateCovariance(hsig, bestArx, arz, arindex, xold);\n            } else {\n                updateCovarianceDiagonalOnly(hsig, bestArz);\n            }\n            // Adapt step size sigma - Eq. (5)\n            sigma *= Math.exp(Math.min(1, (normps/chiN - 1) * cs / damps));\n            final double bestFitness = fitness[arindex[0]];\n            final double worstFitness = fitness[arindex[arindex.length - 1]];\n            if (bestValue > bestFitness) {\n                bestValue = bestFitness;\n                lastResult = optimum;\n                optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)),\n                                             isMinimize ? bestFitness : -bestFitness);\n                if (getConvergenceChecker() != null &&\n                    lastResult != null) {\n                    if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n                        break generationLoop;\n                    }\n                }\n            }\n            // handle termination criteria\n            // Break, if fitness is good enough\n            if (stopFitness != 0) { // only if stopFitness is defined\n                if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n                    break generationLoop;\n                }\n            }\n            final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n            final double[] pcCol = pc.getColumn(0);\n            for (int i = 0; i < dimension; i++) {\n                if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n                    break;\n                }\n                if (i >= dimension - 1) {\n                    break generationLoop;\n                }\n            }\n            for (int i = 0; i < dimension; i++) {\n                if (sigma * sqrtDiagC[i] > stopTolUpX) {\n                    break generationLoop;\n                }\n            }\n            final double historyBest = min(fitnessHistory);\n            final double historyWorst = max(fitnessHistory);\n            if (iterations > 2 &&\n                Math.max(historyWorst, worstFitness) -\n                Math.min(historyBest, bestFitness) < stopTolFun) {\n                break generationLoop;\n            }\n            if (iterations > fitnessHistory.length &&\n                historyWorst - historyBest < stopTolHistFun) {\n                break generationLoop;\n            }\n            // condition number of the covariance matrix exceeds 1e14\n            if (max(diagD) / min(diagD) > 1e7) {\n                break generationLoop;\n            }\n            // user defined termination\n            if (getConvergenceChecker() != null) {\n                final PointValuePair current\n                    = new PointValuePair(bestArx.getColumn(0),\n                                         isMinimize ? bestFitness : -bestFitness);\n                lastResult = current;\n                if (getConvergenceChecker().converged(iterations, current, optimum)) {\n                    break generationLoop;\n                }\n            }\n            // Adjust step size in case of equal function values (flat fitness)\n            if (bestValue == fitness[arindex[(int)(0.1+lambda/4.)]]) {\n                sigma = sigma * Math.exp(0.2 + cs / damps);\n            }\n            if (iterations > 2 && Math.max(historyWorst, bestFitness) -\n                Math.min(historyBest, bestFitness) == 0) {\n                sigma = sigma * Math.exp(0.2 + cs / damps);\n            }\n            // store best in history\n            push(fitnessHistory,bestFitness);\n            fitfun.setValueRange(worstFitness-bestFitness);\n            if (generateStatistics) {\n                statisticsSigmaHistory.add(sigma);\n                statisticsFitnessHistory.add(bestFitness);\n                statisticsMeanHistory.add(xmean.transpose());\n                statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n            }\n        }\n        return optimum;\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n         // -------------------- Initialization --------------------------------\n        isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n        final FitnessFunction fitfun = new FitnessFunction();\n        final double[] guess = getStartPoint();\n        // number of objective variables/problem dimension\n        dimension = guess.length;\n        initializeCMA(guess);\n        iterations = 0;\n        double bestValue = fitfun.value(guess);\n        push(fitnessHistory, bestValue);\n        PointValuePair optimum\n            = new PointValuePair(getStartPoint(),\n                                 isMinimize ? bestValue : -bestValue);\n        PointValuePair lastResult = null;\n\n        // -------------------- Generation Loop --------------------------------\n\n        generationLoop:\n        for (iterations = 1; iterations <= maxIterations; iterations++) {\n\n            // Generate and evaluate lambda offspring\n            final RealMatrix arz = randn1(dimension, lambda);\n            final RealMatrix arx = zeros(dimension, lambda);\n            final double[] fitness = new double[lambda];\n            // generate random offspring\n            for (int k = 0; k < lambda; k++) {\n                RealMatrix arxk = null;\n                for (int i = 0; i < checkFeasableCount + 1; i++) {\n                    if (diagonalOnly <= 0) {\n                        arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k))\n                                         .scalarMultiply(sigma)); // m + sig * Normal(0,C)\n                    } else {\n                        arxk = xmean.add(times(diagD,arz.getColumnMatrix(k))\n                                         .scalarMultiply(sigma));\n                    }\n                    if (i >= checkFeasableCount ||\n                        fitfun.isFeasible(arxk.getColumn(0))) {\n                        break;\n                    }\n                    // regenerate random arguments for row\n                    arz.setColumn(k, randn(dimension));\n                }\n                copyColumn(arxk, 0, arx, k);\n                try {\n                    fitness[k] = fitfun.value(arx.getColumn(k)); // compute fitness\n                } catch (TooManyEvaluationsException e) {\n                    break generationLoop;\n                }\n            }\n            // Sort by fitness and compute weighted mean into xmean\n            final int[] arindex = sortedIndices(fitness);\n            // Calculate new xmean, this is selection and recombination\n            final RealMatrix xold = xmean; // for speed up of Eq. (2) and (3)\n            final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n            xmean = bestArx.multiply(weights);\n            final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n            final RealMatrix zmean = bestArz.multiply(weights);\n            final boolean hsig = updateEvolutionPaths(zmean, xold);\n            if (diagonalOnly <= 0) {\n                updateCovariance(hsig, bestArx, arz, arindex, xold);\n            } else {\n                updateCovarianceDiagonalOnly(hsig, bestArz);\n            }\n            // Adapt step size sigma - Eq. (5)\n            sigma *= Math.exp(Math.min(1, (normps/chiN - 1) * cs / damps));\n            final double bestFitness = fitness[arindex[0]];\n            final double worstFitness = fitness[arindex[arindex.length - 1]];\n            if (bestValue > bestFitness) {\n                bestValue = bestFitness;\n                lastResult = optimum;\n                optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)),\n                                             isMinimize ? bestFitness : -bestFitness);\n\n                // Fix: Ensure lastResult is assigned before checking convergence\n                if (getConvergenceChecker() != null) {\n                    lastResult = optimum;\n                    if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n                        break generationLoop;\n                    }\n                }\n            }\n            // handle termination criteria\n            // Break, if fitness is good enough\n            if (stopFitness != 0) { // only if stopFitness is defined\n                if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n                    break generationLoop;\n                }\n            }\n            final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n            final double[] pcCol = pc.getColumn(0);\n            for (int i = 0; i < dimension; i++) {\n                if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n                    break;\n                }\n                if (i >= dimension - 1) {\n                    break generationLoop;\n                }\n            }\n            for (int i = 0; i < dimension; i++) {\n                if (sigma * sqrtDiagC[i] > stopTolUpX) {\n                    break generationLoop;\n                }\n            }\n            final double historyBest = min(fitnessHistory);\n            final double historyWorst = max(fitnessHistory);\n            if (iterations > 2 &&\n                Math.max(historyWorst, worstFitness) -\n                Math.min(historyBest, bestFitness) < stopTolFun) {\n                break generationLoop;\n            }\n            if (iterations > fitnessHistory.length &&\n                historyWorst - historyBest < stopTolHistFun) {\n                break generationLoop;\n            }\n            // condition number of the covariance matrix exceeds 1e14\n            if (max(diagD) / min(diagD) > 1e7) {\n                break generationLoop;\n            }\n            // user defined termination\n            if (getConvergenceChecker() != null) {\n                final PointValuePair current\n                    = new PointValuePair(bestArx.getColumn(0),\n                                         isMinimize ? bestFitness : -bestFitness);\n                if (lastResult != null &&\n                    getConvergenceChecker().converged(iterations, current, lastResult)) {\n                    break generationLoop;\n                    }\n                lastResult = current;\n            }\n            // Adjust step size in case of equal function values (flat fitness)\n            if (bestValue == fitness[arindex[(int)(0.1+lambda/4.)]]) {\n                sigma = sigma * Math.exp(0.2 + cs / damps);\n            }\n            if (iterations > 2 && Math.max(historyWorst, bestFitness) -\n                Math.min(historyBest, bestFitness) == 0) {\n                sigma = sigma * Math.exp(0.2 + cs / damps);\n            }\n            // store best in history\n            push(fitnessHistory,bestFitness);\n            fitfun.setValueRange(worstFitness-bestFitness);\n            if (generateStatistics) {\n                statisticsSigmaHistory.add(sigma);\n                statisticsFitnessHistory.add(bestFitness);\n                statisticsMeanHistory.add(xmean.transpose());\n                statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n            }\n        }\n        return optimum;\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n         // -------------------- Initialization --------------------------------\n        isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n        final FitnessFunction fitfun = new FitnessFunction();\n        final double[] guess = getStartPoint();\n        // number of objective variables/problem dimension\n        dimension = guess.length;\n        initializeCMA(guess);\n        iterations = 0;\n        double bestValue = fitfun.value(guess);\n        push(fitnessHistory, bestValue);\n        PointValuePair optimum\n            = new PointValuePair(getStartPoint(),\n                                 isMinimize ? bestValue : -bestValue);\n        PointValuePair lastResult = null;\n\n        // -------------------- Generation Loop --------------------------------\n\n        generationLoop:\n        for (iterations = 1; iterations <= maxIterations; iterations++) {\n\n            // Generate and evaluate lambda offspring\n            final RealMatrix arz = randn1(dimension, lambda);\n            final RealMatrix arx = zeros(dimension, lambda);\n            final double[] fitness = new double[lambda];\n            // generate random offspring\n            for (int k = 0; k < lambda; k++) {\n                RealMatrix arxk = null;\n                for (int i = 0; i < checkFeasableCount + 1; i++) {\n                    if (diagonalOnly <= 0) {\n                        arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k))\n                                         .scalarMultiply(sigma)); // m + sig * Normal(0,C)\n                    } else {\n                        arxk = xmean.add(times(diagD,arz.getColumnMatrix(k))\n                                         .scalarMultiply(sigma));\n                    }\n                    if (i >= checkFeasableCount ||\n                        fitfun.isFeasible(arxk.getColumn(0))) {\n                        break;\n                    }\n                    // regenerate random arguments for row\n                    arz.setColumn(k, randn(dimension));\n                }\n                copyColumn(arxk, 0, arx, k);\n                try {\n                    fitness[k] = fitfun.value(fitfun.repair(arx.getColumn(k))); // compute fitness on repaired solution\n                } catch (TooManyEvaluationsException e) {\n                    break generationLoop;\n                }\n            }\n            // Sort by fitness and compute weighted mean into xmean\n            final int[] arindex = sortedIndices(fitness);\n            // Calculate new xmean, this is selection and recombination\n            final RealMatrix xold = xmean; // for speed up of Eq. (2) and (3)\n            final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n            xmean = bestArx.multiply(weights);\n            final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n            final RealMatrix zmean = bestArz.multiply(weights);\n            final boolean hsig = updateEvolutionPaths(zmean, xold);\n            if (diagonalOnly <= 0) {\n                updateCovariance(hsig, bestArx, arz, arindex, xold);\n            } else {\n                updateCovarianceDiagonalOnly(hsig, bestArz);\n            }\n            // Adapt step size sigma - Eq. (5)\n            sigma *= Math.exp(Math.min(1, (normps/chiN - 1) * cs / damps));\n            final double bestFitness = fitness[arindex[0]];\n            final double worstFitness = fitness[arindex[arindex.length - 1]];\n            if (bestValue > bestFitness) {\n                bestValue = bestFitness;\n                lastResult = optimum;\n                optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)),\n                                             isMinimize ? bestFitness : -bestFitness);\n                if (getConvergenceChecker() != null &&\n                    lastResult != null) {\n                    if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n                        break generationLoop;\n                    }\n                }\n            }\n            // handle termination criteria\n            // Break, if fitness is good enough\n            if (stopFitness != 0) { // only if stopFitness is defined\n                if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n                    break generationLoop;\n                }\n            }\n            final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n            final double[] pcCol = pc.getColumn(0);\n            for (int i = 0; i < dimension; i++) {\n                if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n                    break;\n                }\n                if (i >= dimension - 1) {\n                    break generationLoop;\n                }\n            }\n            for (int i = 0; i < dimension; i++) {\n                if (sigma * sqrtDiagC[i] > stopTolUpX) {\n                    break generationLoop;\n                }\n            }\n            final double historyBest = min(fitnessHistory);\n            final double historyWorst = max(fitnessHistory);\n            if (iterations > 2 &&\n                Math.max(historyWorst, worstFitness) -\n                Math.min(historyBest, bestFitness) < stopTolFun) {\n                break generationLoop;\n            }\n            if (iterations > fitnessHistory.length &&\n                historyWorst - historyBest < stopTolHistFun) {\n                break generationLoop;\n            }\n            // condition number of the covariance matrix exceeds 1e14\n            if (max(diagD) / min(diagD) > 1e7) {\n                break generationLoop;\n            }\n            // user defined termination\n            if (getConvergenceChecker() != null) {\n                final PointValuePair current\n                    = new PointValuePair(bestArx.getColumn(0),\n                                         isMinimize ? bestFitness : -bestFitness);\n                if (lastResult != null &&\n                    getConvergenceChecker().converged(iterations, current, lastResult)) {\n                    break generationLoop;\n                    }\n                lastResult = current;\n            }\n            // Adjust step size in case of equal function values (flat fitness)\n            if (bestValue == fitness[arindex[(int)(0.1+lambda/4.)]]) {\n                sigma = sigma * Math.exp(0.2 + cs / damps);\n            }\n            if (iterations > 2 && Math.max(historyWorst, bestFitness) -\n                Math.min(historyBest, bestFitness) == 0) {\n                sigma = sigma * Math.exp(0.2 + cs / damps);\n            }\n            // store best in history\n            push(fitnessHistory,bestFitness);\n            fitfun.setValueRange(worstFitness-bestFitness);\n            if (generateStatistics) {\n                statisticsSigmaHistory.add(sigma);\n                statisticsFitnessHistory.add(bestFitness);\n                statisticsMeanHistory.add(xmean.transpose());\n                statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n            }\n        }\n        return optimum;\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n         // -------------------- Initialization --------------------------------\n        isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n        final FitnessFunction fitfun = new FitnessFunction();\n        final double[] guess = getStartPoint();\n        // number of objective variables/problem dimension\n        dimension = guess.length;\n        initializeCMA(guess);\n        iterations = 0;\n        double bestValue = fitfun.value(guess);\n        push(fitnessHistory, bestValue);\n        PointValuePair optimum\n            = new PointValuePair(getStartPoint(),\n                                 isMinimize ? bestValue : -bestValue);\n        PointValuePair lastResult = null;\n\n        // -------------------- Generation Loop --------------------------------\n\n        generationLoop:\n        for (iterations = 1; iterations <= maxIterations; iterations++) {\n\n            // Generate and evaluate lambda offspring\n            final RealMatrix arz = randn1(dimension, lambda);\n            final RealMatrix arx = zeros(dimension, lambda);\n            final double[] fitness = new double[lambda];\n            // generate random offspring\n            for (int k = 0; k < lambda; k++) {\n                RealMatrix arxk = null;\n                for (int i = 0; i < checkFeasableCount + 1; i++) {\n                    if (diagonalOnly <= 0) {\n                        arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k))\n                                         .scalarMultiply(sigma)); // m + sig * Normal(0,C)\n                    } else {\n                        arxk = xmean.add(times(diagD,arz.getColumnMatrix(k))\n                                         .scalarMultiply(sigma));\n                    }\n                    if (i >= checkFeasableCount ||\n                        fitfun.isFeasible(arxk.getColumn(0))) {\n                        break;\n                    }\n                    // regenerate random arguments for row\n                    arz.setColumn(k, randn(dimension));\n                }\n                copyColumn(arxk, 0, arx, k);\n                try {\n                    double potentialFitness = fitfun.value(arx.getColumn(k));\n                    if (!Double.isNaN(potentialFitness)) { // Ensure fitness is valid\n                        fitness[k] = potentialFitness; // compute fitness\n                    } else {\n                        fitness[k] = Double.POSITIVE_INFINITY; // Set to a worse value if invalid \n                    }\n                } catch (TooManyEvaluationsException e) {\n                    break generationLoop;\n                }\n            }\n            // Sort by fitness and compute weighted mean into xmean\n            final int[] arindex = sortedIndices(fitness);\n            // Calculate new xmean, this is selection and recombination\n            final RealMatrix xold = xmean; // for speed up of Eq. (2) and (3)\n            final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n            xmean = bestArx.multiply(weights);\n            final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n            final RealMatrix zmean = bestArz.multiply(weights);\n            final boolean hsig = updateEvolutionPaths(zmean, xold);\n            if (diagonalOnly <= 0) {\n                updateCovariance(hsig, bestArx, arz, arindex, xold);\n            } else {\n                updateCovarianceDiagonalOnly(hsig, bestArz);\n            }\n            // Adapt step size sigma - Eq. (5)\n            sigma *= Math.exp(Math.min(1, (normps/chiN - 1) * cs / damps));\n            final double bestFitness = fitness[arindex[0]];\n            final double worstFitness = fitness[arindex[arindex.length - 1]];\n            if (bestValue == 0 || bestValue > bestFitness) {\n                bestValue = bestFitness;\n                lastResult = optimum;\n                optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)),\n                                             isMinimize ? bestFitness : -bestFitness);\n                if (getConvergenceChecker() != null &&\n                    lastResult != null) {\n                    if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n                        break generationLoop;\n                    }\n                }\n            }\n            // handle termination criteria\n            // Break, if fitness is good enough\n            if (stopFitness != 0) { // only if stopFitness is defined\n                if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n                    break generationLoop;\n                }\n            }\n            final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n            final double[] pcCol = pc.getColumn(0);\n            for (int i = 0; i < dimension; i++) {\n                if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n                    break;\n                }\n                if (i >= dimension - 1) {\n                    break generationLoop;\n                }\n            }\n            for (int i = 0; i < dimension; i++) {\n                if (sigma * sqrtDiagC[i] > stopTolUpX) {\n                    break generationLoop;\n                }\n            }\n            final double historyBest = min(fitnessHistory);\n            final double historyWorst = max(fitnessHistory);\n            if (iterations > 2 &&\n                Math.max(historyWorst, worstFitness) -\n                Math.min(historyBest, bestFitness) < stopTolFun) {\n                break generationLoop;\n            }\n            if (iterations > fitnessHistory.length &&\n                historyWorst - historyBest < stopTolHistFun) {\n                break generationLoop;\n            }\n            // condition number of the covariance matrix exceeds 1e14\n            if (max(diagD) / min(diagD) > 1e7) {\n                break generationLoop;\n            }\n            // user defined termination\n            if (getConvergenceChecker() != null) {\n                final PointValuePair current\n                    = new PointValuePair(bestArx.getColumn(0),\n                                         isMinimize ? bestFitness : -bestFitness);\n                if (lastResult != null &&\n                    getConvergenceChecker().converged(iterations, current, lastResult)) {\n                    break generationLoop;\n                    }\n                lastResult = current;\n            }\n            // Adjust step size in case of equal function values (flat fitness)\n            if (bestValue == fitness[arindex[(int)(0.1+lambda/4.)]]) {\n                sigma = sigma * Math.exp(0.2 + cs / damps);\n            }\n            if (iterations > 2 && Math.max(historyWorst, bestFitness) -\n                Math.min(historyBest, bestFitness) == 0) {\n                sigma = sigma * Math.exp(0.2 + cs / damps);\n            }\n            // store best in history\n            push(fitnessHistory,bestFitness);\n            fitfun.setValueRange(worstFitness-bestFitness);\n            if (generateStatistics) {\n                statisticsSigmaHistory.add(sigma);\n                statisticsFitnessHistory.add(bestFitness);\n                statisticsMeanHistory.add(xmean.transpose());\n                statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n            }\n        }\n        return optimum;\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        // -------------------- Initialization --------------------------------\n        isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n        final FitnessFunction fitfun = new FitnessFunction();\n        final double[] guess = getStartPoint();\n        // number of objective variables/problem dimension\n        dimension = guess.length;\n        initializeCMA(guess);\n        iterations = 0;\n        double bestValue = fitfun.value(guess);\n        push(fitnessHistory, bestValue);\n        PointValuePair optimum\n            = new PointValuePair(getStartPoint(),\n                                 isMinimize ? bestValue : -bestValue);\n        PointValuePair lastResult = null;\n\n        // -------------------- Generation Loop --------------------------------\n\n        generationLoop:\n        for (iterations = 1; iterations <= maxIterations; iterations++) {\n\n            // Generate and evaluate lambda offspring\n            final RealMatrix arz = randn1(dimension, lambda);\n            final RealMatrix arx = zeros(dimension, lambda);\n            final double[] fitness = new double[lambda];\n            // generate random offspring\n            for (int k = 0; k < lambda; k++) {\n                RealMatrix arxk = null;\n                for (int i = 0; i < checkFeasableCount + 1; i++) {\n                    if (diagonalOnly <= 0) {\n                        arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k))\n                                         .scalarMultiply(sigma)); // m + sig * Normal(0,C)\n                    } else {\n                        arxk = xmean.add(times(diagD,arz.getColumnMatrix(k))\n                                         .scalarMultiply(sigma));\n                    }\n                    if (i >= checkFeasableCount ||\n                        fitfun.isFeasible(arxk.getColumn(0))) {\n                        break;\n                    }\n                    // regenerate random arguments for row\n                    arz.setColumn(k, randn(dimension));\n                }\n                copyColumn(arxk, 0, arx, k);\n                try {\n                    fitness[k] = fitfun.value(arx.getColumn(k)); // compute fitness\n                } catch (TooManyEvaluationsException e) {\n                    break generationLoop;\n                }\n            }\n            // Sort by fitness and compute weighted mean into xmean\n            final int[] arindex = sortedIndices(fitness);\n            // Calculate new xmean, this is selection and recombination\n            final RealMatrix xold = xmean; // for speed up of Eq. (2) and (3)\n            final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n            xmean = bestArx.multiply(weights);\n            final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n            final RealMatrix zmean = bestArz.multiply(weights);\n            final boolean hsig = updateEvolutionPaths(zmean, xold);\n            if (diagonalOnly <= 0) {\n                updateCovariance(hsig, bestArx, arz, arindex, xold);\n            } else {\n                updateCovarianceDiagonalOnly(hsig, bestArz);\n            }\n            // Adapt step size sigma - Eq. (5)\n            sigma *= Math.exp(Math.min(1, (normps/chiN - 1) * cs / damps));\n            final double bestFitness = fitness[arindex[0]];\n            final double worstFitness = fitness[arindex[arindex.length - 1]];\n            if (bestValue > bestFitness) {\n                lastResult = optimum;\n                bestValue = bestFitness;\n                optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)),\n                                             isMinimize ? bestFitness : -bestFitness);\n                if (getConvergenceChecker() != null &&\n                    lastResult != null) {\n                    if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n                        break generationLoop;\n                    }\n                }\n            }\n            // handle termination criteria\n            // Break, if fitness is good enough\n            if (stopFitness != 0) { // only if stopFitness is defined\n                if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n                    break generationLoop;\n                }\n            }\n            final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n            final double[] pcCol = pc.getColumn(0);\n            for (int i = 0; i < dimension; i++) {\n                if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n                    break;\n                }\n                if (i >= dimension - 1) {\n                    break generationLoop;\n                }\n            }\n            for (int i = 0; i < dimension; i++) {\n                if (sigma * sqrtDiagC[i] > stopTolUpX) {\n                    break generationLoop;\n                }\n            }\n            final double historyBest = min(fitnessHistory);\n            final double historyWorst = max(fitnessHistory);\n            if (iterations > 2 &&\n                Math.max(historyWorst, worstFitness) -\n                Math.min(historyBest, bestFitness) < stopTolFun) {\n                break generationLoop;\n            }\n            if (iterations > fitnessHistory.length &&\n                historyWorst - historyBest < stopTolHistFun) {\n                break generationLoop;\n            }\n            // condition number of the covariance matrix exceeds 1e14\n            if (max(diagD) / min(diagD) > 1e7) {\n                break generationLoop;\n            }\n            // user defined termination\n            if (getConvergenceChecker() != null) {\n                final PointValuePair current\n                    = new PointValuePair(bestArx.getColumn(0),\n                                         isMinimize ? bestFitness : -bestFitness);\n                if (lastResult != null &&\n                    getConvergenceChecker().converged(iterations, current, lastResult)) {\n                    break generationLoop;\n                }\n                lastResult = current;\n            }\n            // Adjust step size in case of equal function values (flat fitness)\n            if (bestValue == fitness[arindex[(int)(0.1+lambda/4.)]]) {\n                sigma = sigma * Math.exp(0.2 + cs / damps);\n            }\n            if (iterations > 2 && Math.max(historyWorst, bestFitness) -\n                Math.min(historyBest, bestFitness) == 0) {\n                sigma = sigma * Math.exp(0.2 + cs / damps);\n            }\n            // store best in history\n            push(fitnessHistory,bestFitness);\n            fitfun.setValueRange(worstFitness-bestFitness);\n            if (generateStatistics) {\n                statisticsSigmaHistory.add(sigma);\n                statisticsFitnessHistory.add(bestFitness);\n                statisticsMeanHistory.add(xmean.transpose());\n                statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n            }\n        }\n        return optimum;\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        // -------------------- Initialization --------------------------------\n        isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n        final FitnessFunction fitfun = new FitnessFunction();\n        final double[] guess = getStartPoint();\n        // number of objective variables/problem dimension\n        dimension = guess.length;\n        initializeCMA(guess);\n        iterations = 0;\n        double bestValue = fitfun.value(guess);\n        push(fitnessHistory, bestValue);\n        PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n\n        // -------------------- Generation Loop --------------------------------\n\n        generationLoop:\n        for (iterations = 1; iterations <= maxIterations; iterations++) {\n\n            // Generate and evaluate lambda offspring\n            final RealMatrix arz = randn1(dimension, lambda);\n            final RealMatrix arx = zeros(dimension, lambda);\n            final double[] fitness = new double[lambda];\n            // generate random offspring\n            for (int k = 0; k < lambda; k++) {\n                RealMatrix arxk = null;\n                for (int i = 0; i < checkFeasableCount + 1; i++) {\n                    if (diagonalOnly <= 0) {\n                        arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k))\n                                        .scalarMultiply(sigma)); // m + sig * Normal(0,C)\n                    } else {\n                        arxk = xmean.add(times(diagD, arz.getColumnMatrix(k))\n                                        .scalarMultiply(sigma));\n                    }\n                    if (i >= checkFeasableCount ||\n                        fitfun.isFeasible(arxk.getColumn(0))) {\n                        break;\n                    }\n                    // regenerate random arguments for row\n                    arz.setColumn(k, randn(dimension));\n                }\n                copyColumn(arxk, 0, arx, k);\n                try {\n                    fitness[k] = fitfun.value(arx.getColumn(k)); // compute fitness\n                } catch (TooManyEvaluationsException e) {\n                    break generationLoop;\n                }\n            }\n            // Sort by fitness and compute weighted mean into xmean\n            final int[] arindex = sortedIndices(fitness);\n            // Calculate new xmean, this is selection and recombination\n            final RealMatrix xold = xmean; // for speed up of Eq. (2) and (3)\n            final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n            xmean = bestArx.multiply(weights);\n            final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n            final RealMatrix zmean = bestArz.multiply(weights);\n            final boolean hsig = updateEvolutionPaths(zmean, xold);\n            if (diagonalOnly <= 0) {\n                updateCovariance(hsig, bestArx, arz, arindex, xold);\n            } else {\n                updateCovarianceDiagonalOnly(hsig, bestArz);\n            }\n            // Adapt step size sigma - Eq. (5)\n            sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps));\n            final double bestFitness = fitness[arindex[0]];\n            final double worstFitness = fitness[arindex[arindex.length - 1]];\n            if (bestValue > bestFitness) {\n                bestValue = bestFitness;\n                optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n                if (getConvergenceChecker() != null) {\n                    if (getConvergenceChecker().converged(iterations, optimum, new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness))) {\n                        break generationLoop;\n                    }\n                }\n            }\n            // handle termination criteria\n            // Break, if fitness is good enough\n            if (stopFitness != 0) { // only if stopFitness is defined\n                if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n                    break generationLoop;\n                }\n            }\n            final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n            final double[] pcCol = pc.getColumn(0);\n            for (int i = 0; i < dimension; i++) {\n                if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n                    break;\n                }\n                if (i >= dimension - 1) {\n                    break generationLoop;\n                }\n            }\n            for (int i = 0; i < dimension; i++) {\n                if (sigma * sqrtDiagC[i] > stopTolUpX) {\n                    break generationLoop;\n                }\n            }\n            final double historyBest = min(fitnessHistory);\n            final double historyWorst = max(fitnessHistory);\n            if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n                break generationLoop;\n            }\n            if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n                break generationLoop;\n            }\n            // condition number of the covariance matrix exceeds 1e14\n            if (max(diagD) / min(diagD) > 1e7) {\n                break generationLoop;\n            }\n            // user defined termination\n            if (getConvergenceChecker() != null) {\n                final PointValuePair current = new PointValuePair(bestArx.getColumn(0),\n                                        isMinimize ? bestFitness : -bestFitness);\n                if (getConvergenceChecker().converged(iterations, current, optimum)) {\n                    break generationLoop;\n                }\n            }\n            // Adjust step size in case of equal function values (flat fitness)\n            if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n                sigma = sigma * Math.exp(0.2 + cs / damps);\n            }\n            if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n                sigma = sigma * Math.exp(0.2 + cs / damps);\n            }\n            // store best in history\n            push(fitnessHistory, bestFitness);\n            fitfun.setValueRange(worstFitness - bestFitness);\n            if (generateStatistics) {\n                statisticsSigmaHistory.add(sigma);\n                statisticsFitnessHistory.add(bestFitness);\n                statisticsMeanHistory.add(xmean.transpose());\n                statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n            }\n        }\n        return optimum;\n    }\n"
                        ]
                    }
                }
            },
            {
                "path": "src/main/java/org/apache/commons/math3/optim/nonlinear/scalar/noderiv/PowellOptimizer.java",
                "start_loc": 172,
                "end_loc": 268,
                "buggy_function": "    @Override\n    protected PointValuePair doOptimize() {\n        checkParameters();\n\n        final GoalType goal = getGoalType();\n        final double[] guess = getStartPoint();\n        final int n = guess.length;\n\n        final double[][] direc = new double[n][n];\n        for (int i = 0; i < n; i++) {\n            direc[i][i] = 1;\n        }\n\n        final ConvergenceChecker<PointValuePair> checker\n            = getConvergenceChecker();\n\n        double[] x = guess;\n        double fVal = computeObjectiveValue(x);\n        double[] x1 = x.clone();\n        int iter = 0;\n        while (true) {\n            ++iter;\n\n            double fX = fVal;\n            double fX2 = 0;\n            double delta = 0;\n            int bigInd = 0;\n            double alphaMin = 0;\n\n            for (int i = 0; i < n; i++) {\n                final double[] d = MathArrays.copyOf(direc[i]);\n\n                fX2 = fVal;\n\n                final UnivariatePointValuePair optimum = line.search(x, d);\n                fVal = optimum.getValue();\n                alphaMin = optimum.getPoint();\n                final double[][] result = newPointAndDirection(x, d, alphaMin);\n                x = result[0];\n\n                if ((fX2 - fVal) > delta) {\n                    delta = fX2 - fVal;\n                    bigInd = i;\n                }\n            }\n\n            // Default convergence check.\n            boolean stop = 2 * (fX - fVal) <=\n                (relativeThreshold * (FastMath.abs(fX) + FastMath.abs(fVal)) +\n                 absoluteThreshold);\n\n            final PointValuePair previous = new PointValuePair(x1, fX);\n            final PointValuePair current = new PointValuePair(x, fVal);\n            if (!stop) { // User-defined stopping criteria.\n                if (checker != null) {\n                    stop = checker.converged(iter, previous, current);\n                }\n            }\n            if (stop) {\n                if (goal == GoalType.MINIMIZE) {\n                    return (fVal < fX) ? current : previous;\n                } else {\n                    return (fVal > fX) ? current : previous;\n                }\n            }\n\n            final double[] d = new double[n];\n            final double[] x2 = new double[n];\n            for (int i = 0; i < n; i++) {\n                d[i] = x[i] - x1[i];\n                x2[i] = 2 * x[i] - x1[i];\n            }\n\n            x1 = x.clone();\n            fX2 = computeObjectiveValue(x2);\n\n            if (fX > fX2) {\n                double t = 2 * (fX + fX2 - 2 * fVal);\n                double temp = fX - fVal - delta;\n                t *= temp * temp;\n                temp = fX - fX2;\n                t -= delta * temp * temp;\n\n                if (t < 0.0) {\n                    final UnivariatePointValuePair optimum = line.search(x, d);\n                    fVal = optimum.getValue();\n                    alphaMin = optimum.getPoint();\n                    final double[][] result = newPointAndDirection(x, d, alphaMin);\n                    x = result[0];\n\n                    final int lastInd = n - 1;\n                    direc[bigInd] = direc[lastInd];\n                    direc[lastInd] = result[1];\n                }\n            }\n        }\n    }\n",
                "fixed_function": "    @Override\n    protected PointValuePair doOptimize() {\n        checkParameters();\n\n        final GoalType goal = getGoalType();\n        final double[] guess = getStartPoint();\n        final int n = guess.length;\n\n        final double[][] direc = new double[n][n];\n        for (int i = 0; i < n; i++) {\n            direc[i][i] = 1;\n        }\n\n        final ConvergenceChecker<PointValuePair> checker\n            = getConvergenceChecker();\n\n        double[] x = guess;\n        double fVal = computeObjectiveValue(x);\n        double[] x1 = x.clone();\n        while (true) {\n            incrementIterationCount();\n\n            double fX = fVal;\n            double fX2 = 0;\n            double delta = 0;\n            int bigInd = 0;\n            double alphaMin = 0;\n\n            for (int i = 0; i < n; i++) {\n                final double[] d = MathArrays.copyOf(direc[i]);\n\n                fX2 = fVal;\n\n                final UnivariatePointValuePair optimum = line.search(x, d);\n                fVal = optimum.getValue();\n                alphaMin = optimum.getPoint();\n                final double[][] result = newPointAndDirection(x, d, alphaMin);\n                x = result[0];\n\n                if ((fX2 - fVal) > delta) {\n                    delta = fX2 - fVal;\n                    bigInd = i;\n                }\n            }\n\n            // Default convergence check.\n            boolean stop = 2 * (fX - fVal) <=\n                (relativeThreshold * (FastMath.abs(fX) + FastMath.abs(fVal)) +\n                 absoluteThreshold);\n\n            final PointValuePair previous = new PointValuePair(x1, fX);\n            final PointValuePair current = new PointValuePair(x, fVal);\n            if (!stop) { // User-defined stopping criteria.\n                if (checker != null) {\n                    stop = checker.converged(getIterations(), previous, current);\n                }\n            }\n            if (stop) {\n                if (goal == GoalType.MINIMIZE) {\n                    return (fVal < fX) ? current : previous;\n                } else {\n                    return (fVal > fX) ? current : previous;\n                }\n            }\n\n            final double[] d = new double[n];\n            final double[] x2 = new double[n];\n            for (int i = 0; i < n; i++) {\n                d[i] = x[i] - x1[i];\n                x2[i] = 2 * x[i] - x1[i];\n            }\n\n            x1 = x.clone();\n            fX2 = computeObjectiveValue(x2);\n\n            if (fX > fX2) {\n                double t = 2 * (fX + fX2 - 2 * fVal);\n                double temp = fX - fVal - delta;\n                t *= temp * temp;\n                temp = fX - fX2;\n                t -= delta * temp * temp;\n\n                if (t < 0.0) {\n                    final UnivariatePointValuePair optimum = line.search(x, d);\n                    fVal = optimum.getValue();\n                    alphaMin = optimum.getPoint();\n                    final double[][] result = newPointAndDirection(x, d, alphaMin);\n                    x = result[0];\n\n                    final int lastInd = n - 1;\n                    direc[bigInd] = direc[lastInd];\n                    direc[lastInd] = result[1];\n                }\n            }\n        }\n    }\n",
                "comment": "    /** {@inheritDoc} */\n",
                "normalized_body": [
                    "@Override protected PointValuePair doOptimize() { checkParameters(); final GoalType goal = getGoalType(); final double[] guess = getStartPoint(); final int n = guess.length; final double[][] direc = new double[n][n]; for (int i = 0; i < n; i++) { direc[i][i] = 1; } final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker(); double[] x = guess; double fVal = computeObjectiveValue(x); double[] x1 = x.clone(); int iter = 0; while (true) { ++iter; double fX = fVal; double fX2 = 0; double delta = 0; int bigInd = 0; double alphaMin = 0; for (int i = 0; i < n; i++) { final double[] d = MathArrays.copyOf(direc[i]); fX2 = fVal; final UnivariatePointValuePair optimum = line.search(x, d); fVal = optimum.getValue(); alphaMin = optimum.getPoint(); final double[][] result = newPointAndDirection(x, d, alphaMin); x = result[0]; if ((fX2 - fVal) > delta) { delta = fX2 - fVal; bigInd = i; } } boolean stop = 2 * (fX - fVal) <= (relativeThreshold * (FastMath.abs(fX) + FastMath.abs(fVal)) + absoluteThreshold); final PointValuePair previous = new PointValuePair(x1, fX); final PointValuePair current = new PointValuePair(x, fVal); if (!stop) { if (checker != null) { stop = checker.converged(iter, previous, current); } } if (stop) { if (goal == GoalType.MINIMIZE) { return (fVal < fX) ? current : previous; } else { return (fVal > fX) ? current : previous; } } final double[] d = new double[n]; final double[] x2 = new double[n]; for (int i = 0; i < n; i++) { d[i] = x[i] - x1[i]; x2[i] = 2 * x[i] - x1[i]; } x1 = x.clone(); fX2 = computeObjectiveValue(x2); if (fX > fX2) { double t = 2 * (fX + fX2 - 2 * fVal); double temp = fX - fVal - delta; t *= temp * temp; temp = fX - fX2; t -= delta * temp * temp; if (t < 0.0) { final UnivariatePointValuePair optimum = line.search(x, d); fVal = optimum.getValue(); alphaMin = optimum.getPoint(); final double[][] result = newPointAndDirection(x, d, alphaMin); x = result[0]; final int lastInd = n - 1; direc[bigInd] = direc[lastInd]; direc[lastInd] = result[1]; } } } }"
                ],
                "top_similar_methods": [
                    [
                        1.0000001788139343,
                        "@Override protected PointValuePair doOptimize() { checkParameters(); final GoalType goal = getGoalType(); final double[] guess = getStartPoint(); final int n = guess.length; final double[][] direc = new double[n][n]; for (int i = 0; i < n; i++) { direc[i][i] = 1; } final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker(); double[] x = guess; double fVal = computeObjectiveValue(x); double[] x1 = x.clone(); int iter = 0; while (true) { ++iter; double fX = fVal; double fX2 = 0; double delta = 0; int bigInd = 0; double alphaMin = 0; for (int i = 0; i < n; i++) { final double[] d = MathArrays.copyOf(direc[i]); fX2 = fVal; final UnivariatePointValuePair optimum = line.search(x, d); fVal = optimum.getValue(); alphaMin = optimum.getPoint(); final double[][] result = newPointAndDirection(x, d, alphaMin); x = result[0]; if ((fX2 - fVal) > delta) { delta = fX2 - fVal; bigInd = i; } } boolean stop = 2 * (fX - fVal) <= (relativeThreshold * (FastMath.abs(fX) + FastMath.abs(fVal)) + absoluteThreshold); final PointValuePair previous = new PointValuePair(x1, fX); final PointValuePair current = new PointValuePair(x, fVal); if (!stop) { if (checker != null) { stop = checker.converged(iter, previous, current); } } if (stop) { if (goal == GoalType.MINIMIZE) { return (fVal < fX) ? current : previous; } else { return (fVal > fX) ? current : previous; } } final double[] d = new double[n]; final double[] x2 = new double[n]; for (int i = 0; i < n; i++) { d[i] = x[i] - x1[i]; x2[i] = 2 * x[i] - x1[i]; } x1 = x.clone(); fX2 = computeObjectiveValue(x2); if (fX > fX2) { double t = 2 * (fX + fX2 - 2 * fVal); double temp = fX - fVal - delta; t *= temp * temp; temp = fX - fX2; t -= delta * temp * temp; if (t < 0.0) { final UnivariatePointValuePair optimum = line.search(x, d); fVal = optimum.getValue(); alphaMin = optimum.getPoint(); final double[][] result = newPointAndDirection(x, d, alphaMin); x = result[0]; final int lastInd = n - 1; direc[bigInd] = direc[lastInd]; direc[lastInd] = result[1]; } } } }"
                    ],
                    [
                        0.9989590048789978,
                        "@Override protected PointValuePair doOptimize() { final GoalType goal = getGoalType(); final double[] guess = getStartPoint(); final int n = guess.length; final double[][] direc = new double[n][n]; for (int i = 0; i < n; i++) { direc[i][i] = 1; } final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker(); double[] x = guess; double fVal = computeObjectiveValue(x); double[] x1 = x.clone(); int iter = 0; while (true) { ++iter; double fX = fVal; double fX2 = 0; double delta = 0; int bigInd = 0; double alphaMin = 0; for (int i = 0; i < n; i++) { final double[] d = MathArrays.copyOf(direc[i]); fX2 = fVal; final UnivariatePointValuePair optimum = line.search(x, d); fVal = optimum.getValue(); alphaMin = optimum.getPoint(); final double[][] result = newPointAndDirection(x, d, alphaMin); x = result[0]; if ((fX2 - fVal) > delta) { delta = fX2 - fVal; bigInd = i; } } boolean stop = 2 * (fX - fVal) <= (relativeThreshold * (FastMath.abs(fX) + FastMath.abs(fVal)) + absoluteThreshold); final PointValuePair previous = new PointValuePair(x1, fX); final PointValuePair current = new PointValuePair(x, fVal); if (!stop) { if (checker != null) { stop = checker.converged(iter, previous, current); } } if (stop) { if (goal == GoalType.MINIMIZE) { return (fVal < fX) ? current : previous; } else { return (fVal > fX) ? current : previous; } } final double[] d = new double[n]; final double[] x2 = new double[n]; for (int i = 0; i < n; i++) { d[i] = x[i] - x1[i]; x2[i] = 2 * x[i] - x1[i]; } x1 = x.clone(); fX2 = computeObjectiveValue(x2); if (fX > fX2) { double t = 2 * (fX + fX2 - 2 * fVal); double temp = fX - fVal - delta; t *= temp * temp; temp = fX - fX2; t -= delta * temp * temp; if (t < 0.0) { final UnivariatePointValuePair optimum = line.search(x, d); fVal = optimum.getValue(); alphaMin = optimum.getPoint(); final double[][] result = newPointAndDirection(x, d, alphaMin); x = result[0]; final int lastInd = n - 1; direc[bigInd] = direc[lastInd]; direc[lastInd] = result[1]; } } } }"
                    ],
                    [
                        0.9859073758125305,
                        "@Override protected double doIntegrate() throws TooManyEvaluationsException, MaxCountExceededException { final int m = iterations.getMaximalCount() + 1; double[] previousRow = new double[m]; double[] currentRow = new double[m]; TrapezoidIntegrator qtrap = new TrapezoidIntegrator(); currentRow[0] = qtrap.stage(this, 0); iterations.incrementCount(); double olds = currentRow[0]; while (true) { final int i = iterations.getCount(); final double[] tmpRow = previousRow; previousRow = currentRow; currentRow = tmpRow; currentRow[0] = qtrap.stage(this, i); iterations.incrementCount(); for (int j = 1; j <= i; j++) { final double r = (1L << (2 * j)) - 1; final double tIJm1 = currentRow[j - 1]; currentRow[j] = tIJm1 + (tIJm1 - previousRow[j - 1]) / r; } final double s = currentRow[i]; if (i >= getMinimalIterationCount()) { final double delta = FastMath.abs(s - olds); final double rLimit = getRelativeAccuracy() * (FastMath.abs(olds) + FastMath.abs(s)) * 0.5; if ((delta <= rLimit) || (delta <= getAbsoluteAccuracy())) { return s; } } olds = s; } }"
                    ],
                    [
                        0.9855962693691254,
                        "@Test public void testParaboloid2D() { MultivariateFunction f = new MultivariateFunction() { public double value(double[] x) { if (x.length != 2) { throw new IllegalArgumentException(); } return 2 * x[0] * x[0] - 3 * x[1] * x[1] + 4 * x[0] * x[1] - 5; } }; MultivariateInterpolator interpolator = new MicrosphereInterpolator(); final int n = 121; final int dim = 2; double[][] x = new double[n][dim]; double[] y = new double[n]; int index = 0; for (int i = -10; i <= 10; i += 2) { for (int j = -10; j <= 10; j += 2) { x[index][0] = i; x[index][1] = j; y[index] = f.value(x[index]); ++index; } } MultivariateFunction p = interpolator.interpolate(x, y); double[] c = new double[dim]; double expected, result; c[0] = 0; c[1] = 0; expected = f.value(c); result = p.value(c); Assert.assertEquals(\\\"On sample point\\\", expected, result, FastMath.ulp(1d)); c[0] = 2 + 1e-5; c[1] = 2 - 1e-5; expected = f.value(c); result = p.value(c); Assert.assertEquals(\\\"1e-5 away from sample point\\\", expected, result, 1e-3); }"
                    ],
                    [
                        0.9849744141101837,
                        "@Test public void testParaboloid() { double[] xval = new double[] { 3, 4, 5, 6.5 }; double[] yval = new double[] { -4, -3, -1, 2, 2.5 }; BivariateFunction f = new BivariateFunction() { public double value(double x, double y) { return 2 * x * x - 3 * y * y + 4 * x * y - 5; } }; double[][] zval = new double[xval.length][yval.length]; for (int i = 0; i < xval.length; i++) { for (int j = 0; j < yval.length; j++) { zval[i][j] = f.value(xval[i], yval[j]); } } double[][] dZdX = new double[xval.length][yval.length]; BivariateFunction dfdX = new BivariateFunction() { public double value(double x, double y) { return 4 * (x + y); } }; for (int i = 0; i < xval.length; i++) { for (int j = 0; j < yval.length; j++) { dZdX[i][j] = dfdX.value(xval[i], yval[j]); } } double[][] dZdY = new double[xval.length][yval.length]; BivariateFunction dfdY = new BivariateFunction() { public double value(double x, double y) { return 4 * x - 6 * y; } }; for (int i = 0; i < xval.length; i++) { for (int j = 0; j < yval.length; j++) { dZdY[i][j] = dfdY.value(xval[i], yval[j]); } } double[][] dZdXdY = new double[xval.length][yval.length]; for (int i = 0; i < xval.length; i++) { for (int j = 0; j < yval.length; j++) { dZdXdY[i][j] = 4; } } BivariateFunction bcf = new BicubicSplineInterpolatingFunction(xval, yval, zval, dZdX, dZdY, dZdXdY); double x, y; double expected, result; x = 4; y = -3; expected = f.value(x, y); result = bcf.value(x, y); Assert.assertEquals(\\\"On sample point\\\", expected, result, 1e-15); x = 4.5; y = -1.5; expected = f.value(x, y); result = bcf.value(x, y); Assert.assertEquals(\\\"Half-way between sample points (middle of the patch)\\\", expected, result, 2); x = 3.5; y = -3.5; expected = f.value(x, y); result = bcf.value(x, y); Assert.assertEquals(\\\"Half-way between sample points (border of the patch)\\\", expected, result, 2); }"
                    ],
                    [
                        0.9846338331699371,
                        "public RealMatrix power(final int p) throws NotPositiveException, NonSquareMatrixException { if (p < 0) { throw new NotPositiveException(LocalizedFormats.NOT_POSITIVE_EXPONENT, p); } if (!isSquare()) { throw new NonSquareMatrixException(getRowDimension(), getColumnDimension()); } if (p == 0) { return MatrixUtils.createRealIdentityMatrix(this.getRowDimension()); } if (p == 1) { return this.copy(); } final int power = p - 1; final char[] binaryRepresentation = Integer.toBinaryString(power).toCharArray(); final ArrayList<Integer> nonZeroPositions = new ArrayList<Integer>(); int maxI = -1; for (int i = 0; i < binaryRepresentation.length; ++i) { if (binaryRepresentation[i] == '1') { final int pos = binaryRepresentation.length - i - 1; nonZeroPositions.add(pos); if (maxI == -1) { maxI = pos; } } } RealMatrix[] results = new RealMatrix[maxI + 1]; results[0] = this.copy(); for (int i = 1; i <= maxI; ++i) { results[i] = results[i - 1].multiply(results[i - 1]); } RealMatrix result = this.copy(); for (Integer i : nonZeroPositions) { result = result.multiply(results[i]); } return result; }"
                    ],
                    [
                        0.9843352437019348,
                        "@Override protected PointValuePair doOptimize() { final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker(); final double[] point = getStartPoint(); final GoalType goal = getGoalType(); final int n = point.length; double[] r = computeObjectiveGradient(point); if (goal == GoalType.MINIMIZE) { for (int i = 0; i < n; i++) { r[i] = -r[i]; } } double[] steepestDescent = preconditioner.precondition(point, r); double[] searchDirection = steepestDescent.clone(); double delta = 0; for (int i = 0; i < n; ++i) { delta += r[i] * searchDirection[i]; } PointValuePair current = null; int iter = 0; int maxEval = getMaxEvaluations(); while (true) { ++iter; final double objective = computeObjectiveValue(point); PointValuePair previous = current; current = new PointValuePair(point, objective); if (previous != null) { if (checker.converged(iter, previous, current)) { return current; } } final UnivariateFunction lsf = new LineSearchFunction(point, searchDirection); final double uB = findUpperBound(lsf, 0, initialStep); final double step = solver.solve(maxEval, lsf, 0, uB, 1e-15); maxEval -= solver.getEvaluations(); for (int i = 0; i < point.length; ++i) { point[i] += step * searchDirection[i]; } r = computeObjectiveGradient(point); if (goal == GoalType.MINIMIZE) { for (int i = 0; i < n; ++i) { r[i] = -r[i]; } } final double deltaOld = delta; final double[] newSteepestDescent = preconditioner.precondition(point, r); delta = 0; for (int i = 0; i < n; ++i) { delta += r[i] * newSteepestDescent[i]; } final double beta; switch(updateFormula) { case FLETCHER_REEVES: beta = delta / deltaOld; break; case POLAK_RIBIERE: double deltaMid = 0; for (int i = 0; i < r.length; ++i) { deltaMid += r[i] * steepestDescent[i]; } beta = (delta - deltaMid) / deltaOld; break; default: throw new MathInternalError(); } steepestDescent = newSteepestDescent; if (iter % n == 0 || beta < 0) { searchDirection = steepestDescent.clone(); } else { for (int i = 0; i < n; ++i) { searchDirection[i] = steepestDescent[i] + beta * searchDirection[i]; } } } }"
                    ],
                    [
                        0.9840424656867981,
                        "public RealMatrix solve(RealMatrix b) { final int n = qrt.length; final int m = qrt[0].length; if (b.getRowDimension() != m) { throw new DimensionMismatchException(b.getRowDimension(), m); } if (!isNonSingular()) { throw new SingularMatrixException(); } final int columns = b.getColumnDimension(); final int blockSize = BlockRealMatrix.BLOCK_SIZE; final int cBlocks = (columns + blockSize - 1) / blockSize; final double[][] xBlocks = BlockRealMatrix.createBlocksLayout(n, columns); final double[][] y = new double[b.getRowDimension()][blockSize]; final double[] alpha = new double[blockSize]; for (int kBlock = 0; kBlock < cBlocks; ++kBlock) { final int kStart = kBlock * blockSize; final int kEnd = FastMath.min(kStart + blockSize, columns); final int kWidth = kEnd - kStart; b.copySubMatrix(0, m - 1, kStart, kEnd - 1, y); for (int minor = 0; minor < FastMath.min(m, n); minor++) { final double[] qrtMinor = qrt[minor]; final double factor = 1.0 / (rDiag[minor] * qrtMinor[minor]); Arrays.fill(alpha, 0, kWidth, 0.0); for (int row = minor; row < m; ++row) { final double d = qrtMinor[row]; final double[] yRow = y[row]; for (int k = 0; k < kWidth; ++k) { alpha[k] += d * yRow[k]; } } for (int k = 0; k < kWidth; ++k) { alpha[k] *= factor; } for (int row = minor; row < m; ++row) { final double d = qrtMinor[row]; final double[] yRow = y[row]; for (int k = 0; k < kWidth; ++k) { yRow[k] += alpha[k] * d; } } } for (int j = rDiag.length - 1; j >= 0; --j) { final int jBlock = j / blockSize; final int jStart = jBlock * blockSize; final double factor = 1.0 / rDiag[j]; final double[] yJ = y[j]; final double[] xBlock = xBlocks[jBlock * cBlocks + kBlock]; int index = (j - jStart) * kWidth; for (int k = 0; k < kWidth; ++k) { yJ[k] *= factor; xBlock[index++] = yJ[k]; } final double[] qrtJ = qrt[j]; for (int i = 0; i < j; ++i) { final double rIJ = qrtJ[i]; final double[] yI = y[i]; for (int k = 0; k < kWidth; ++k) { yI[k] -= yJ[k] * rIJ; } } } } return new BlockRealMatrix(n, columns, xBlocks, false); }"
                    ],
                    [
                        0.9838719964027405,
                        "@Override public void integrate(final ExpandableStatefulODE equations, final double t) throws NumberIsTooSmallException, DimensionMismatchException, MaxCountExceededException, NoBracketingException { sanityChecks(equations, t); setEquations(equations); final boolean forward = t > equations.getTime(); final double[] y0 = equations.getCompleteState(); final double[] y = y0.clone(); final int stages = c.length + 1; final double[][] yDotK = new double[stages][y.length]; final double[] yTmp = y0.clone(); final double[] yDotTmp = new double[y.length]; final RungeKuttaStepInterpolator interpolator = (RungeKuttaStepInterpolator) prototype.copy(); interpolator.reinitialize(this, yTmp, yDotK, forward, equations.getPrimaryMapper(), equations.getSecondaryMappers()); interpolator.storeTime(equations.getTime()); stepStart = equations.getTime(); double hNew = 0; boolean firstTime = true; initIntegration(equations.getTime(), y0, t); isLastStep = false; do { interpolator.shift(); double error = 10; while (error >= 1.0) { if (firstTime || !fsal) { computeDerivatives(stepStart, y, yDotK[0]); } if (firstTime) { final double[] scale = new double[mainSetDimension]; if (vecAbsoluteTolerance == null) { for (int i = 0; i < scale.length; ++i) { scale[i] = scalAbsoluteTolerance + scalRelativeTolerance * FastMath.abs(y[i]); } } else { for (int i = 0; i < scale.length; ++i) { scale[i] = vecAbsoluteTolerance[i] + vecRelativeTolerance[i] * FastMath.abs(y[i]); } } hNew = initializeStep(forward, getOrder(), scale, stepStart, y, yDotK[0], yTmp, yDotK[1]); firstTime = false; } stepSize = hNew; if (forward) { if (stepStart + stepSize >= t) { stepSize = t - stepStart; } } else { if (stepStart + stepSize <= t) { stepSize = t - stepStart; } } for (int k = 1; k < stages; ++k) { for (int j = 0; j < y0.length; ++j) { double sum = a[k - 1][0] * yDotK[0][j]; for (int l = 1; l < k; ++l) { sum += a[k - 1][l] * yDotK[l][j]; } yTmp[j] = y[j] + stepSize * sum; } computeDerivatives(stepStart + c[k - 1] * stepSize, yTmp, yDotK[k]); } for (int j = 0; j < y0.length; ++j) { double sum = b[0] * yDotK[0][j]; for (int l = 1; l < stages; ++l) { sum += b[l] * yDotK[l][j]; } yTmp[j] = y[j] + stepSize * sum; } error = estimateError(yDotK, y, yTmp, stepSize); if (error >= 1.0) { final double factor = FastMath.min(maxGrowth, FastMath.max(minReduction, safety * FastMath.pow(error, exp))); hNew = filterStep(stepSize * factor, forward, false); } } interpolator.storeTime(stepStart + stepSize); System.arraycopy(yTmp, 0, y, 0, y0.length); System.arraycopy(yDotK[stages - 1], 0, yDotTmp, 0, y0.length); stepStart = acceptStep(interpolator, y, yDotTmp, t); System.arraycopy(y, 0, yTmp, 0, y.length); if (!isLastStep) { interpolator.storeTime(stepStart); if (fsal) { System.arraycopy(yDotTmp, 0, yDotK[0], 0, y0.length); } final double factor = FastMath.min(maxGrowth, FastMath.max(minReduction, safety * FastMath.pow(error, exp))); final double scaledH = stepSize * factor; final double nextT = stepStart + scaledH; final boolean nextIsLast = forward ? (nextT >= t) : (nextT <= t); hNew = filterStep(scaledH, forward, nextIsLast); final double filteredNextT = stepStart + hNew; final boolean filteredNextIsLast = forward ? (filteredNextT >= t) : (filteredNextT <= t); if (filteredNextIsLast) { hNew = t - stepStart; } } } while (!isLastStep); equations.setTime(stepStart); equations.setCompleteState(y); resetInternalState(); }"
                    ],
                    [
                        0.983646959066391,
                        "private long nextPoisson(double meanPoisson) { final double pivot = 40.0d; if (meanPoisson < pivot) { double p = FastMath.exp(-meanPoisson); long n = 0; double r = 1.0d; double rnd = 1.0d; while (n < 1000 * meanPoisson) { rnd = random.nextDouble(); r = r * rnd; if (r >= p) { n++; } else { return n; } } return n; } else { final double lambda = FastMath.floor(meanPoisson); final double lambdaFractional = meanPoisson - lambda; final double logLambda = FastMath.log(lambda); final double logLambdaFactorial = ArithmeticUtils.factorialLog((int) lambda); final long y2 = lambdaFractional < Double.MIN_VALUE ? 0 : nextPoisson(lambdaFractional); final double delta = FastMath.sqrt(lambda * FastMath.log(32 * lambda / FastMath.PI + 1)); final double halfDelta = delta / 2; final double twolpd = 2 * lambda + delta; final double a1 = FastMath.sqrt(FastMath.PI * twolpd) * FastMath.exp(1 / 8 * lambda); final double a2 = (twolpd / delta) * FastMath.exp(-delta * (1 + delta) / twolpd); final double aSum = a1 + a2 + 1; final double p1 = a1 / aSum; final double p2 = a2 / aSum; final double c1 = 1 / (8 * lambda); double x = 0; double y = 0; double v = 0; int a = 0; double t = 0; double qr = 0; double qa = 0; for (; ; ) { final double u = random.nextDouble(); if (u <= p1) { final double n = random.nextGaussian(); x = n * FastMath.sqrt(lambda + halfDelta) - 0.5d; if (x > delta || x < -lambda) { continue; } y = x < 0 ? FastMath.floor(x) : FastMath.ceil(x); final double e = exponential.sample(); v = -e - (n * n / 2) + c1; } else { if (u > p1 + p2) { y = lambda; break; } else { x = delta + (twolpd / delta) * exponential.sample(); y = FastMath.ceil(x); v = -exponential.sample() - delta * (x + 1) / twolpd; } } a = x < 0 ? 1 : 0; t = y * (y + 1) / (2 * lambda); if (v < -t && a == 0) { y = lambda + y; break; } qr = t * ((2 * y + 1) / (6 * lambda) - 1); qa = qr - (t * t) / (3 * (lambda + a * (y + 1))); if (v < qa) { y = lambda + y; break; } if (v > qr) { continue; } if (v < y * logLambda - ArithmeticUtils.factorialLog((int) (y + lambda)) + logLambdaFactorial) { y = lambda + y; break; } } return y2 + (long) y; } }"
                    ],
                    [
                        0.9831061065196991,
                        "public void saundersTest(final int n, final boolean goodb, final boolean precon, final double shift, final double pertbn) { final RealLinearOperator a = new RealLinearOperator() { @Override public RealVector operate(final RealVector x) { if (x.getDimension() != n) { throw new DimensionMismatchException(x.getDimension(), n); } final double[] y = new double[n]; for (int i = 0; i < n; i++) { y[i] = (i + 1) * 1.1 / n * x.getEntry(i); } return new ArrayRealVector(y, false); } @Override public int getRowDimension() { return n; } @Override public int getColumnDimension() { return n; } }; final double shiftm = shift; final double pertm = FastMath.abs(pertbn); final RealLinearOperator minv; if (precon) { minv = new RealLinearOperator() { @Override public int getRowDimension() { return n; } @Override public int getColumnDimension() { return n; } @Override public RealVector operate(final RealVector x) { if (x.getDimension() != n) { throw new DimensionMismatchException(x.getDimension(), n); } final double[] y = new double[n]; for (int i = 0; i < n; i++) { double d = (i + 1) * 1.1 / n; d = FastMath.abs(d - shiftm); if (i % 10 == 0) { d += pertm; } y[i] = x.getEntry(i) / d; } return new ArrayRealVector(y, false); } }; } else { minv = null; } final RealVector xtrue = new ArrayRealVector(n); for (int i = 0; i < n; i++) { xtrue.setEntry(i, n - i); } final RealVector b = a.operate(xtrue); b.combineToSelf(1.0, -shift, xtrue); final SymmLQ solver = new SymmLQ(2 * n, 1E-12, true); final RealVector x = solver.solve(a, minv, b, goodb, shift); final RealVector y = a.operate(x); final RealVector r1 = new ArrayRealVector(n); for (int i = 0; i < n; i++) { final double bi = b.getEntry(i); final double yi = y.getEntry(i); final double xi = x.getEntry(i); r1.setEntry(i, bi - yi + shift * xi); } final double enorm = x.subtract(xtrue).getNorm() / xtrue.getNorm(); final double etol = 1E-5; Assert.assertTrue(\\\"enorm=\\\" + enorm + \\\", \\\" + solver.getIterationManager().getIterations(), enorm <= etol); }"
                    ],
                    [
                        0.9830521643161774,
                        "public double evaluate(final double[] values, final int begin, final int length, final double p) throws MathIllegalArgumentException { test(values, begin, length); if ((p > 100) || (p <= 0)) { throw new OutOfRangeException(LocalizedFormats.OUT_OF_BOUNDS_QUANTILE_VALUE, p, 0, 100); } if (length == 0) { return Double.NaN; } if (length == 1) { return values[begin]; } double n = length; double pos = p * (n + 1) / 100; double fpos = FastMath.floor(pos); int intPos = (int) fpos; double dif = pos - fpos; double[] work; int[] pivotsHeap; if (values == getDataRef()) { work = getDataRef(); pivotsHeap = cachedPivots; } else { work = new double[length]; System.arraycopy(values, begin, work, 0, length); pivotsHeap = new int[(0x1 << MAX_CACHED_LEVELS) - 1]; Arrays.fill(pivotsHeap, -1); } if (pos < 1) { return select(work, pivotsHeap, 0); } if (pos >= n) { return select(work, pivotsHeap, length - 1); } double lower = select(work, pivotsHeap, intPos - 1); double upper = select(work, pivotsHeap, intPos); return lower + dif * (upper - lower); }"
                    ]
                ],
                "directly_related_tests": [
                    "org.apache.commons.math3.genetics.FixedElapsedTimeTest::testIsSatisfied"
                ],
                "generation": {
                    "1": {
                        "prompt": "You are an expert Java developer. Below is a buggy method from a large Java project. This method causes one or more test failures.\nYour task is to fix the bug in this method. Use the provided test failure(s) to guide your reasoning.\n\n---\n\n## Buggy Method\n\n```java\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        checkParameters();\n\n        final GoalType goal = getGoalType();\n        final double[] guess = getStartPoint();\n        final int n = guess.length;\n\n        final double[][] direc = new double[n][n];\n        for (int i = 0; i < n; i++) {\n            direc[i][i] = 1;\n        }\n\n        final ConvergenceChecker<PointValuePair> checker\n            = getConvergenceChecker();\n\n        double[] x = guess;\n        double fVal = computeObjectiveValue(x);\n        double[] x1 = x.clone();\n        int iter = 0;\n        while (true) {\n            ++iter;\n\n            double fX = fVal;\n            double fX2 = 0;\n            double delta = 0;\n            int bigInd = 0;\n            double alphaMin = 0;\n\n            for (int i = 0; i < n; i++) {\n                final double[] d = MathArrays.copyOf(direc[i]);\n\n                fX2 = fVal;\n\n                final UnivariatePointValuePair optimum = line.search(x, d);\n                fVal = optimum.getValue();\n                alphaMin = optimum.getPoint();\n                final double[][] result = newPointAndDirection(x, d, alphaMin);\n                x = result[0];\n\n                if ((fX2 - fVal) > delta) {\n                    delta = fX2 - fVal;\n                    bigInd = i;\n                }\n            }\n\n            // Default convergence check.\n            boolean stop = 2 * (fX - fVal) <=\n                (relativeThreshold * (FastMath.abs(fX) + FastMath.abs(fVal)) +\n                 absoluteThreshold);\n\n            final PointValuePair previous = new PointValuePair(x1, fX);\n            final PointValuePair current = new PointValuePair(x, fVal);\n            if (!stop) { // User-defined stopping criteria.\n                if (checker != null) {\n                    stop = checker.converged(iter, previous, current);\n                }\n            }\n            if (stop) {\n                if (goal == GoalType.MINIMIZE) {\n                    return (fVal < fX) ? current : previous;\n                } else {\n                    return (fVal > fX) ? current : previous;\n                }\n            }\n\n            final double[] d = new double[n];\n            final double[] x2 = new double[n];\n            for (int i = 0; i < n; i++) {\n                d[i] = x[i] - x1[i];\n                x2[i] = 2 * x[i] - x1[i];\n            }\n\n            x1 = x.clone();\n            fX2 = computeObjectiveValue(x2);\n\n            if (fX > fX2) {\n                double t = 2 * (fX + fX2 - 2 * fVal);\n                double temp = fX - fVal - delta;\n                t *= temp * temp;\n                temp = fX - fX2;\n                t -= delta * temp * temp;\n\n                if (t < 0.0) {\n                    final UnivariatePointValuePair optimum = line.search(x, d);\n                    fVal = optimum.getValue();\n                    alphaMin = optimum.getPoint();\n                    final double[][] result = newPointAndDirection(x, d, alphaMin);\n                    x = result[0];\n\n                    final int lastInd = n - 1;\n                    direc[bigInd] = direc[lastInd];\n                    direc[lastInd] = result[1];\n                }\n            }\n        }\n    }\n\n```\n---\n\n## Failing Test Case(s)\n\n\n---\n\n## Your Goal\n\nFix the buggy method. Return only the fixed Java method. Do not include explanations or other text.\n",
                        "patches": [
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        checkParameters();\n\n        final GoalType goal = getGoalType();\n        final double[] guess = getStartPoint();\n        final int n = guess.length;\n\n        final double[][] direc = new double[n][n];\n        for (int i = 0; i < n; i++) {\n            direc[i][i] = 1;\n        }\n\n        final ConvergenceChecker<PointValuePair> checker\n            = getConvergenceChecker();\n\n        double[] x = guess;\n        double fVal = computeObjectiveValue(x);\n        double[] x1 = x.clone();\n        int iter = 0;\n        while (true) {\n            ++iter;\n\n            double fX = fVal;\n            double fX2 = 0;\n            double delta = 0;\n            int bigInd = 0;\n            double alphaMin = 0;\n\n            for (int i = 0; i < n; i++) {\n                final double[] d = MathArrays.copyOf(direc[i]);\n\n                fX2 = fVal;\n\n                final UnivariatePointValuePair optimum = line.search(x, d);\n                fVal = optimum.getValue();\n                alphaMin = optimum.getPoint();\n                x = newPointAndDirection(x, d, alphaMin)[0];\n\n                if ((fX2 - fVal) > delta) {\n                    delta = fX2 - fVal;\n                    bigInd = i;\n                }\n            }\n\n            // Default convergence check.\n            boolean stop = 2 * (fX - fVal) <=\n                    (relativeThreshold * (FastMath.abs(fX) + FastMath.abs(fVal)) +\n                            absoluteThreshold);\n\n            final PointValuePair previous = new PointValuePair(x1, fX);\n            final PointValuePair current = new PointValuePair(x, fVal);\n            if (!stop) { // User-defined stopping criteria.\n                if (checker != null) {\n                    stop = checker.converged(iter, previous, current);\n                }\n            }\n            if (stop) {\n                if (goal == GoalType.MINIMIZE) {\n                    return (fVal < fX) ? current : previous;\n                } else {\n                    return (fVal > fX) ? current : previous;\n                }\n            }\n\n            final double[] d = new double[n];\n            final double[] x2 = new double[n];\n            for (int i = 0; i < n; i++) {\n                d[i] = x[i] - x1[i];\n                x2[i] = 2 * x[i] - x1[i];\n            }\n\n            x1 = x.clone();\n            fX2 = computeObjectiveValue(x2);\n\n            if (fX > fX2) {\n                double t = 2 * (fX + fX2 - 2 * fVal);\n                double temp = fX - fVal - delta;\n                t *= temp * temp;\n                temp = fX - fX2;\n                t -= delta * temp * temp;\n\n                if (t < 0.0) {\n                    final UnivariatePointValuePair optimum = line.search(x, d);\n                    fVal = optimum.getValue();\n                    alphaMin = optimum.getPoint();\n                    final double[][] result = newPointAndDirection(x, d, alphaMin);\n                    x = result[0];\n\n                    final int lastInd = n - 1;\n                    direc[bigInd] = direc[lastInd];\n                    direc[lastInd] = result[1];\n                }\n            }\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        checkParameters();\n\n        final GoalType goal = getGoalType();\n        final double[] guess = getStartPoint();\n        final int n = guess.length;\n\n        final double[][] direc = new double[n][n];\n        for (int i = 0; i < n; i++) {\n            direc[i][i] = 1;\n        }\n\n        final ConvergenceChecker<PointValuePair> checker\n            = getConvergenceChecker();\n\n        double[] x = guess;\n        double fVal = computeObjectiveValue(x);\n        double[] x1 = x.clone();\n        int iter = 0;\n        while (true) {\n            ++iter;\n\n            double fX = fVal;\n            double fX2 = 0;\n            double delta = 0;\n            int bigInd = 0;\n            double alphaMin = 0;\n\n            for (int i = 0; i < n; i++) {\n                final double[] d = MathArrays.copyOf(direc[i]);\n\n                fX2 = fVal;\n\n                final UnivariatePointValuePair optimum = line.search(x, d);\n                fVal = optimum.getValue();\n                alphaMin = optimum.getPoint();\n                x = newPointAndDirection(x, d, alphaMin)[0];\n\n                if ((fX2 - fVal) > delta) {\n                    delta = fX2 - fVal;\n                    bigInd = i;\n                }\n            }\n\n            // Default convergence check.\n            boolean stop = 2 * (fX - fVal) <=\n                (relativeThreshold * (FastMath.abs(fX) + FastMath.abs(fVal)) +\n                 absoluteThreshold);\n\n            final PointValuePair previous = new PointValuePair(x1, fX);\n            final PointValuePair current = new PointValuePair(x, fVal);\n            if (!stop) { // User-defined stopping criteria.\n                if (checker != null) {\n                    stop = checker.converged(iter, previous, current);\n                }\n            }\n            if (stop) {\n                if (goal == GoalType.MINIMIZE) {\n                    return (fVal < fX) ? current : previous;\n                } else {\n                    return (fVal > fX) ? current : previous;\n                }\n            }\n\n            final double[] d = new double[n];\n            final double[] x2 = new double[n];\n            for (int i = 0; i < n; i++) {\n                d[i] = x[i] - x1[i];\n                x2[i] = 2 * x[i] - x1[i];\n            }\n\n            x1 = x.clone();\n            fX2 = computeObjectiveValue(x2);\n\n            if (fX > fX2) {\n                double t = 2 * (fX + fX2 - 2 * fVal);\n                double temp = fX - fVal - delta;\n                t *= temp * temp;\n                temp = fX - fX2;\n                t -= delta * temp * temp;\n\n                if (t < 0.0) {\n                    final UnivariatePointValuePair optimum = line.search(x, d);\n                    fVal = optimum.getValue();\n                    alphaMin = optimum.getPoint();\n                    final double[][] result = newPointAndDirection(x, d, alphaMin);\n                    x = result[0];\n\n                    final int lastInd = n - 1;\n                    direc[bigInd] = direc[lastInd];\n                    direc[lastInd] = result[1];\n                }\n            }\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        checkParameters();\n\n        final GoalType goal = getGoalType();\n        final double[] guess = getStartPoint();\n        final int n = guess.length;\n\n        final double[][] direc = new double[n][n];\n        for (int i = 0; i < n; i++) {\n            direc[i][i] = 1;\n        }\n\n        final ConvergenceChecker<PointValuePair> checker\n            = getConvergenceChecker();\n\n        double[] x = guess;\n        double fVal = computeObjectiveValue(x);\n        double[] x1 = x.clone();\n        int iter = 0;\n        while (true) {\n            ++iter;\n\n            double fX = fVal;\n            double fX2 = 0;\n            double delta = 0;\n            int bigInd = 0;\n            double alphaMin = 0;\n\n            for (int i = 0; i < n; i++) {\n                final double[] d = MathArrays.copyOf(direc[i]);\n\n                fX2 = fVal;\n\n                final UnivariatePointValuePair optimum = line.search(x, d);\n                fVal = optimum.getValue();\n                alphaMin = optimum.getPoint();\n                final double[][] result = newPointAndDirection(x, d, alphaMin);\n                x = result[0];\n                direc[i] = result[1];\n\n                if ((fX2 - fVal) > delta) {\n                    delta = fX2 - fVal;\n                    bigInd = i;\n                }\n            }\n\n            // Default convergence check.\n            boolean stop = 2 * (fX - fVal) <=\n                (relativeThreshold * (FastMath.abs(fX) + FastMath.abs(fVal)) +\n                absoluteThreshold);\n\n            final PointValuePair previous = new PointValuePair(x1, fX);\n            final PointValuePair current = new PointValuePair(x, fVal);\n            if (!stop) { // User-defined stopping criteria.\n                if (checker != null) {\n                    stop = checker.converged(iter, previous, current);\n                }\n            }\n            if (stop) {\n                if (goal == GoalType.MINIMIZE) {\n                    return (fVal < fX) ? current : previous;\n                } else {\n                    return (fVal > fX) ? current : previous;\n                }\n            }\n\n            final double[] d = new double[n];\n            final double[] x2 = new double[n];\n            for (int i = 0; i < n; i++) {\n                d[i] = x[i] - x1[i];\n                x2[i] = 2 * x[i] - x1[i];\n            }\n\n            x1 = x.clone();\n            fX2 = computeObjectiveValue(x2);\n\n            if (fX > fX2) {\n                double t = 2 * (fX + fX2 - 2 * fVal);\n                double temp = fX - fVal - delta;\n                t *= temp * temp;\n                temp = fX - fX2;\n                t -= delta * temp * temp;\n\n                if (t < 0.0) {\n                    final UnivariatePointValuePair optimum = line.search(x, d);\n                    fVal = optimum.getValue();\n                    alphaMin = optimum.getPoint();\n                    final double[][] result = newPointAndDirection(x, d, alphaMin);\n                    x = result[0];\n\n                    final int lastInd = n - 1;\n                    direc[bigInd] = direc[lastInd];\n                    direc[lastInd] = result[1];\n                }\n            }\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        checkParameters();\n\n        final GoalType goal = getGoalType();\n        final double[] guess = getStartPoint();\n        final int n = guess.length;\n\n        final double[][] direc = new double[n][n];\n        for (int i = 0; i < n; i++) {\n            direc[i][i] = 1;\n        }\n\n        final ConvergenceChecker<PointValuePair> checker\n            = getConvergenceChecker();\n\n        double[] x = guess.clone();\n        double fVal = computeObjectiveValue(x);\n        double[] x1 = x.clone();\n        int iter = 0;\n        while (true) {\n            ++iter;\n\n            double fX = fVal;\n            double fX2 = 0;\n            double delta = 0;\n            int bigInd = 0;\n            double alphaMin = 0;\n\n            for (int i = 0; i < n; i++) {\n                final double[] d = MathArrays.copyOf(direc[i]);\n\n                fX2 = fVal;\n\n                final UnivariatePointValuePair optimum = line.search(x, d);\n                fVal = optimum.getValue();\n                alphaMin = optimum.getPoint();\n                final double[][] result = newPointAndDirection(x, d, alphaMin);\n                x = result[0];\n\n                if ((fX2 - fVal) > delta) {\n                    delta = fX2 - fVal;\n                    bigInd = i;\n                }\n            }\n\n            // Default convergence check.\n            boolean stop = 2 * (fX - fVal) <=\n                (relativeThreshold * (FastMath.abs(fX) + FastMath.abs(fVal)) +\n                 absoluteThreshold);\n\n            final PointValuePair previous = new PointValuePair(x1, fX);\n            final PointValuePair current = new PointValuePair(x, fVal);\n            if (!stop) { // User-defined stopping criteria.\n                if (checker != null) {\n                    stop = checker.converged(iter, previous, current);\n                }\n            }\n            if (stop) {\n                if (goal == GoalType.MINIMIZE) {\n                    return (fVal < fX) ? current : previous;\n                } else {\n                    return (fVal > fX) ? current : previous;\n                }\n            }\n\n            final double[] d = new double[n];\n            final double[] x2 = new double[n];\n            for (int i = 0; i < n; i++) {\n                d[i] = x[i] - x1[i];\n                x2[i] = 2 * x[i] - x1[i];\n            }\n\n            x1 = x.clone();\n            fX2 = computeObjectiveValue(x2);\n\n            if (fX > fX2) {\n                double t = 2 * (fX + fX2 - 2 * fVal);\n                double temp = fX - fVal - delta;\n                t *= temp * temp;\n                temp = fX - fX2;\n                t -= delta * temp * temp;\n\n                if (t < 0.0) {\n                    for (int i = 0; i < n; i++) {\n                        x2[i] = x[i] + alphaMin * d[i];\n                    }\n                    fX2 = computeObjectiveValue(x2);\n\n                    // if fX2 is less than fVal, we perform the obtained transform\n                    if (fX2 < fVal) {\n                        double[] newDir = new double[n];\n                        for (int i = 0; i < n; i++) {\n                            newDir[i] = x2[i] - x[i];\n                        }\n                        final double[][] result = newPointAndDirection(x, newDir, 1.0);\n                        x = result[0];\n\n                        final int lastInd = n - 1;\n                        direc[bigInd] = MathArrays.copyOf(direc[lastInd], n);\n                        direc[lastInd] = result[1];\n                    }\n                }\n            }\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        checkParameters();\n\n        final GoalType goal = getGoalType();\n        final double[] guess = getStartPoint();\n        final int n = guess.length;\n\n        final double[][] direc = new double[n][n];\n        for (int i = 0; i < n; i++) {\n            direc[i][i] = 1;\n        }\n\n        final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n\n        double[] x = guess.clone(); // Ensure a copy is made\n        double fVal = computeObjectiveValue(x);\n        double[] x1 = x.clone();\n        int iter = 0;\n        while (true) {\n            ++iter;\n\n            double fX = fVal;\n            double fX2 = 0;\n            double delta = 0;\n            int bigInd = 0;\n            double alphaMin = 0;\n\n            for (int i = 0; i < n; i++) {\n                final double[] d = MathArrays.copyOf(direc[i]);\n\n                fX2 = fVal;\n\n                final UnivariatePointValuePair optimum = line.search(x, d);\n                fVal = optimum.getValue();\n                alphaMin = optimum.getPoint();\n                final double[][] result = newPointAndDirection(x, d, alphaMin);\n                x = result[0];\n\n                if ((fX2 - fVal) > delta) {\n                    delta = fX2 - fVal;\n                    bigInd = i;\n                }\n            }\n\n            // Default convergence check.\n            boolean stop = 2 * (fX - fVal) <=\n                (relativeThreshold * (FastMath.abs(fX) + FastMath.abs(fVal)) +\n                 absoluteThreshold);\n\n            final PointValuePair previous = new PointValuePair(x1, fX);\n            final PointValuePair current = new PointValuePair(x, fVal);\n            if (!stop) { // User-defined stopping criteria.\n                if (checker != null) {\n                    stop = checker.converged(iter, previous, current);\n                }\n            }\n            if (stop) {\n                return current;\n            }\n\n            final double[] d = new double[n];\n            final double[] x2 = new double[n];\n            for (int i = 0; i < n; i++) {\n                d[i] = x[i] - x1[i];\n                x2[i] = 2 * x[i] - x1[i];\n            }\n\n            x1 = x.clone();\n            fX2 = computeObjectiveValue(x2);\n\n            if (fX > fX2) {\n                double t = 2 * (fX + fX2 - 2 * fVal);\n                double temp = fX - fVal - delta;\n                t *= temp * temp;\n                temp = fX - fX2;\n                t -= delta * temp * temp;\n\n                if (t < 0.0) {\n                    final UnivariatePointValuePair optimum = line.search(x, d);\n                    fVal = optimum.getValue();\n                    alphaMin = optimum.getPoint();\n                    final double[][] result = newPointAndDirection(x, d, alphaMin);\n                    x = result[0];\n\n                    final int lastInd = n - 1;\n                    direc[bigInd] = direc[lastInd];\n                    direc[lastInd] = result[1];\n                }\n            }\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        checkParameters();\n\n        final GoalType goal = getGoalType();\n        final double[] guess = getStartPoint();\n        final int n = guess.length;\n\n        final double[][] direc = new double[n][n];\n        for (int i = 0; i < n; i++) {\n            direc[i][i] = 1;\n        }\n\n        final ConvergenceChecker<PointValuePair> checker\n            = getConvergenceChecker();\n\n        double[] x = guess.clone(); // Clone the guess to avoid modification\n        double fVal = computeObjectiveValue(x);\n        double[] x1 = x.clone();\n        int iter = 0;\n        while (true) {\n            ++iter;\n\n            double fX = fVal;\n            double fX2 = 0;\n            double delta = 0;\n            int bigInd = 0;\n            double alphaMin = 0;\n\n            for (int i = 0; i < n; i++) {\n                final double[] d = MathArrays.copyOf(direc[i]);\n\n                fX2 = fVal;\n\n                final UnivariatePointValuePair optimum = line.search(x, d);\n                fVal = optimum.getValue();\n                alphaMin = optimum.getPoint();\n                final double[][] result = newPointAndDirection(x, d, alphaMin);\n                x = result[0];\n\n                if ((fX2 - fVal) > delta) {\n                    delta = fX2 - fVal;\n                    bigInd = i;\n                }\n            }\n\n            // Default convergence check.\n            boolean stop = 2 * (fX - fVal) <=\n                (getRelativeThreshold() * (FastMath.abs(fX) + FastMath.abs(fVal)) +\n                 getAbsoluteThreshold()); // Use getter methods for thresholds\n\n            final PointValuePair previous = new PointValuePair(x1.clone(), fX);\n            final PointValuePair current = new PointValuePair(x.clone(), fVal);\n            if (!stop) { // User-defined stopping criteria.\n                if (checker != null) {\n                    stop = checker.converged(iter, previous, current);\n                }\n            }\n            if (stop) {\n                if (goal == GoalType.MINIMIZE) {\n                    return (fVal < fX) ? current : previous;\n                } else {\n                    return (fVal > fX) ? current : previous;\n                }\n            }\n\n            final double[] d = new double[n];\n            final double[] x2 = new double[n];\n            for (int i = 0; i < n; i++) {\n                d[i] = x[i] - x1[i];\n                x2[i] = 2 * x[i] - x1[i];\n            }\n\n            x1 = x.clone();\n            fX2 = computeObjectiveValue(x2);\n\n            if (goal == GoalType.MINIMIZE ? fX > fX2 : fX < fX2) {\n                double t = 2 * (fX + fX2 - 2 * fVal);\n                double temp = fX - fVal - delta;\n                t *= temp * temp;\n                temp = fX - fX2;\n                t -= delta * temp * temp;\n\n                if (t < 0.0) {\n                    final UnivariatePointValuePair optimum = line.search(x, d);\n                    fVal = optimum.getValue();\n                    alphaMin = optimum.getPoint();\n                    final double[][] result = newPointAndDirection(x, d, alphaMin);\n                    x = result[0];\n\n                    final int lastInd = n - 1;\n                    direc[bigInd] = direc[lastInd];\n                    direc[lastInd] = result[1];\n                }\n            }\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        checkParameters();\n\n        final GoalType goal = getGoalType();\n        final double[] guess = getStartPoint();\n        final int n = guess.length;\n\n        final double[][] direc = new double[n][n];\n        for (int i = 0; i < n; i++) {\n            direc[i][i] = 1;\n        }\n\n        final ConvergenceChecker<PointValuePair> checker\n            = getConvergenceChecker();\n\n        double[] x = guess;\n        double fVal = computeObjectiveValue(x);\n        double[] x1 = x.clone();\n        int iter = 0;\n        while (true) {\n            ++iter;\n\n            double fX = fVal;\n            double fX2 = 0;\n            double delta = 0;\n            int bigInd = 0;\n            double alphaMin = 0;\n\n            for (int i = 0; i < n; i++) {\n                final double[] d = MathArrays.copyOf(direc[i]);\n\n                fX2 = fVal;\n\n                final UnivariatePointValuePair optimum = line.search(x, d);\n                fVal = optimum.getValue();\n                alphaMin = optimum.getPoint();\n                // Fix: Assign new 'x' from the result of newPointAndDirection()\n                final double[][] result = newPointAndDirection(x, d, alphaMin);\n                x = MathArrays.copyOf(result[0]);\n\n                if ((fX2 - fVal) > delta) {\n                    delta = fX2 - fVal;\n                    bigInd = i;\n                }\n            }\n\n            // Default convergence check.\n            boolean stop = 2 * (fX - fVal) <=\n                (relativeThreshold * (FastMath.abs(fX) + FastMath.abs(fVal)) +\n                 absoluteThreshold);\n\n            final PointValuePair previous = new PointValuePair(x1, fX);\n            final PointValuePair current = new PointValuePair(x, fVal);\n            if (!stop) { // User-defined stopping criteria.\n                if (checker != null) {\n                    stop = checker.converged(iter, previous, current);\n                }\n            }\n            if (stop) {\n                if (goal == GoalType.MINIMIZE) {\n                    return (fVal < fX) ? current : previous;\n                } else {\n                    return (fVal > fX) ? current : previous;\n                }\n            }\n\n            final double[] d = new double[n];\n            final double[] x2 = new double[n];\n            for (int i = 0; i < n; i++) {\n                d[i] = x[i] - x1[i];\n                x2[i] = 2 * x[i] - x1[i];\n            }\n\n            x1 = x.clone();\n            fX2 = computeObjectiveValue(x2);\n\n            if (fX > fX2) {\n                double t = 2 * (fX + fX2 - 2 * fVal);\n                double temp = fX - fVal - delta;\n                t *= temp * temp;\n                temp = fX - fX2;\n                t -= delta * temp * temp;\n\n                if (t < 0.0) {\n                    final UnivariatePointValuePair optimum = line.search(x, d);\n                    fVal = optimum.getValue();\n                    alphaMin = optimum.getPoint();\n                    final double[][] result = newPointAndDirection(x, d, alphaMin);\n                    x = MathArrays.copyOf(result[0]);\n\n                    final int lastInd = n - 1;\n                    direc[bigInd] = direc[lastInd];\n                    direc[lastInd] = result[1];\n                }\n            }\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        checkParameters();\n\n        final GoalType goal = getGoalType();\n        final double[] guess = getStartPoint();\n        final int n = guess.length;\n\n        final double[][] direc = new double[n][n];\n        for (int i = 0; i < n; i++) {\n            direc[i][i] = 1;\n        }\n\n        final ConvergenceChecker<PointValuePair> checker\n            = getConvergenceChecker();\n\n        double[] x = guess;\n        double fVal = computeObjectiveValue(x);\n        double[] x1 = x.clone();\n        int iter = 0;\n        while (true) {\n            ++iter;\n\n            double fX = fVal;\n            double fX2 = 0;\n            double delta = 0;\n            int bigInd = 0;\n            double alphaMin = 0;\n\n            for (int i = 0; i < n; i++) {\n                final double[] d = MathArrays.copyOf(direc[i]);\n\n                fX2 = fVal;\n\n                final UnivariatePointValuePair optimum = line.search(x, d);\n                fVal = optimum.getValue();\n                alphaMin = optimum.getPoint();\n                final double[][] result = newPointAndDirection(x, d, alphaMin);\n                x = result[0];\n\n                if ((fX2 - fVal) > delta) {\n                    delta = fX2 - fVal;\n                    bigInd = i;\n                }\n            }\n\n            // Default convergence check.\n            boolean stop = 2 * (fX - fVal) <=\n                (relativeThreshold * (FastMath.abs(fX) + FastMath.abs(fVal)) +\n                 absoluteThreshold);\n\n            final PointValuePair previous = new PointValuePair(x1, fX);\n            final PointValuePair current = new PointValuePair(x, fVal);\n            if (!stop) { // User-defined stopping criteria.\n                if (checker != null) {\n                    stop = checker.converged(iter, previous, current);\n                }\n            }\n            if (stop) {\n                return current;\n            }\n\n            final double[] d = new double[n];\n            final double[] x2 = new double[n];\n            for (int i = 0; i < n; i++) {\n                d[i] = x[i] - x1[i];\n                x2[i] = 2 * x[i] - x1[i];\n            }\n\n            x1 = x.clone();\n            fX2 = computeObjectiveValue(x2);\n\n            if (fX > fX2) {\n                double t = 2 * (fX + fX2 - 2 * fVal);\n                double temp = fX - fVal - delta;\n                t *= temp * temp;\n                temp = fX - fX2;\n                t -= delta * temp * temp;\n\n                if (t < 0.0) {\n                    final UnivariatePointValuePair optimum = line.search(x, d);\n                    fVal = optimum.getValue();\n                    alphaMin = optimum.getPoint();\n                    final double[][] result = newPointAndDirection(x, d, alphaMin);\n                    x = result[0];\n\n                    final int lastInd = n - 1;\n                    direc[bigInd] = direc[lastInd];\n                    direc[lastInd] = result[1];\n                }\n            }\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        checkParameters();\n\n        final GoalType goal = getGoalType();\n        final double[] guess = getStartPoint();\n        final int n = guess.length;\n\n        final double[][] direc = new double[n][n];\n        for (int i = 0; i < n; i++) {\n            direc[i][i] = 1;\n        }\n\n        final ConvergenceChecker<PointValuePair> checker\n            = getConvergenceChecker();\n\n        double[] x = guess;\n        double fVal = computeObjectiveValue(x);\n        double[] x1 = x.clone();\n        int iter = 0;\n        while (true) {\n            ++iter;\n\n            double fX = fVal;\n            double fX2 = 0;\n            double delta = 0;\n            int bigInd = 0;\n            double alphaMin = 0;\n\n            for (int i = 0; i < n; i++) {\n                final double[] d = MathArrays.copyOf(direc[i]);\n\n                fX2 = fVal;\n\n                final UnivariatePointValuePair optimum = line.search(x, d);\n                fVal = optimum.getValue();\n                alphaMin = optimum.getPoint();\n                final double[][] result = newPointAndDirection(x, d, alphaMin);\n                x = result[0];\n\n                if ((fX2 - fVal) > delta) {\n                    delta = fX2 - fVal;\n                    bigInd = i;\n                }\n            }\n\n            // Default convergence check.\n            boolean stop = 2 * (fX - fVal) <=\n                (relativeThreshold * (FastMath.abs(fX) + FastMath.abs(fVal)) +\n                 absoluteThreshold);\n\n            final PointValuePair previous = new PointValuePair(x1, fX);\n            final PointValuePair current = new PointValuePair(x, fVal);\n            if (!stop) { // User-defined stopping criteria.\n                if (checker != null) {\n                    stop = checker.converged(iter, previous, current);\n                }\n            }\n            if (stop) {\n                if (goal == GoalType.MINIMIZE) {\n                    return (fVal < fX) ? current : previous;\n                } else {\n                    return (fVal > fX) ? current : previous;\n                }\n            }\n\n            final double[] d = new double[n];\n            final double[] x2 = new double[n];\n            for (int i = 0; i < n; i++) {\n                d[i] = x[i] - x1[i];\n                x2[i] = 2 * x[i] - x1[i];\n                x1[i] = x[i]; // Ensure x1 is updated correctly\n            }\n\n            fX2 = computeObjectiveValue(x2);\n\n            if (fX > fX2) {\n                double t = 2 * (fX + fX2 - 2 * fVal);\n                double temp = fX - fVal - delta;\n                t *= temp * temp;\n                temp = fX - fX2;\n                t -= delta * temp * temp;\n\n                if (t < 0.0) {\n                    final UnivariatePointValuePair optimum = line.search(x, d);\n                    fVal = optimum.getValue();\n                    alphaMin = optimum.getPoint();\n                    final double[][] result = newPointAndDirection(x, d, alphaMin);\n                    x = result[0];\n\n                    final int lastInd = n - 1;\n                    direc[bigInd] = direc[lastInd];\n                    direc[lastInd] = result[1];\n                }\n            }\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        checkParameters();\n\n        final GoalType goal = getGoalType();\n        final double[] guess = getStartPoint();\n        final int n = guess.length;\n\n        final double[][] direc = new double[n][n];\n        for (int i = 0; i < n; i++) {\n            direc[i][i] = 1;\n        }\n\n        final ConvergenceChecker<PointValuePair> checker\n            = getConvergenceChecker();\n\n        double[] x = guess.clone();\n        double fVal = computeObjectiveValue(x);\n        double[] x1 = x.clone();\n        int iter = 0;\n        while (true) {\n            ++iter;\n\n            double fX = fVal;\n            double fX2 = 0;\n            double delta = 0;\n            int bigInd = 0;\n            double alphaMin = 0;\n\n            for (int i = 0; i < n; i++) {\n                final double[] d = MathArrays.copyOf(direc[i]);\n\n                fX2 = fVal;\n\n                final UnivariatePointValuePair optimum = line.search(x, d);\n                fVal = optimum.getValue();\n                alphaMin = optimum.getPoint();\n                final double[][] result = newPointAndDirection(x, d, alphaMin);\n                x = result[0];\n\n                if ((fX2 - fVal) > delta) {\n                    delta = fX2 - fVal;\n                    bigInd = i;\n                }\n            }\n\n            // Default convergence check.\n            boolean stop = 2 * (fX - fVal) <=\n                (relativeThreshold * (FastMath.abs(fX) + FastMath.abs(fVal)) +\n                 absoluteThreshold);\n\n            final PointValuePair previous = new PointValuePair(x1, fX);\n            final PointValuePair current = new PointValuePair(x, fVal);\n            if (!stop) { // User-defined stopping criteria.\n                if (checker != null) {\n                    stop = checker.converged(iter, previous, current);\n                }\n            }\n            if (stop) {\n                if (goal == GoalType.MINIMIZE) {\n                    return (fVal < fX) ? current : previous;\n                } else {\n                    return (fVal > fX) ? current : previous;\n                }\n            }\n\n            final double[] d = new double[n];\n            final double[] x2 = new double[n];\n            for (int i = 0; i < n; i++) {\n                d[i] = x[i] - x1[i];\n                x2[i] = 2 * x[i] - x1[i];\n            }\n\n            x1 = x.clone();\n            fX2 = computeObjectiveValue(x2);\n\n            if (fX > fX2) {\n                double t = 2 * (fX + fX2 - 2 * fVal);\n                double temp = fX - fVal - delta;\n                t *= temp * temp;\n                temp = fX - fX2;\n                t -= delta * temp * temp;\n\n                if (t < 0.0) {\n                    final UnivariatePointValuePair optimum = line.search(x, d);\n                    fVal = optimum.getValue();\n                    alphaMin = optimum.getPoint();\n                    final double[][] result = newPointAndDirection(x, d, alphaMin);\n                    x = result[0];\n\n                    final int lastInd = n - 1;\n                    direc[bigInd] = direc[lastInd];\n                    direc[lastInd] = MathArrays.copyOf(result[1]);\n                }\n            }\n        }\n    }\n"
                        ],
                        "plausible_patches": [
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        checkParameters();\n\n        final GoalType goal = getGoalType();\n        final double[] guess = getStartPoint();\n        final int n = guess.length;\n\n        final double[][] direc = new double[n][n];\n        for (int i = 0; i < n; i++) {\n            direc[i][i] = 1;\n        }\n\n        final ConvergenceChecker<PointValuePair> checker\n            = getConvergenceChecker();\n\n        double[] x = guess;\n        double fVal = computeObjectiveValue(x);\n        double[] x1 = x.clone();\n        int iter = 0;\n        while (true) {\n            ++iter;\n\n            double fX = fVal;\n            double fX2 = 0;\n            double delta = 0;\n            int bigInd = 0;\n            double alphaMin = 0;\n\n            for (int i = 0; i < n; i++) {\n                final double[] d = MathArrays.copyOf(direc[i]);\n\n                fX2 = fVal;\n\n                final UnivariatePointValuePair optimum = line.search(x, d);\n                fVal = optimum.getValue();\n                alphaMin = optimum.getPoint();\n                x = newPointAndDirection(x, d, alphaMin)[0];\n\n                if ((fX2 - fVal) > delta) {\n                    delta = fX2 - fVal;\n                    bigInd = i;\n                }\n            }\n\n            // Default convergence check.\n            boolean stop = 2 * (fX - fVal) <=\n                    (relativeThreshold * (FastMath.abs(fX) + FastMath.abs(fVal)) +\n                            absoluteThreshold);\n\n            final PointValuePair previous = new PointValuePair(x1, fX);\n            final PointValuePair current = new PointValuePair(x, fVal);\n            if (!stop) { // User-defined stopping criteria.\n                if (checker != null) {\n                    stop = checker.converged(iter, previous, current);\n                }\n            }\n            if (stop) {\n                if (goal == GoalType.MINIMIZE) {\n                    return (fVal < fX) ? current : previous;\n                } else {\n                    return (fVal > fX) ? current : previous;\n                }\n            }\n\n            final double[] d = new double[n];\n            final double[] x2 = new double[n];\n            for (int i = 0; i < n; i++) {\n                d[i] = x[i] - x1[i];\n                x2[i] = 2 * x[i] - x1[i];\n            }\n\n            x1 = x.clone();\n            fX2 = computeObjectiveValue(x2);\n\n            if (fX > fX2) {\n                double t = 2 * (fX + fX2 - 2 * fVal);\n                double temp = fX - fVal - delta;\n                t *= temp * temp;\n                temp = fX - fX2;\n                t -= delta * temp * temp;\n\n                if (t < 0.0) {\n                    final UnivariatePointValuePair optimum = line.search(x, d);\n                    fVal = optimum.getValue();\n                    alphaMin = optimum.getPoint();\n                    final double[][] result = newPointAndDirection(x, d, alphaMin);\n                    x = result[0];\n\n                    final int lastInd = n - 1;\n                    direc[bigInd] = direc[lastInd];\n                    direc[lastInd] = result[1];\n                }\n            }\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        checkParameters();\n\n        final GoalType goal = getGoalType();\n        final double[] guess = getStartPoint();\n        final int n = guess.length;\n\n        final double[][] direc = new double[n][n];\n        for (int i = 0; i < n; i++) {\n            direc[i][i] = 1;\n        }\n\n        final ConvergenceChecker<PointValuePair> checker\n            = getConvergenceChecker();\n\n        double[] x = guess;\n        double fVal = computeObjectiveValue(x);\n        double[] x1 = x.clone();\n        int iter = 0;\n        while (true) {\n            ++iter;\n\n            double fX = fVal;\n            double fX2 = 0;\n            double delta = 0;\n            int bigInd = 0;\n            double alphaMin = 0;\n\n            for (int i = 0; i < n; i++) {\n                final double[] d = MathArrays.copyOf(direc[i]);\n\n                fX2 = fVal;\n\n                final UnivariatePointValuePair optimum = line.search(x, d);\n                fVal = optimum.getValue();\n                alphaMin = optimum.getPoint();\n                final double[][] result = newPointAndDirection(x, d, alphaMin);\n                x = result[0];\n                direc[i] = result[1];\n\n                if ((fX2 - fVal) > delta) {\n                    delta = fX2 - fVal;\n                    bigInd = i;\n                }\n            }\n\n            // Default convergence check.\n            boolean stop = 2 * (fX - fVal) <=\n                (relativeThreshold * (FastMath.abs(fX) + FastMath.abs(fVal)) +\n                absoluteThreshold);\n\n            final PointValuePair previous = new PointValuePair(x1, fX);\n            final PointValuePair current = new PointValuePair(x, fVal);\n            if (!stop) { // User-defined stopping criteria.\n                if (checker != null) {\n                    stop = checker.converged(iter, previous, current);\n                }\n            }\n            if (stop) {\n                if (goal == GoalType.MINIMIZE) {\n                    return (fVal < fX) ? current : previous;\n                } else {\n                    return (fVal > fX) ? current : previous;\n                }\n            }\n\n            final double[] d = new double[n];\n            final double[] x2 = new double[n];\n            for (int i = 0; i < n; i++) {\n                d[i] = x[i] - x1[i];\n                x2[i] = 2 * x[i] - x1[i];\n            }\n\n            x1 = x.clone();\n            fX2 = computeObjectiveValue(x2);\n\n            if (fX > fX2) {\n                double t = 2 * (fX + fX2 - 2 * fVal);\n                double temp = fX - fVal - delta;\n                t *= temp * temp;\n                temp = fX - fX2;\n                t -= delta * temp * temp;\n\n                if (t < 0.0) {\n                    final UnivariatePointValuePair optimum = line.search(x, d);\n                    fVal = optimum.getValue();\n                    alphaMin = optimum.getPoint();\n                    final double[][] result = newPointAndDirection(x, d, alphaMin);\n                    x = result[0];\n\n                    final int lastInd = n - 1;\n                    direc[bigInd] = direc[lastInd];\n                    direc[lastInd] = result[1];\n                }\n            }\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        checkParameters();\n\n        final GoalType goal = getGoalType();\n        final double[] guess = getStartPoint();\n        final int n = guess.length;\n\n        final double[][] direc = new double[n][n];\n        for (int i = 0; i < n; i++) {\n            direc[i][i] = 1;\n        }\n\n        final ConvergenceChecker<PointValuePair> checker\n            = getConvergenceChecker();\n\n        double[] x = guess.clone();\n        double fVal = computeObjectiveValue(x);\n        double[] x1 = x.clone();\n        int iter = 0;\n        while (true) {\n            ++iter;\n\n            double fX = fVal;\n            double fX2 = 0;\n            double delta = 0;\n            int bigInd = 0;\n            double alphaMin = 0;\n\n            for (int i = 0; i < n; i++) {\n                final double[] d = MathArrays.copyOf(direc[i]);\n\n                fX2 = fVal;\n\n                final UnivariatePointValuePair optimum = line.search(x, d);\n                fVal = optimum.getValue();\n                alphaMin = optimum.getPoint();\n                final double[][] result = newPointAndDirection(x, d, alphaMin);\n                x = result[0];\n\n                if ((fX2 - fVal) > delta) {\n                    delta = fX2 - fVal;\n                    bigInd = i;\n                }\n            }\n\n            // Default convergence check.\n            boolean stop = 2 * (fX - fVal) <=\n                (relativeThreshold * (FastMath.abs(fX) + FastMath.abs(fVal)) +\n                 absoluteThreshold);\n\n            final PointValuePair previous = new PointValuePair(x1, fX);\n            final PointValuePair current = new PointValuePair(x, fVal);\n            if (!stop) { // User-defined stopping criteria.\n                if (checker != null) {\n                    stop = checker.converged(iter, previous, current);\n                }\n            }\n            if (stop) {\n                if (goal == GoalType.MINIMIZE) {\n                    return (fVal < fX) ? current : previous;\n                } else {\n                    return (fVal > fX) ? current : previous;\n                }\n            }\n\n            final double[] d = new double[n];\n            final double[] x2 = new double[n];\n            for (int i = 0; i < n; i++) {\n                d[i] = x[i] - x1[i];\n                x2[i] = 2 * x[i] - x1[i];\n            }\n\n            x1 = x.clone();\n            fX2 = computeObjectiveValue(x2);\n\n            if (fX > fX2) {\n                double t = 2 * (fX + fX2 - 2 * fVal);\n                double temp = fX - fVal - delta;\n                t *= temp * temp;\n                temp = fX - fX2;\n                t -= delta * temp * temp;\n\n                if (t < 0.0) {\n                    for (int i = 0; i < n; i++) {\n                        x2[i] = x[i] + alphaMin * d[i];\n                    }\n                    fX2 = computeObjectiveValue(x2);\n\n                    // if fX2 is less than fVal, we perform the obtained transform\n                    if (fX2 < fVal) {\n                        double[] newDir = new double[n];\n                        for (int i = 0; i < n; i++) {\n                            newDir[i] = x2[i] - x[i];\n                        }\n                        final double[][] result = newPointAndDirection(x, newDir, 1.0);\n                        x = result[0];\n\n                        final int lastInd = n - 1;\n                        direc[bigInd] = MathArrays.copyOf(direc[lastInd], n);\n                        direc[lastInd] = result[1];\n                    }\n                }\n            }\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        checkParameters();\n\n        final GoalType goal = getGoalType();\n        final double[] guess = getStartPoint();\n        final int n = guess.length;\n\n        final double[][] direc = new double[n][n];\n        for (int i = 0; i < n; i++) {\n            direc[i][i] = 1;\n        }\n\n        final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n\n        double[] x = guess.clone(); // Ensure a copy is made\n        double fVal = computeObjectiveValue(x);\n        double[] x1 = x.clone();\n        int iter = 0;\n        while (true) {\n            ++iter;\n\n            double fX = fVal;\n            double fX2 = 0;\n            double delta = 0;\n            int bigInd = 0;\n            double alphaMin = 0;\n\n            for (int i = 0; i < n; i++) {\n                final double[] d = MathArrays.copyOf(direc[i]);\n\n                fX2 = fVal;\n\n                final UnivariatePointValuePair optimum = line.search(x, d);\n                fVal = optimum.getValue();\n                alphaMin = optimum.getPoint();\n                final double[][] result = newPointAndDirection(x, d, alphaMin);\n                x = result[0];\n\n                if ((fX2 - fVal) > delta) {\n                    delta = fX2 - fVal;\n                    bigInd = i;\n                }\n            }\n\n            // Default convergence check.\n            boolean stop = 2 * (fX - fVal) <=\n                (relativeThreshold * (FastMath.abs(fX) + FastMath.abs(fVal)) +\n                 absoluteThreshold);\n\n            final PointValuePair previous = new PointValuePair(x1, fX);\n            final PointValuePair current = new PointValuePair(x, fVal);\n            if (!stop) { // User-defined stopping criteria.\n                if (checker != null) {\n                    stop = checker.converged(iter, previous, current);\n                }\n            }\n            if (stop) {\n                return current;\n            }\n\n            final double[] d = new double[n];\n            final double[] x2 = new double[n];\n            for (int i = 0; i < n; i++) {\n                d[i] = x[i] - x1[i];\n                x2[i] = 2 * x[i] - x1[i];\n            }\n\n            x1 = x.clone();\n            fX2 = computeObjectiveValue(x2);\n\n            if (fX > fX2) {\n                double t = 2 * (fX + fX2 - 2 * fVal);\n                double temp = fX - fVal - delta;\n                t *= temp * temp;\n                temp = fX - fX2;\n                t -= delta * temp * temp;\n\n                if (t < 0.0) {\n                    final UnivariatePointValuePair optimum = line.search(x, d);\n                    fVal = optimum.getValue();\n                    alphaMin = optimum.getPoint();\n                    final double[][] result = newPointAndDirection(x, d, alphaMin);\n                    x = result[0];\n\n                    final int lastInd = n - 1;\n                    direc[bigInd] = direc[lastInd];\n                    direc[lastInd] = result[1];\n                }\n            }\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        checkParameters();\n\n        final GoalType goal = getGoalType();\n        final double[] guess = getStartPoint();\n        final int n = guess.length;\n\n        final double[][] direc = new double[n][n];\n        for (int i = 0; i < n; i++) {\n            direc[i][i] = 1;\n        }\n\n        final ConvergenceChecker<PointValuePair> checker\n            = getConvergenceChecker();\n\n        double[] x = guess;\n        double fVal = computeObjectiveValue(x);\n        double[] x1 = x.clone();\n        int iter = 0;\n        while (true) {\n            ++iter;\n\n            double fX = fVal;\n            double fX2 = 0;\n            double delta = 0;\n            int bigInd = 0;\n            double alphaMin = 0;\n\n            for (int i = 0; i < n; i++) {\n                final double[] d = MathArrays.copyOf(direc[i]);\n\n                fX2 = fVal;\n\n                final UnivariatePointValuePair optimum = line.search(x, d);\n                fVal = optimum.getValue();\n                alphaMin = optimum.getPoint();\n                // Fix: Assign new 'x' from the result of newPointAndDirection()\n                final double[][] result = newPointAndDirection(x, d, alphaMin);\n                x = MathArrays.copyOf(result[0]);\n\n                if ((fX2 - fVal) > delta) {\n                    delta = fX2 - fVal;\n                    bigInd = i;\n                }\n            }\n\n            // Default convergence check.\n            boolean stop = 2 * (fX - fVal) <=\n                (relativeThreshold * (FastMath.abs(fX) + FastMath.abs(fVal)) +\n                 absoluteThreshold);\n\n            final PointValuePair previous = new PointValuePair(x1, fX);\n            final PointValuePair current = new PointValuePair(x, fVal);\n            if (!stop) { // User-defined stopping criteria.\n                if (checker != null) {\n                    stop = checker.converged(iter, previous, current);\n                }\n            }\n            if (stop) {\n                if (goal == GoalType.MINIMIZE) {\n                    return (fVal < fX) ? current : previous;\n                } else {\n                    return (fVal > fX) ? current : previous;\n                }\n            }\n\n            final double[] d = new double[n];\n            final double[] x2 = new double[n];\n            for (int i = 0; i < n; i++) {\n                d[i] = x[i] - x1[i];\n                x2[i] = 2 * x[i] - x1[i];\n            }\n\n            x1 = x.clone();\n            fX2 = computeObjectiveValue(x2);\n\n            if (fX > fX2) {\n                double t = 2 * (fX + fX2 - 2 * fVal);\n                double temp = fX - fVal - delta;\n                t *= temp * temp;\n                temp = fX - fX2;\n                t -= delta * temp * temp;\n\n                if (t < 0.0) {\n                    final UnivariatePointValuePair optimum = line.search(x, d);\n                    fVal = optimum.getValue();\n                    alphaMin = optimum.getPoint();\n                    final double[][] result = newPointAndDirection(x, d, alphaMin);\n                    x = MathArrays.copyOf(result[0]);\n\n                    final int lastInd = n - 1;\n                    direc[bigInd] = direc[lastInd];\n                    direc[lastInd] = result[1];\n                }\n            }\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        checkParameters();\n\n        final GoalType goal = getGoalType();\n        final double[] guess = getStartPoint();\n        final int n = guess.length;\n\n        final double[][] direc = new double[n][n];\n        for (int i = 0; i < n; i++) {\n            direc[i][i] = 1;\n        }\n\n        final ConvergenceChecker<PointValuePair> checker\n            = getConvergenceChecker();\n\n        double[] x = guess;\n        double fVal = computeObjectiveValue(x);\n        double[] x1 = x.clone();\n        int iter = 0;\n        while (true) {\n            ++iter;\n\n            double fX = fVal;\n            double fX2 = 0;\n            double delta = 0;\n            int bigInd = 0;\n            double alphaMin = 0;\n\n            for (int i = 0; i < n; i++) {\n                final double[] d = MathArrays.copyOf(direc[i]);\n\n                fX2 = fVal;\n\n                final UnivariatePointValuePair optimum = line.search(x, d);\n                fVal = optimum.getValue();\n                alphaMin = optimum.getPoint();\n                final double[][] result = newPointAndDirection(x, d, alphaMin);\n                x = result[0];\n\n                if ((fX2 - fVal) > delta) {\n                    delta = fX2 - fVal;\n                    bigInd = i;\n                }\n            }\n\n            // Default convergence check.\n            boolean stop = 2 * (fX - fVal) <=\n                (relativeThreshold * (FastMath.abs(fX) + FastMath.abs(fVal)) +\n                 absoluteThreshold);\n\n            final PointValuePair previous = new PointValuePair(x1, fX);\n            final PointValuePair current = new PointValuePair(x, fVal);\n            if (!stop) { // User-defined stopping criteria.\n                if (checker != null) {\n                    stop = checker.converged(iter, previous, current);\n                }\n            }\n            if (stop) {\n                return current;\n            }\n\n            final double[] d = new double[n];\n            final double[] x2 = new double[n];\n            for (int i = 0; i < n; i++) {\n                d[i] = x[i] - x1[i];\n                x2[i] = 2 * x[i] - x1[i];\n            }\n\n            x1 = x.clone();\n            fX2 = computeObjectiveValue(x2);\n\n            if (fX > fX2) {\n                double t = 2 * (fX + fX2 - 2 * fVal);\n                double temp = fX - fVal - delta;\n                t *= temp * temp;\n                temp = fX - fX2;\n                t -= delta * temp * temp;\n\n                if (t < 0.0) {\n                    final UnivariatePointValuePair optimum = line.search(x, d);\n                    fVal = optimum.getValue();\n                    alphaMin = optimum.getPoint();\n                    final double[][] result = newPointAndDirection(x, d, alphaMin);\n                    x = result[0];\n\n                    final int lastInd = n - 1;\n                    direc[bigInd] = direc[lastInd];\n                    direc[lastInd] = result[1];\n                }\n            }\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        checkParameters();\n\n        final GoalType goal = getGoalType();\n        final double[] guess = getStartPoint();\n        final int n = guess.length;\n\n        final double[][] direc = new double[n][n];\n        for (int i = 0; i < n; i++) {\n            direc[i][i] = 1;\n        }\n\n        final ConvergenceChecker<PointValuePair> checker\n            = getConvergenceChecker();\n\n        double[] x = guess;\n        double fVal = computeObjectiveValue(x);\n        double[] x1 = x.clone();\n        int iter = 0;\n        while (true) {\n            ++iter;\n\n            double fX = fVal;\n            double fX2 = 0;\n            double delta = 0;\n            int bigInd = 0;\n            double alphaMin = 0;\n\n            for (int i = 0; i < n; i++) {\n                final double[] d = MathArrays.copyOf(direc[i]);\n\n                fX2 = fVal;\n\n                final UnivariatePointValuePair optimum = line.search(x, d);\n                fVal = optimum.getValue();\n                alphaMin = optimum.getPoint();\n                final double[][] result = newPointAndDirection(x, d, alphaMin);\n                x = result[0];\n\n                if ((fX2 - fVal) > delta) {\n                    delta = fX2 - fVal;\n                    bigInd = i;\n                }\n            }\n\n            // Default convergence check.\n            boolean stop = 2 * (fX - fVal) <=\n                (relativeThreshold * (FastMath.abs(fX) + FastMath.abs(fVal)) +\n                 absoluteThreshold);\n\n            final PointValuePair previous = new PointValuePair(x1, fX);\n            final PointValuePair current = new PointValuePair(x, fVal);\n            if (!stop) { // User-defined stopping criteria.\n                if (checker != null) {\n                    stop = checker.converged(iter, previous, current);\n                }\n            }\n            if (stop) {\n                if (goal == GoalType.MINIMIZE) {\n                    return (fVal < fX) ? current : previous;\n                } else {\n                    return (fVal > fX) ? current : previous;\n                }\n            }\n\n            final double[] d = new double[n];\n            final double[] x2 = new double[n];\n            for (int i = 0; i < n; i++) {\n                d[i] = x[i] - x1[i];\n                x2[i] = 2 * x[i] - x1[i];\n                x1[i] = x[i]; // Ensure x1 is updated correctly\n            }\n\n            fX2 = computeObjectiveValue(x2);\n\n            if (fX > fX2) {\n                double t = 2 * (fX + fX2 - 2 * fVal);\n                double temp = fX - fVal - delta;\n                t *= temp * temp;\n                temp = fX - fX2;\n                t -= delta * temp * temp;\n\n                if (t < 0.0) {\n                    final UnivariatePointValuePair optimum = line.search(x, d);\n                    fVal = optimum.getValue();\n                    alphaMin = optimum.getPoint();\n                    final double[][] result = newPointAndDirection(x, d, alphaMin);\n                    x = result[0];\n\n                    final int lastInd = n - 1;\n                    direc[bigInd] = direc[lastInd];\n                    direc[lastInd] = result[1];\n                }\n            }\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        checkParameters();\n\n        final GoalType goal = getGoalType();\n        final double[] guess = getStartPoint();\n        final int n = guess.length;\n\n        final double[][] direc = new double[n][n];\n        for (int i = 0; i < n; i++) {\n            direc[i][i] = 1;\n        }\n\n        final ConvergenceChecker<PointValuePair> checker\n            = getConvergenceChecker();\n\n        double[] x = guess.clone();\n        double fVal = computeObjectiveValue(x);\n        double[] x1 = x.clone();\n        int iter = 0;\n        while (true) {\n            ++iter;\n\n            double fX = fVal;\n            double fX2 = 0;\n            double delta = 0;\n            int bigInd = 0;\n            double alphaMin = 0;\n\n            for (int i = 0; i < n; i++) {\n                final double[] d = MathArrays.copyOf(direc[i]);\n\n                fX2 = fVal;\n\n                final UnivariatePointValuePair optimum = line.search(x, d);\n                fVal = optimum.getValue();\n                alphaMin = optimum.getPoint();\n                final double[][] result = newPointAndDirection(x, d, alphaMin);\n                x = result[0];\n\n                if ((fX2 - fVal) > delta) {\n                    delta = fX2 - fVal;\n                    bigInd = i;\n                }\n            }\n\n            // Default convergence check.\n            boolean stop = 2 * (fX - fVal) <=\n                (relativeThreshold * (FastMath.abs(fX) + FastMath.abs(fVal)) +\n                 absoluteThreshold);\n\n            final PointValuePair previous = new PointValuePair(x1, fX);\n            final PointValuePair current = new PointValuePair(x, fVal);\n            if (!stop) { // User-defined stopping criteria.\n                if (checker != null) {\n                    stop = checker.converged(iter, previous, current);\n                }\n            }\n            if (stop) {\n                if (goal == GoalType.MINIMIZE) {\n                    return (fVal < fX) ? current : previous;\n                } else {\n                    return (fVal > fX) ? current : previous;\n                }\n            }\n\n            final double[] d = new double[n];\n            final double[] x2 = new double[n];\n            for (int i = 0; i < n; i++) {\n                d[i] = x[i] - x1[i];\n                x2[i] = 2 * x[i] - x1[i];\n            }\n\n            x1 = x.clone();\n            fX2 = computeObjectiveValue(x2);\n\n            if (fX > fX2) {\n                double t = 2 * (fX + fX2 - 2 * fVal);\n                double temp = fX - fVal - delta;\n                t *= temp * temp;\n                temp = fX - fX2;\n                t -= delta * temp * temp;\n\n                if (t < 0.0) {\n                    final UnivariatePointValuePair optimum = line.search(x, d);\n                    fVal = optimum.getValue();\n                    alphaMin = optimum.getPoint();\n                    final double[][] result = newPointAndDirection(x, d, alphaMin);\n                    x = result[0];\n\n                    final int lastInd = n - 1;\n                    direc[bigInd] = direc[lastInd];\n                    direc[lastInd] = MathArrays.copyOf(result[1]);\n                }\n            }\n        }\n    }\n"
                        ]
                    }
                }
            },
            {
                "path": "src/main/java/org/apache/commons/math3/optim/nonlinear/scalar/noderiv/SimplexOptimizer.java",
                "start_loc": 126,
                "end_loc": 177,
                "buggy_function": "    @Override\n    protected PointValuePair doOptimize() {\n        checkParameters();\n\n        // Indirect call to \"computeObjectiveValue\" in order to update the\n        // evaluations counter.\n        final MultivariateFunction evalFunc\n            = new MultivariateFunction() {\n                public double value(double[] point) {\n                    return computeObjectiveValue(point);\n                }\n            };\n\n        final boolean isMinim = getGoalType() == GoalType.MINIMIZE;\n        final Comparator<PointValuePair> comparator\n            = new Comparator<PointValuePair>() {\n            public int compare(final PointValuePair o1,\n                               final PointValuePair o2) {\n                final double v1 = o1.getValue();\n                final double v2 = o2.getValue();\n                return isMinim ? Double.compare(v1, v2) : Double.compare(v2, v1);\n            }\n        };\n\n        // Initialize search.\n        simplex.build(getStartPoint());\n        simplex.evaluate(evalFunc, comparator);\n\n        PointValuePair[] previous = null;\n        int iteration = 0;\n        final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n        while (true) {\n            if (iteration > 0) {\n                boolean converged = true;\n                for (int i = 0; i < simplex.getSize(); i++) {\n                    PointValuePair prev = previous[i];\n                    converged = converged &&\n                        checker.converged(iteration, prev, simplex.getPoint(i));\n                }\n                if (converged) {\n                    // We have found an optimum.\n                    return simplex.getPoint(0);\n                }\n            }\n\n            // We still need to search.\n            previous = simplex.getPoints();\n            simplex.iterate(evalFunc, comparator);\n\n\t\t\t++iteration;\n        }\n    }\n",
                "fixed_function": "    @Override\n    protected PointValuePair doOptimize() {\n        checkParameters();\n\n        // Indirect call to \"computeObjectiveValue\" in order to update the\n        // evaluations counter.\n        final MultivariateFunction evalFunc\n            = new MultivariateFunction() {\n                public double value(double[] point) {\n                    return computeObjectiveValue(point);\n                }\n            };\n\n        final boolean isMinim = getGoalType() == GoalType.MINIMIZE;\n        final Comparator<PointValuePair> comparator\n            = new Comparator<PointValuePair>() {\n            public int compare(final PointValuePair o1,\n                               final PointValuePair o2) {\n                final double v1 = o1.getValue();\n                final double v2 = o2.getValue();\n                return isMinim ? Double.compare(v1, v2) : Double.compare(v2, v1);\n            }\n        };\n\n        // Initialize search.\n        simplex.build(getStartPoint());\n        simplex.evaluate(evalFunc, comparator);\n\n        PointValuePair[] previous = null;\n        int iteration = 0;\n        final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n        while (true) {\n            if (getIterations() > 0) {\n                boolean converged = true;\n                for (int i = 0; i < simplex.getSize(); i++) {\n                    PointValuePair prev = previous[i];\n                    converged = converged &&\n                        checker.converged(iteration, prev, simplex.getPoint(i));\n                }\n                if (converged) {\n                    // We have found an optimum.\n                    return simplex.getPoint(0);\n                }\n            }\n\n            // We still need to search.\n            previous = simplex.getPoints();\n            simplex.iterate(evalFunc, comparator);\n\n            incrementIterationCount();\n        }\n    }\n",
                "comment": "    /** {@inheritDoc} */\n",
                "normalized_body": [
                    "@Override protected PointValuePair doOptimize() { checkParameters(); final MultivariateFunction evalFunc = new MultivariateFunction() { public double value(double[] point) { return computeObjectiveValue(point); } }; final boolean isMinim = getGoalType() == GoalType.MINIMIZE; final Comparator<PointValuePair> comparator = new Comparator<PointValuePair>() { public int compare(final PointValuePair o1, final PointValuePair o2) { final double v1 = o1.getValue(); final double v2 = o2.getValue(); return isMinim ? Double.compare(v1, v2) : Double.compare(v2, v1); } }; simplex.build(getStartPoint()); simplex.evaluate(evalFunc, comparator); PointValuePair[] previous = null; int iteration = 0; final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker(); while (true) { if (iteration > 0) { boolean converged = true; for (int i = 0; i < simplex.getSize(); i++) { PointValuePair prev = previous[i]; converged = converged && checker.converged(iteration, prev, simplex.getPoint(i)); } if (converged) { return simplex.getPoint(0); } } previous = simplex.getPoints(); simplex.iterate(evalFunc, comparator); ++iteration; } }"
                ],
                "top_similar_methods": [
                    [
                        0.9999998807907104,
                        "@Override protected PointValuePair doOptimize() { checkParameters(); final MultivariateFunction evalFunc = new MultivariateFunction() { public double value(double[] point) { return computeObjectiveValue(point); } }; final boolean isMinim = getGoalType() == GoalType.MINIMIZE; final Comparator<PointValuePair> comparator = new Comparator<PointValuePair>() { public int compare(final PointValuePair o1, final PointValuePair o2) { final double v1 = o1.getValue(); final double v2 = o2.getValue(); return isMinim ? Double.compare(v1, v2) : Double.compare(v2, v1); } }; simplex.build(getStartPoint()); simplex.evaluate(evalFunc, comparator); PointValuePair[] previous = null; int iteration = 0; final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker(); while (true) { if (iteration > 0) { boolean converged = true; for (int i = 0; i < simplex.getSize(); i++) { PointValuePair prev = previous[i]; converged = converged && checker.converged(iteration, prev, simplex.getPoint(i)); } if (converged) { return simplex.getPoint(0); } } previous = simplex.getPoints(); simplex.iterate(evalFunc, comparator); ++iteration; } }"
                    ],
                    [
                        0.9940184354782104,
                        "@Override protected PointValuePair doOptimize() { if (simplex == null) { throw new NullArgumentException(); } final MultivariateFunction evalFunc = new MultivariateFunction() { public double value(double[] point) { return computeObjectiveValue(point); } }; final boolean isMinim = getGoalType() == GoalType.MINIMIZE; final Comparator<PointValuePair> comparator = new Comparator<PointValuePair>() { public int compare(final PointValuePair o1, final PointValuePair o2) { final double v1 = o1.getValue(); final double v2 = o2.getValue(); return isMinim ? Double.compare(v1, v2) : Double.compare(v2, v1); } }; simplex.build(getStartPoint()); simplex.evaluate(evalFunc, comparator); PointValuePair[] previous = null; int iteration = 0; final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker(); while (true) { if (iteration > 0) { boolean converged = true; for (int i = 0; i < simplex.getSize(); i++) { PointValuePair prev = previous[i]; converged = converged && checker.converged(iteration, prev, simplex.getPoint(i)); } if (converged) { return simplex.getPoint(0); } } previous = simplex.getPoints(); simplex.iterate(evalFunc, comparator); ++iteration; } }"
                    ],
                    [
                        0.9853882789611816,
                        "@Override protected PointValuePair doOptimize() { final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker(); final double[] point = getStartPoint(); final GoalType goal = getGoalType(); final int n = point.length; double[] r = computeObjectiveGradient(point); if (goal == GoalType.MINIMIZE) { for (int i = 0; i < n; i++) { r[i] = -r[i]; } } double[] steepestDescent = preconditioner.precondition(point, r); double[] searchDirection = steepestDescent.clone(); double delta = 0; for (int i = 0; i < n; ++i) { delta += r[i] * searchDirection[i]; } PointValuePair current = null; int iter = 0; int maxEval = getMaxEvaluations(); while (true) { ++iter; final double objective = computeObjectiveValue(point); PointValuePair previous = current; current = new PointValuePair(point, objective); if (previous != null) { if (checker.converged(iter, previous, current)) { return current; } } final UnivariateFunction lsf = new LineSearchFunction(point, searchDirection); final double uB = findUpperBound(lsf, 0, initialStep); final double step = solver.solve(maxEval, lsf, 0, uB, 1e-15); maxEval -= solver.getEvaluations(); for (int i = 0; i < point.length; ++i) { point[i] += step * searchDirection[i]; } r = computeObjectiveGradient(point); if (goal == GoalType.MINIMIZE) { for (int i = 0; i < n; ++i) { r[i] = -r[i]; } } final double deltaOld = delta; final double[] newSteepestDescent = preconditioner.precondition(point, r); delta = 0; for (int i = 0; i < n; ++i) { delta += r[i] * newSteepestDescent[i]; } final double beta; switch(updateFormula) { case FLETCHER_REEVES: beta = delta / deltaOld; break; case POLAK_RIBIERE: double deltaMid = 0; for (int i = 0; i < r.length; ++i) { deltaMid += r[i] * steepestDescent[i]; } beta = (delta - deltaMid) / deltaOld; break; default: throw new MathInternalError(); } steepestDescent = newSteepestDescent; if (iter % n == 0 || beta < 0) { searchDirection = steepestDescent.clone(); } else { for (int i = 0; i < n; ++i) { searchDirection[i] = steepestDescent[i] + beta * searchDirection[i]; } } } }"
                    ],
                    [
                        0.9837446212768555,
                        "@Test public void testParametersErrorMonteCarloObservations() { final double yError = 15; final double slope = 123.456; final double offset = -98.765; final RandomStraightLinePointGenerator lineGenerator = new RandomStraightLinePointGenerator(slope, offset, yError, -1e3, 1e4, 138577L); final int numObs = 100; final int numParams = 2; final SummaryStatistics[] paramsFoundByDirectSolution = new SummaryStatistics[numParams]; final SummaryStatistics[] sigmaEstimate = new SummaryStatistics[numParams]; for (int i = 0; i < numParams; i++) { paramsFoundByDirectSolution[i] = new SummaryStatistics(); sigmaEstimate[i] = new SummaryStatistics(); } final AbstractLeastSquaresOptimizer optim = new DummyOptimizer(); final double[] init = { slope, offset }; final int mcRepeat = MONTE_CARLO_RUNS; int mcCount = 0; while (mcCount < mcRepeat) { final Point2D.Double[] obs = lineGenerator.generate(numObs); final StraightLineProblem problem = new StraightLineProblem(yError); for (int i = 0; i < numObs; i++) { final Point2D.Double p = obs[i]; problem.addPoint(p.x, p.y); } final double[] regress = problem.solve(); final PointVectorValuePair optimum = optim.optimize(new MaxEval(Integer.MAX_VALUE), problem.getModelFunction(), problem.getModelFunctionJacobian(), new Target(problem.target()), new Weight(problem.weight()), new InitialGuess(init)); final double[] sigma = optim.computeSigma(optimum.getPoint(), 1e-14); for (int i = 0; i < numParams; i++) { paramsFoundByDirectSolution[i].addValue(regress[i]); sigmaEstimate[i].addValue(sigma[i]); } ++mcCount; } final String line = \\\"--------------------------------------------------------------\\\"; System.out.println(\\\" True value Mean Std deviation\\\"); for (int i = 0; i < numParams; i++) { System.out.println(line); System.out.println(\\\"Parameter #\\\" + i); StatisticalSummary s = paramsFoundByDirectSolution[i].getSummary(); System.out.printf(\\\" %+.6e %+.6e %+.6e\\n\\\", init[i], s.getMean(), s.getStandardDeviation()); s = sigmaEstimate[i].getSummary(); System.out.printf(\\\"sigma: %+.6e (%+.6e)\\n\\\", s.getMean(), s.getStandardDeviation()); } System.out.println(line); for (int i = 0; i < numParams; i++) { Assert.assertEquals(paramsFoundByDirectSolution[i].getSummary().getStandardDeviation(), sigmaEstimate[i].getSummary().getMean(), 8e-2); } }"
                    ],
                    [
                        0.9836817979812622,
                        "public void handleStep(final StepInterpolator interpolator, final boolean isLast) throws MaxCountExceededException { if (lastState == null) { firstTime = interpolator.getPreviousTime(); lastTime = interpolator.getPreviousTime(); interpolator.setInterpolatedTime(lastTime); lastState = interpolator.getInterpolatedState().clone(); lastDerivatives = interpolator.getInterpolatedDerivatives().clone(); forward = interpolator.getCurrentTime() >= lastTime; if (!forward) { h = -h; } } double nextTime = (mode == StepNormalizerMode.INCREMENT) ? lastTime + h : (FastMath.floor(lastTime / h) + 1) * h; if (mode == StepNormalizerMode.MULTIPLES && Precision.equals(nextTime, lastTime, 1)) { nextTime += h; } boolean nextInStep = isNextInStep(nextTime, interpolator); while (nextInStep) { doNormalizedStep(false); storeStep(interpolator, nextTime); nextTime += h; nextInStep = isNextInStep(nextTime, interpolator); } if (isLast) { boolean addLast = bounds.lastIncluded() && lastTime != interpolator.getCurrentTime(); doNormalizedStep(!addLast); if (addLast) { storeStep(interpolator, interpolator.getCurrentTime()); doNormalizedStep(true); } } }"
                    ],
                    [
                        0.9831656217575073,
                        "public void saundersTest(final int n, final boolean goodb, final boolean precon, final double shift, final double pertbn) { final RealLinearOperator a = new RealLinearOperator() { @Override public RealVector operate(final RealVector x) { if (x.getDimension() != n) { throw new DimensionMismatchException(x.getDimension(), n); } final double[] y = new double[n]; for (int i = 0; i < n; i++) { y[i] = (i + 1) * 1.1 / n * x.getEntry(i); } return new ArrayRealVector(y, false); } @Override public int getRowDimension() { return n; } @Override public int getColumnDimension() { return n; } }; final double shiftm = shift; final double pertm = FastMath.abs(pertbn); final RealLinearOperator minv; if (precon) { minv = new RealLinearOperator() { @Override public int getRowDimension() { return n; } @Override public int getColumnDimension() { return n; } @Override public RealVector operate(final RealVector x) { if (x.getDimension() != n) { throw new DimensionMismatchException(x.getDimension(), n); } final double[] y = new double[n]; for (int i = 0; i < n; i++) { double d = (i + 1) * 1.1 / n; d = FastMath.abs(d - shiftm); if (i % 10 == 0) { d += pertm; } y[i] = x.getEntry(i) / d; } return new ArrayRealVector(y, false); } }; } else { minv = null; } final RealVector xtrue = new ArrayRealVector(n); for (int i = 0; i < n; i++) { xtrue.setEntry(i, n - i); } final RealVector b = a.operate(xtrue); b.combineToSelf(1.0, -shift, xtrue); final SymmLQ solver = new SymmLQ(2 * n, 1E-12, true); final RealVector x = solver.solve(a, minv, b, goodb, shift); final RealVector y = a.operate(x); final RealVector r1 = new ArrayRealVector(n); for (int i = 0; i < n; i++) { final double bi = b.getEntry(i); final double yi = y.getEntry(i); final double xi = x.getEntry(i); r1.setEntry(i, bi - yi + shift * xi); } final double enorm = x.subtract(xtrue).getNorm() / xtrue.getNorm(); final double etol = 1E-5; Assert.assertTrue(\\\"enorm=\\\" + enorm + \\\", \\\" + solver.getIterationManager().getIterations(), enorm <= etol); }"
                    ],
                    [
                        0.9829344749450684,
                        "protected double acceptStep(final AbstractStepInterpolator interpolator, final double[] y, final double[] yDot, final double tEnd) throws MaxCountExceededException, DimensionMismatchException, NoBracketingException { double previousT = interpolator.getGlobalPreviousTime(); final double currentT = interpolator.getGlobalCurrentTime(); if (!statesInitialized) { for (EventState state : eventsStates) { state.reinitializeBegin(interpolator); } statesInitialized = true; } final int orderingSign = interpolator.isForward() ? +1 : -1; SortedSet<EventState> occuringEvents = new TreeSet<EventState>(new Comparator<EventState>() { public int compare(EventState es0, EventState es1) { return orderingSign * Double.compare(es0.getEventTime(), es1.getEventTime()); } }); for (final EventState state : eventsStates) { if (state.evaluateStep(interpolator)) { occuringEvents.add(state); } } while (!occuringEvents.isEmpty()) { final Iterator<EventState> iterator = occuringEvents.iterator(); final EventState currentEvent = iterator.next(); iterator.remove(); final double eventT = currentEvent.getEventTime(); interpolator.setSoftPreviousTime(previousT); interpolator.setSoftCurrentTime(eventT); interpolator.setInterpolatedTime(eventT); final double[] eventY = interpolator.getInterpolatedState().clone(); for (final EventState state : eventsStates) { state.stepAccepted(eventT, eventY); isLastStep = isLastStep || state.stop(); } for (final StepHandler handler : stepHandlers) { handler.handleStep(interpolator, isLastStep); } if (isLastStep) { System.arraycopy(eventY, 0, y, 0, y.length); return eventT; } boolean needReset = false; for (final EventState state : eventsStates) { needReset = needReset || state.reset(eventT, eventY); } if (needReset) { System.arraycopy(eventY, 0, y, 0, y.length); computeDerivatives(eventT, y, yDot); resetOccurred = true; return eventT; } previousT = eventT; interpolator.setSoftPreviousTime(eventT); interpolator.setSoftCurrentTime(currentT); if (currentEvent.evaluateStep(interpolator)) { occuringEvents.add(currentEvent); } } interpolator.setInterpolatedTime(currentT); final double[] currentY = interpolator.getInterpolatedState(); for (final EventState state : eventsStates) { state.stepAccepted(currentT, currentY); isLastStep = isLastStep || state.stop(); } isLastStep = isLastStep || Precision.equals(currentT, tEnd, 1); for (StepHandler handler : stepHandlers) { handler.handleStep(interpolator, isLastStep); } return currentT; }"
                    ],
                    [
                        0.9825053811073303,
                        "@Test public void testParametersErrorMonteCarloObservations() { final double yError = 15; final double slope = 123.456; final double offset = -98.765; final RandomStraightLinePointGenerator lineGenerator = new RandomStraightLinePointGenerator(slope, offset, yError, -1e3, 1e4, 138577L); final int numObs = 100; final int numParams = 2; final SummaryStatistics[] paramsFoundByDirectSolution = new SummaryStatistics[numParams]; final SummaryStatistics[] sigmaEstimate = new SummaryStatistics[numParams]; for (int i = 0; i < numParams; i++) { paramsFoundByDirectSolution[i] = new SummaryStatistics(); sigmaEstimate[i] = new SummaryStatistics(); } final AbstractLeastSquaresOptimizer optim = new DummyOptimizer(); final double[] init = { slope, offset }; final int mcRepeat = MONTE_CARLO_RUNS; int mcCount = 0; while (mcCount < mcRepeat) { final Point2D.Double[] obs = lineGenerator.generate(numObs); final StraightLineProblem problem = new StraightLineProblem(yError); for (int i = 0; i < numObs; i++) { final Point2D.Double p = obs[i]; problem.addPoint(p.x, p.y); } final double[] regress = problem.solve(); final PointVectorValuePair optimum = optim.optimize(Integer.MAX_VALUE, problem, problem.target(), problem.weight(), init); final double[] sigma = optim.computeSigma(optimum.getPoint(), 1e-14); for (int i = 0; i < numParams; i++) { paramsFoundByDirectSolution[i].addValue(regress[i]); sigmaEstimate[i].addValue(sigma[i]); } ++mcCount; } final String line = \\\"--------------------------------------------------------------\\\"; System.out.println(\\\" True value Mean Std deviation\\\"); for (int i = 0; i < numParams; i++) { System.out.println(line); System.out.println(\\\"Parameter #\\\" + i); StatisticalSummary s = paramsFoundByDirectSolution[i].getSummary(); System.out.printf(\\\" %+.6e %+.6e %+.6e\\n\\\", init[i], s.getMean(), s.getStandardDeviation()); s = sigmaEstimate[i].getSummary(); System.out.printf(\\\"sigma: %+.6e (%+.6e)\\n\\\", s.getMean(), s.getStandardDeviation()); } System.out.println(line); for (int i = 0; i < numParams; i++) { Assert.assertEquals(paramsFoundByDirectSolution[i].getSummary().getStandardDeviation(), sigmaEstimate[i].getSummary().getMean(), 8e-2); } }"
                    ],
                    [
                        0.9821717739105225,
                        "private DerivativeStructure evaluate(final DerivativeStructure t, final double t0, final double[] y) throws NumberIsTooLargeException { final double[] top = new double[nbPoints]; final double[] bottom = new double[nbPoints]; for (int i = 0; i < nbPoints; ++i) { bottom[i] = y[i]; for (int j = 1; j <= i; ++j) { bottom[i - j] = (bottom[i - j + 1] - bottom[i - j]) / (j * stepSize); } top[i] = bottom[0]; } final int order = t.getOrder(); final int parameters = t.getFreeParameters(); final double[] derivatives = t.getAllDerivatives(); final double dt0 = t.getValue() - t0; DerivativeStructure interpolation = new DerivativeStructure(parameters, order, 0.0); DerivativeStructure monomial = null; for (int i = 0; i < nbPoints; ++i) { if (i == 0) { monomial = new DerivativeStructure(parameters, order, 1.0); } else { derivatives[0] = dt0 - (i - 1) * stepSize; final DerivativeStructure deltaX = new DerivativeStructure(parameters, order, derivatives); monomial = monomial.multiply(deltaX); } interpolation = interpolation.add(monomial.multiply(top[i])); } return interpolation; }"
                    ],
                    [
                        0.9821483492851257,
                        "private static void doTestTransformComplex(final int n, final double tol, final DftNormalization normalization, final TransformType type) { final FastFourierTransformer fft; fft = new FastFourierTransformer(normalization); final Complex[] x = createComplexData(n); final Complex[] expected; final double s; if (type == TransformType.FORWARD) { expected = dft(x, -1); if (normalization == DftNormalization.STANDARD) { s = 1.0; } else { s = 1.0 / FastMath.sqrt(n); } } else { expected = dft(x, 1); if (normalization == DftNormalization.STANDARD) { s = 1.0 / n; } else { s = 1.0 / FastMath.sqrt(n); } } final Complex[] actual = fft.transform(x, type); for (int i = 0; i < n; i++) { final String msg; msg = String.format(\\\"%s, %s, %d, %d\\\", normalization, type, n, i); final double re = s * expected[i].getReal(); Assert.assertEquals(msg, re, actual[i].getReal(), tol * FastMath.abs(re)); final double im = s * expected[i].getImaginary(); Assert.assertEquals(msg, im, actual[i].getImaginary(), tol * FastMath.abs(re)); } }"
                    ],
                    [
                        0.982131838798523,
                        "private static BSPTree<Euclidean2D> verticesToTree(final double hyperplaneThickness, final Vector2D... vertices) { final int n = vertices.length; if (n == 0) { return new BSPTree<Euclidean2D>(Boolean.TRUE); } final Vertex[] vArray = new Vertex[n]; for (int i = 0; i < n; ++i) { vArray[i] = new Vertex(vertices[i]); } List<Edge> edges = new ArrayList<Edge>(); for (int i = 0; i < n; ++i) { final Vertex start = vArray[i]; final Vertex end = vArray[(i + 1) % n]; Line line = start.sharedLineWith(end); if (line == null) { line = new Line(start.getLocation(), end.getLocation()); } edges.add(new Edge(start, end, line)); for (final Vertex vertex : vArray) { if (vertex != start && vertex != end && FastMath.abs(line.getOffset(vertex.getLocation())) <= hyperplaneThickness) { vertex.bindWith(line); } } } final BSPTree<Euclidean2D> tree = new BSPTree<Euclidean2D>(); insertEdges(hyperplaneThickness, tree, edges); return tree; }"
                    ],
                    [
                        0.9814485311508179,
                        "@Override protected UnivariatePointValuePair doOptimize() { for (int i = 0; i < optimData.length; i++) { if (optimData[i] instanceof MaxEval) { optimData[i] = null; maxEvalIndex = i; continue; } if (optimData[i] instanceof SearchInterval) { optimData[i] = null; searchIntervalIndex = i; continue; } } if (maxEvalIndex == -1) { throw new MathIllegalStateException(); } if (searchIntervalIndex == -1) { throw new MathIllegalStateException(); } RuntimeException lastException = null; optima = new UnivariatePointValuePair[starts]; totalEvaluations = 0; final int maxEval = getMaxEvaluations(); final double min = getMin(); final double max = getMax(); final double startValue = getStartValue(); for (int i = 0; i < starts; i++) { try { optimData[maxEvalIndex] = new MaxEval(maxEval - totalEvaluations); final double s = (i == 0) ? startValue : min + generator.nextDouble() * (max - min); optimData[searchIntervalIndex] = new SearchInterval(min, max, s); optima[i] = optimizer.optimize(optimData); } catch (RuntimeException mue) { lastException = mue; optima[i] = null; } totalEvaluations += optimizer.getEvaluations(); } sortPairs(getGoalType()); if (optima[0] == null) { throw lastException; } return optima[0]; }"
                    ]
                ],
                "directly_related_tests": [
                    "org.apache.commons.math3.genetics.FixedElapsedTimeTest::testIsSatisfied"
                ],
                "generation": {
                    "1": {
                        "prompt": "You are an expert Java developer. Below is a buggy method from a large Java project. This method causes one or more test failures.\nYour task is to fix the bug in this method. Use the provided test failure(s) to guide your reasoning.\n\n---\n\n## Buggy Method\n\n```java\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        checkParameters();\n\n        // Indirect call to \"computeObjectiveValue\" in order to update the\n        // evaluations counter.\n        final MultivariateFunction evalFunc\n            = new MultivariateFunction() {\n                public double value(double[] point) {\n                    return computeObjectiveValue(point);\n                }\n            };\n\n        final boolean isMinim = getGoalType() == GoalType.MINIMIZE;\n        final Comparator<PointValuePair> comparator\n            = new Comparator<PointValuePair>() {\n            public int compare(final PointValuePair o1,\n                               final PointValuePair o2) {\n                final double v1 = o1.getValue();\n                final double v2 = o2.getValue();\n                return isMinim ? Double.compare(v1, v2) : Double.compare(v2, v1);\n            }\n        };\n\n        // Initialize search.\n        simplex.build(getStartPoint());\n        simplex.evaluate(evalFunc, comparator);\n\n        PointValuePair[] previous = null;\n        int iteration = 0;\n        final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n        while (true) {\n            if (iteration > 0) {\n                boolean converged = true;\n                for (int i = 0; i < simplex.getSize(); i++) {\n                    PointValuePair prev = previous[i];\n                    converged = converged &&\n                        checker.converged(iteration, prev, simplex.getPoint(i));\n                }\n                if (converged) {\n                    // We have found an optimum.\n                    return simplex.getPoint(0);\n                }\n            }\n\n            // We still need to search.\n            previous = simplex.getPoints();\n            simplex.iterate(evalFunc, comparator);\n\n\t\t\t++iteration;\n        }\n    }\n\n```\n---\n\n## Failing Test Case(s)\n\n\n---\n\n## Your Goal\n\nFix the buggy method. Return only the fixed Java method. Do not include explanations or other text.\n",
                        "patches": [
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        checkParameters();\n\n        // Indirect call to \"computeObjectiveValue\" in order to update the\n        // evaluations counter.\n        final MultivariateFunction evalFunc = new MultivariateFunction() {\n            public double value(double[] point) {\n                return computeObjectiveValue(point);\n            }\n        };\n\n        final boolean isMinim = getGoalType() == GoalType.MINIMIZE;\n        final Comparator<PointValuePair> comparator = new Comparator<PointValuePair>() {\n            public int compare(final PointValuePair o1, final PointValuePair o2) {\n                final double v1 = o1.getValue();\n                final double v2 = o2.getValue();\n                return isMinim ? Double.compare(v1, v2) : Double.compare(v2, v1);\n            }\n        };\n\n        // Initialize search.\n        simplex.build(getStartPoint());\n        simplex.evaluate(evalFunc, comparator);\n\n        PointValuePair[] previous = null;\n        int iteration = 0;\n        final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n        while (true) {\n            if (iteration > 0) {\n                boolean converged = true;\n                for (int i = 0; i < simplex.getSize(); i++) {\n                    PointValuePair prev = previous[i];\n                    converged = converged &&\n                        checker.converged(iteration, prev, simplex.getPoint(i));\n                }\n                if (converged) {\n                    // We have found an optimum.\n                    return simplex.getPoint(0);\n                }\n            }\n\n            // We still need to search.\n            previous = simplex.getPoints(); // Fix: Make a defensive copy of the current points\n            previous = Arrays.copyOf(previous, previous.length);\n            simplex.iterate(evalFunc, comparator);\n\n            ++iteration;\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        checkParameters();\n\n        // Indirect call to \"computeObjectiveValue\" in order to update the\n        // evaluations counter.\n        final MultivariateFunction evalFunc\n            = new MultivariateFunction() {\n                public double value(double[] point) {\n                    return computeObjectiveValue(point);\n                }\n            };\n\n        final boolean isMinim = getGoalType() == GoalType.MINIMIZE;\n        final Comparator<PointValuePair> comparator\n            = new Comparator<PointValuePair>() {\n            public int compare(final PointValuePair o1,\n                               final PointValuePair o2) {\n                final double v1 = o1.getValue();\n                final double v2 = o2.getValue();\n                return isMinim ? Double.compare(v1, v2) : Double.compare(v2, v1);\n            }\n        };\n\n        // Initialize search.\n        simplex.build(getStartPoint());\n        simplex.evaluate(evalFunc, comparator);\n\n        PointValuePair[] previous = null;\n        int iteration = 0;\n        final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n        while (true) {\n            if (iteration > 0) {\n                boolean converged = true;\n                for (int i = 0; i < simplex.getSize(); i++) {\n                    PointValuePair prev = previous[i];\n                    converged = converged &&\n                        checker.converged(iteration, prev, simplex.getPoint(i));\n                }\n                if (converged) {\n                    // We have found an optimum.\n                    return simplex.getPoint(0);\n                }\n            }\n\n            // We still need to search.\n            previous = simplex.getPoints().clone(); // Fix: Clone to avoid reference issues\n            simplex.iterate(evalFunc, comparator);\n\n            ++iteration;\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        checkParameters();\n\n        // Indirect call to \"computeObjectiveValue\" in order to update the\n        // evaluations counter.\n        final MultivariateFunction evalFunc\n            = new MultivariateFunction() {\n                public double value(double[] point) {\n                    return computeObjectiveValue(point);\n                }\n            };\n\n        final boolean isMinim = getGoalType() == GoalType.MINIMIZE;\n        final Comparator<PointValuePair> comparator\n            = new Comparator<PointValuePair>() {\n            public int compare(final PointValuePair o1,\n                               final PointValuePair o2) {\n                final double v1 = o1.getValue();\n                final double v2 = o2.getValue();\n                return isMinim ? Double.compare(v1, v2) : Double.compare(v2, v1);\n            }\n        };\n\n        // Initialize search.\n        simplex.build(getStartPoint());\n        simplex.evaluate(evalFunc, comparator);\n\n        PointValuePair[] previous = null;\n        int iteration = 0;\n        final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n        while (true) {\n            if (iteration > 0) {\n                boolean converged = true;\n                for (int i = 0; i < simplex.getSize(); i++) {\n                    PointValuePair prev = previous[i];\n                    converged = converged &&\n                        checker.converged(iteration, prev, simplex.getPoint(i));\n                }\n                if (converged) {\n                    // We have found an optimum.\n                    return simplex.getPoint(0);\n                }\n            }\n\n            // We still need to search.\n            previous = simplex.getPoints();\n            simplex.iterate(evalFunc, comparator);\n            previous = previous.clone();\n\n            ++iteration;\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        checkParameters();\n\n        // Indirect call to \"computeObjectiveValue\" in order to update the\n        // evaluations counter.\n        final MultivariateFunction evalFunc\n            = new MultivariateFunction() {\n                public double value(double[] point) {\n                    return computeObjectiveValue(point);\n                }\n            };\n\n        final boolean isMinim = getGoalType() == GoalType.MINIMIZE;\n        final Comparator<PointValuePair> comparator\n            = new Comparator<PointValuePair>() {\n            public int compare(final PointValuePair o1,\n                               final PointValuePair o2) {\n                final double v1 = o1.getValue();\n                final double v2 = o2.getValue();\n                return isMinim ? Double.compare(v1, v2) : Double.compare(v2, v1);\n            }\n        };\n\n        // Initialize search.\n        simplex.build(getStartPoint());\n        simplex.evaluate(evalFunc, comparator);\n\n        PointValuePair[] previous = null;\n        int iteration = 0;\n        final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n        while (true) {\n            if (iteration > 0) {\n                boolean converged = true;\n                for (int i = 0; i < simplex.getSize(); i++) {\n                    PointValuePair prev = previous[i];\n                    converged = converged &&\n                        checker.converged(iteration, prev, simplex.getPoint(i));\n                }\n                if (converged) {\n                    // We have found an optimum.\n                    return simplex.getPoint(0);\n                }\n            }\n\n            // We still need to search.\n            previous = simplex.getPoints();\n            simplex.iterate(evalFunc, comparator);\n\n\t\t\t++iteration;\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        checkParameters();\n\n        // Indirect call to \"computeObjectiveValue\" in order to update the\n        // evaluations counter.\n        final MultivariateFunction evalFunc\n            = new MultivariateFunction() {\n                public double value(double[] point) {\n                    return computeObjectiveValue(point);\n                }\n            };\n\n        final boolean isMinim = getGoalType() == GoalType.MINIMIZE;\n        final Comparator<PointValuePair> comparator\n            = new Comparator<PointValuePair>() {\n            public int compare(final PointValuePair o1,\n                               final PointValuePair o2) {\n                final double v1 = o1.getValue();\n                final double v2 = o2.getValue();\n                return isMinim ? Double.compare(v1, v2) : Double.compare(v2, v1);\n            }\n        };\n\n        // Initialize search.\n        simplex.build(getStartPoint());\n        simplex.evaluate(evalFunc, comparator);\n\n        PointValuePair[] previous = null;\n        int iteration = 0;\n        final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n        while (true) {\n            if (iteration > 0) {\n                boolean converged = true;\n                for (int i = 0; i < simplex.getSize(); i++) {\n                    PointValuePair prev = previous[i];\n                    converged = converged &&\n                        checker.converged(iteration, prev, simplex.getPoint(i));\n                }\n                if (converged) {\n                    // We have found an optimum.\n                    return simplex.getPoint(0);\n                }\n            }\n\n            // We still need to search.\n            previous = simplex.getPoints().clone();\n            simplex.iterate(evalFunc, comparator);\n\n            ++iteration;\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        checkParameters();\n\n        // Indirect call to \"computeObjectiveValue\" in order to update the\n        // evaluations counter.\n        final MultivariateFunction evalFunc\n            = new MultivariateFunction() {\n                public double value(double[] point) {\n                    return computeObjectiveValue(point);\n                }\n            };\n\n        final boolean isMinim = getGoalType() == GoalType.MINIMIZE;\n        final Comparator<PointValuePair> comparator\n            = new Comparator<PointValuePair>() {\n            public int compare(final PointValuePair o1,\n                               final PointValuePair o2) {\n                final double v1 = o1.getValue();\n                final double v2 = o2.getValue();\n                return isMinim ? Double.compare(v1, v2) : Double.compare(v2, v1);\n            }\n        };\n\n        // Initialize search.\n        simplex.build(getStartPoint());\n        simplex.evaluate(evalFunc, comparator);\n\n        PointValuePair[] previous = null;\n        int iteration = 0;\n        final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n        while (true) {\n            if (iteration > 0) {\n                boolean converged = true;\n                for (int i = 0; i < simplex.getSize(); i++) {\n                    PointValuePair prev = previous[i];\n                    converged = converged &&\n                        checker.converged(iteration, prev, simplex.getPoint(i));\n                }\n                if (converged) {\n                    // We have found an optimum.\n                    return simplex.getPoint(0);\n                }\n            }\n\n            // We still need to search.\n            previous = simplex.getPoints().clone(); // Use clone to keep a snapshot of the previous points\n            simplex.iterate(evalFunc, comparator);\n\n\t\t\t++iteration;\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        checkParameters();\n\n        // Indirect call to \"computeObjectiveValue\" in order to update the\n        // evaluations counter.\n        final MultivariateFunction evalFunc\n            = new MultivariateFunction() {\n                public double value(double[] point) {\n                    return computeObjectiveValue(point);\n                }\n            };\n\n        final boolean isMinim = getGoalType() == GoalType.MINIMIZE;\n        final Comparator<PointValuePair> comparator\n            = new Comparator<PointValuePair>() {\n            public int compare(final PointValuePair o1,\n                               final PointValuePair o2) {\n                final double v1 = o1.getValue();\n                final double v2 = o2.getValue();\n                return isMinim ? Double.compare(v1, v2) : Double.compare(v2, v1);\n            }\n        };\n\n        // Initialize search.\n        simplex.build(getStartPoint());\n        simplex.evaluate(evalFunc, comparator);\n\n        PointValuePair[] previous = null;\n        int iteration = 0;\n        final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n        while (true) {\n            if (iteration > 0) {\n                boolean converged = true;\n                for (int i = 0; i < simplex.getSize(); i++) {\n                    PointValuePair prev = previous[i];\n                    converged = converged &&\n                        checker.converged(iteration, prev, simplex.getPoint(i));\n                }\n                if (converged) {\n                    // We have found an optimum.\n                    return simplex.getPoint(0);\n                }\n            }\n\n            // We still need to search.\n            previous = simplex.getPoints().clone(); // Fix: clone array to avoid reference sharing\n            simplex.iterate(evalFunc, comparator);\n\n            ++iteration;\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        checkParameters();\n\n        // Indirect call to \"computeObjectiveValue\" in order to update the\n        // evaluations counter.\n        final MultivariateFunction evalFunc\n            = new MultivariateFunction() {\n                public double value(double[] point) {\n                    return computeObjectiveValue(point);\n                }\n            };\n\n        final boolean isMinim = getGoalType() == GoalType.MINIMIZE;\n        final Comparator<PointValuePair> comparator\n            = new Comparator<PointValuePair>() {\n            public int compare(final PointValuePair o1,\n                               final PointValuePair o2) {\n                final double v1 = o1.getValue();\n                final double v2 = o2.getValue();\n                return isMinim ? Double.compare(v1, v2) : Double.compare(v2, v1);\n            }\n        };\n\n        // Initialize search.\n        simplex.build(getStartPoint());\n        simplex.evaluate(evalFunc, comparator);\n\n        PointValuePair[] previous = null;\n        int iteration = 0;\n        final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n        while (true) {\n            if (iteration > 0) {\n                boolean converged = true;\n                for (int i = 0; i < simplex.getSize(); i++) {\n                    PointValuePair prev = previous[i];\n                    converged = converged &&\n                        checker.converged(iteration, prev, simplex.getPoint(i));\n                }\n                if (converged) {\n                    // We have found an optimum.\n                    return simplex.getPoint(0);\n                }\n            }\n\n            // We still need to search.\n            previous = simplex.getPoints().clone(); // Fix: clone the array\n            simplex.iterate(evalFunc, comparator);\n\n            ++iteration;\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        checkParameters();\n\n        // Indirect call to \"computeObjectiveValue\" in order to update the\n        // evaluations counter.\n        final MultivariateFunction evalFunc\n            = new MultivariateFunction() {\n                public double value(double[] point) {\n                    return computeObjectiveValue(point);\n                }\n            };\n\n        final boolean isMinim = getGoalType() == GoalType.MINIMIZE;\n        final Comparator<PointValuePair> comparator\n            = new Comparator<PointValuePair>() {\n            public int compare(final PointValuePair o1,\n                               final PointValuePair o2) {\n                final double v1 = o1.getValue();\n                final double v2 = o2.getValue();\n                return isMinim ? Double.compare(v1, v2) : Double.compare(v2, v1);\n            }\n        };\n\n        // Initialize search.\n        simplex.build(getStartPoint());\n        simplex.evaluate(evalFunc, comparator);\n\n        PointValuePair[] previous = simplex.getPoints();\n        int iteration = 0;\n        final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n        while (true) {\n            if (iteration > 0) {\n                boolean converged = true;\n                for (int i = 0; i < simplex.getSize(); i++) {\n                    PointValuePair prev = previous[i];\n                    converged = converged &&\n                        checker.converged(iteration, prev, simplex.getPoint(i));\n                }\n                if (converged) {\n                    // We have found an optimum.\n                    return simplex.getPoint(0);\n                }\n            }\n\n            // We still need to search.\n            previous = simplex.getPoints().clone();\n            simplex.iterate(evalFunc, comparator);\n\n            ++iteration;\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        checkParameters();\n\n        // Indirect call to \"computeObjectiveValue\" in order to update the\n        // evaluations counter.\n        final MultivariateFunction evalFunc\n            = new MultivariateFunction() {\n                public double value(double[] point) {\n                    return computeObjectiveValue(point);\n                }\n            };\n\n        final boolean isMinim = getGoalType() == GoalType.MINIMIZE;\n        final Comparator<PointValuePair> comparator\n            = new Comparator<PointValuePair>() {\n            public int compare(final PointValuePair o1,\n                               final PointValuePair o2) {\n                final double v1 = o1.getValue();\n                final double v2 = o2.getValue();\n                return isMinim ? Double.compare(v1, v2) : Double.compare(v2, v1);\n            }\n        };\n\n        // Initialize search.\n        simplex.build(getStartPoint());\n        simplex.evaluate(evalFunc, comparator);\n\n        PointValuePair[] previous = null;\n        int iteration = 0;\n        final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n        while (true) {\n            if (iteration > 0) {\n                boolean converged = true;\n                for (int i = 0; i < simplex.getSize(); i++) {\n                    PointValuePair prev = previous[i];\n                    converged = converged &&\n                        checker.converged(iteration, prev, simplex.getPoint(i));\n                }\n                if (converged) {\n                    // We have found an optimum.\n                    return simplex.getPoint(0);\n                }\n            }\n\n            // We still need to search.\n            previous = simplex.getPoints().clone(); // Fix: clone the array to prevent aliasing\n            simplex.iterate(evalFunc, comparator);\n\n            ++iteration;\n        }\n    }\n"
                        ]
                    },
                    "2": {
                        "prompt": "You are an expert Java developer. Below is a buggy method from a large Java project. This method causes one or more test failures.\nYour task is to fix the bug in this method. Use the provided test failure(s) and relevant context to guide your reasoning.\n\n---\n\n## Buggy Method\n\n```java\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointValuePair doOptimize() {\n        checkParameters();\n\n        // Indirect call to \"computeObjectiveValue\" in order to update the\n        // evaluations counter.\n        final MultivariateFunction evalFunc\n            = new MultivariateFunction() {\n                public double value(double[] point) {\n                    return computeObjectiveValue(point);\n                }\n            };\n\n        final boolean isMinim = getGoalType() == GoalType.MINIMIZE;\n        final Comparator<PointValuePair> comparator\n            = new Comparator<PointValuePair>() {\n            public int compare(final PointValuePair o1,\n                               final PointValuePair o2) {\n                final double v1 = o1.getValue();\n                final double v2 = o2.getValue();\n                return isMinim ? Double.compare(v1, v2) : Double.compare(v2, v1);\n            }\n        };\n\n        // Initialize search.\n        simplex.build(getStartPoint());\n        simplex.evaluate(evalFunc, comparator);\n\n        PointValuePair[] previous = null;\n        int iteration = 0;\n        final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n        while (true) {\n            if (iteration > 0) {\n                boolean converged = true;\n                for (int i = 0; i < simplex.getSize(); i++) {\n                    PointValuePair prev = previous[i];\n                    converged = converged &&\n                        checker.converged(iteration, prev, simplex.getPoint(i));\n                }\n                if (converged) {\n                    // We have found an optimum.\n                    return simplex.getPoint(0);\n                }\n            }\n\n            // We still need to search.\n            previous = simplex.getPoints();\n            simplex.iterate(evalFunc, comparator);\n\n\t\t\t++iteration;\n        }\n    }\n\n```\n---\n\n## Failing Test Case(s)\n\n\n\n## \ud83d\udca1 Context from Project\n\n            \n```java\n@Override protected PointValuePair doOptimize() { if (simplex == null) { throw new NullArgumentException(); } final MultivariateFunction evalFunc = new MultivariateFunction() { public double value(double[] point) { return computeObjectiveValue(point); } }; final boolean isMinim = getGoalType() == GoalType.MINIMIZE; final Comparator<PointValuePair> comparator = new Comparator<PointValuePair>() { public int compare(final PointValuePair o1, final PointValuePair o2) { final double v1 = o1.getValue(); final double v2 = o2.getValue(); return isMinim ? Double.compare(v1, v2) : Double.compare(v2, v1); } }; simplex.build(getStartPoint()); simplex.evaluate(evalFunc, comparator); PointValuePair[] previous = null; int iteration = 0; final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker(); while (true) { if (iteration > 0) { boolean converged = true; for (int i = 0; i < simplex.getSize(); i++) { PointValuePair prev = previous[i]; converged = converged && checker.converged(iteration, prev, simplex.getPoint(i)); } if (converged) { return simplex.getPoint(0); } } previous = simplex.getPoints(); simplex.iterate(evalFunc, comparator); ++iteration; } }\n```\n\n            \n```java\n@Override protected PointValuePair doOptimize() { final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker(); final double[] point = getStartPoint(); final GoalType goal = getGoalType(); final int n = point.length; double[] r = computeObjectiveGradient(point); if (goal == GoalType.MINIMIZE) { for (int i = 0; i < n; i++) { r[i] = -r[i]; } } double[] steepestDescent = preconditioner.precondition(point, r); double[] searchDirection = steepestDescent.clone(); double delta = 0; for (int i = 0; i < n; ++i) { delta += r[i] * searchDirection[i]; } PointValuePair current = null; int iter = 0; int maxEval = getMaxEvaluations(); while (true) { ++iter; final double objective = computeObjectiveValue(point); PointValuePair previous = current; current = new PointValuePair(point, objective); if (previous != null) { if (checker.converged(iter, previous, current)) { return current; } } final UnivariateFunction lsf = new LineSearchFunction(point, searchDirection); final double uB = findUpperBound(lsf, 0, initialStep); final double step = solver.solve(maxEval, lsf, 0, uB, 1e-15); maxEval -= solver.getEvaluations(); for (int i = 0; i < point.length; ++i) { point[i] += step * searchDirection[i]; } r = computeObjectiveGradient(point); if (goal == GoalType.MINIMIZE) { for (int i = 0; i < n; ++i) { r[i] = -r[i]; } } final double deltaOld = delta; final double[] newSteepestDescent = preconditioner.precondition(point, r); delta = 0; for (int i = 0; i < n; ++i) { delta += r[i] * newSteepestDescent[i]; } final double beta; switch(updateFormula) { case FLETCHER_REEVES: beta = delta / deltaOld; break; case POLAK_RIBIERE: double deltaMid = 0; for (int i = 0; i < r.length; ++i) { deltaMid += r[i] * steepestDescent[i]; } beta = (delta - deltaMid) / deltaOld; break; default: throw new MathInternalError(); } steepestDescent = newSteepestDescent; if (iter % n == 0 || beta < 0) { searchDirection = steepestDescent.clone(); } else { for (int i = 0; i < n; ++i) { searchDirection[i] = steepestDescent[i] + beta * searchDirection[i]; } } } }\n```\n\n            \n```java\n@Test public void testParametersErrorMonteCarloObservations() { final double yError = 15; final double slope = 123.456; final double offset = -98.765; final RandomStraightLinePointGenerator lineGenerator = new RandomStraightLinePointGenerator(slope, offset, yError, -1e3, 1e4, 138577L); final int numObs = 100; final int numParams = 2; final SummaryStatistics[] paramsFoundByDirectSolution = new SummaryStatistics[numParams]; final SummaryStatistics[] sigmaEstimate = new SummaryStatistics[numParams]; for (int i = 0; i < numParams; i++) { paramsFoundByDirectSolution[i] = new SummaryStatistics(); sigmaEstimate[i] = new SummaryStatistics(); } final AbstractLeastSquaresOptimizer optim = new DummyOptimizer(); final double[] init = { slope, offset }; final int mcRepeat = MONTE_CARLO_RUNS; int mcCount = 0; while (mcCount < mcRepeat) { final Point2D.Double[] obs = lineGenerator.generate(numObs); final StraightLineProblem problem = new StraightLineProblem(yError); for (int i = 0; i < numObs; i++) { final Point2D.Double p = obs[i]; problem.addPoint(p.x, p.y); } final double[] regress = problem.solve(); final PointVectorValuePair optimum = optim.optimize(new MaxEval(Integer.MAX_VALUE), problem.getModelFunction(), problem.getModelFunctionJacobian(), new Target(problem.target()), new Weight(problem.weight()), new InitialGuess(init)); final double[] sigma = optim.computeSigma(optimum.getPoint(), 1e-14); for (int i = 0; i < numParams; i++) { paramsFoundByDirectSolution[i].addValue(regress[i]); sigmaEstimate[i].addValue(sigma[i]); } ++mcCount; } final String line = \\\"--------------------------------------------------------------\\\"; System.out.println(\\\" True value Mean Std deviation\\\"); for (int i = 0; i < numParams; i++) { System.out.println(line); System.out.println(\\\"Parameter #\\\" + i); StatisticalSummary s = paramsFoundByDirectSolution[i].getSummary(); System.out.printf(\\\" %+.6e %+.6e %+.6e\\n\\\", init[i], s.getMean(), s.getStandardDeviation()); s = sigmaEstimate[i].getSummary(); System.out.printf(\\\"sigma: %+.6e (%+.6e)\\n\\\", s.getMean(), s.getStandardDeviation()); } System.out.println(line); for (int i = 0; i < numParams; i++) { Assert.assertEquals(paramsFoundByDirectSolution[i].getSummary().getStandardDeviation(), sigmaEstimate[i].getSummary().getMean(), 8e-2); } }\n```\n\n            \n```java\npublic void handleStep(final StepInterpolator interpolator, final boolean isLast) throws MaxCountExceededException { if (lastState == null) { firstTime = interpolator.getPreviousTime(); lastTime = interpolator.getPreviousTime(); interpolator.setInterpolatedTime(lastTime); lastState = interpolator.getInterpolatedState().clone(); lastDerivatives = interpolator.getInterpolatedDerivatives().clone(); forward = interpolator.getCurrentTime() >= lastTime; if (!forward) { h = -h; } } double nextTime = (mode == StepNormalizerMode.INCREMENT) ? lastTime + h : (FastMath.floor(lastTime / h) + 1) * h; if (mode == StepNormalizerMode.MULTIPLES && Precision.equals(nextTime, lastTime, 1)) { nextTime += h; } boolean nextInStep = isNextInStep(nextTime, interpolator); while (nextInStep) { doNormalizedStep(false); storeStep(interpolator, nextTime); nextTime += h; nextInStep = isNextInStep(nextTime, interpolator); } if (isLast) { boolean addLast = bounds.lastIncluded() && lastTime != interpolator.getCurrentTime(); doNormalizedStep(!addLast); if (addLast) { storeStep(interpolator, interpolator.getCurrentTime()); doNormalizedStep(true); } } }\n```\n\n            \n```java\npublic void saundersTest(final int n, final boolean goodb, final boolean precon, final double shift, final double pertbn) { final RealLinearOperator a = new RealLinearOperator() { @Override public RealVector operate(final RealVector x) { if (x.getDimension() != n) { throw new DimensionMismatchException(x.getDimension(), n); } final double[] y = new double[n]; for (int i = 0; i < n; i++) { y[i] = (i + 1) * 1.1 / n * x.getEntry(i); } return new ArrayRealVector(y, false); } @Override public int getRowDimension() { return n; } @Override public int getColumnDimension() { return n; } }; final double shiftm = shift; final double pertm = FastMath.abs(pertbn); final RealLinearOperator minv; if (precon) { minv = new RealLinearOperator() { @Override public int getRowDimension() { return n; } @Override public int getColumnDimension() { return n; } @Override public RealVector operate(final RealVector x) { if (x.getDimension() != n) { throw new DimensionMismatchException(x.getDimension(), n); } final double[] y = new double[n]; for (int i = 0; i < n; i++) { double d = (i + 1) * 1.1 / n; d = FastMath.abs(d - shiftm); if (i % 10 == 0) { d += pertm; } y[i] = x.getEntry(i) / d; } return new ArrayRealVector(y, false); } }; } else { minv = null; } final RealVector xtrue = new ArrayRealVector(n); for (int i = 0; i < n; i++) { xtrue.setEntry(i, n - i); } final RealVector b = a.operate(xtrue); b.combineToSelf(1.0, -shift, xtrue); final SymmLQ solver = new SymmLQ(2 * n, 1E-12, true); final RealVector x = solver.solve(a, minv, b, goodb, shift); final RealVector y = a.operate(x); final RealVector r1 = new ArrayRealVector(n); for (int i = 0; i < n; i++) { final double bi = b.getEntry(i); final double yi = y.getEntry(i); final double xi = x.getEntry(i); r1.setEntry(i, bi - yi + shift * xi); } final double enorm = x.subtract(xtrue).getNorm() / xtrue.getNorm(); final double etol = 1E-5; Assert.assertTrue(\\\"enorm=\\\" + enorm + \\\", \\\" + solver.getIterationManager().getIterations(), enorm <= etol); }\n```\n\n---\n\n## Your Goal\n\nFix the buggy method. Return only the fixed Java method. Do not include explanations or other text.\n",
                        "patches": [
                            "\n@Override\nprotected PointValuePair doOptimize() {\n    checkParameters();\n\n    final MultivariateFunction evalFunc = new MultivariateFunction() {\n        public double value(double[] point) {\n            return computeObjectiveValue(point);\n        }\n    };\n\n    final boolean isMinim = getGoalType() == GoalType.MINIMIZE;\n    final Comparator<PointValuePair> comparator = new Comparator<PointValuePair>() {\n        public int compare(final PointValuePair o1, final PointValuePair o2) {\n            final double v1 = o1.getValue();\n            final double v2 = o2.getValue();\n            return isMinim ? Double.compare(v1, v2) : Double.compare(v2, v1);\n        }\n    };\n\n    simplex.build(getStartPoint());\n    simplex.evaluate(evalFunc, comparator);\n\n    PointValuePair[] previous = null;\n    int iteration = 0;\n    final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n    while (true) {\n        if (iteration > 0) {\n            boolean converged = true;\n            for (int i = 0; i < simplex.getSize(); i++) {\n                PointValuePair prev = previous[i];\n                converged = converged &&\n                        checker.converged(iteration, prev, simplex.getPoint(i));\n            }\n            if (converged) {\n                return simplex.getPoint(0);\n            }\n        }\n\n        previous = simplex.getPoints();\n        simplex.iterate(evalFunc, comparator);\n\n        ++iteration;\n    }\n}\n",
                            "\n@Override\nprotected PointValuePair doOptimize() {\n    checkParameters();\n\n    // Indirect call to \"computeObjectiveValue\" in order to update the\n    // evaluations counter.\n    final MultivariateFunction evalFunc = new MultivariateFunction() {\n        public double value(double[] point) {\n            return computeObjectiveValue(point);\n        }\n    };\n\n    final boolean isMinim = getGoalType() == GoalType.MINIMIZE;\n    final Comparator<PointValuePair> comparator = new Comparator<PointValuePair>() {\n        public int compare(final PointValuePair o1, final PointValuePair o2) {\n            final double v1 = o1.getValue();\n            final double v2 = o2.getValue();\n            return isMinim ? Double.compare(v1, v2) : Double.compare(v2, v1);\n        }\n    };\n\n    // Initialize search.\n    simplex.build(getStartPoint());\n    simplex.evaluate(evalFunc, comparator);\n\n    PointValuePair[] previous = null;\n    int iteration = 0;\n    final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n    while (true) {\n        if (iteration > 0) {\n            boolean converged = true;\n            for (int i = 0; i < simplex.getSize(); i++) {\n                PointValuePair prev = previous[i];\n                converged = converged && checker.converged(iteration, prev, simplex.getPoints()[i]);\n            }\n            if (converged) {\n                // We have found an optimum.\n                return simplex.getPoints()[0];\n            }\n        }\n\n        // We still need to search.\n        previous = simplex.getPoints().clone();\n        simplex.iterate(evalFunc, comparator);\n\n        ++iteration;\n    }\n}\n",
                            "\n@Override\nprotected PointValuePair doOptimize() {\n    checkParameters();\n\n    // Indirect call to \"computeObjectiveValue\" in order to update the\n    // evaluations counter.\n    final MultivariateFunction evalFunc = new MultivariateFunction() {\n        public double value(double[] point) {\n            return computeObjectiveValue(point);\n        }\n    };\n\n    final boolean isMinim = getGoalType() == GoalType.MINIMIZE;\n    final Comparator<PointValuePair> comparator = new Comparator<PointValuePair>() {\n        public int compare(final PointValuePair o1, final PointValuePair o2) {\n            final double v1 = o1.getValue();\n            final double v2 = o2.getValue();\n            return isMinim ? Double.compare(v1, v2) : Double.compare(v2, v1);\n        }\n    };\n\n    // Initialize search.\n    simplex.build(getStartPoint());\n    simplex.evaluate(evalFunc, comparator);\n\n    int iteration = 0;\n    PointValuePair[] previous;\n\n    final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n    while (true) {\n        previous = simplex.getPoints();\n\n        simplex.iterate(evalFunc, comparator);\n\n        ++iteration;\n\n        boolean converged = true;\n        for (int i = 0; i < simplex.getSize(); i++) {\n            PointValuePair prev = previous[i];\n            converged = converged && checker.converged(iteration, prev, simplex.getPoint(i));\n        }\n\n        if (converged) {\n            // We have found an optimum.\n            return simplex.getPoint(0);\n        }\n    }\n}\n",
                            "\n@Override\nprotected PointValuePair doOptimize() {\n    checkParameters();\n\n    // Indirect call to \"computeObjectiveValue\" in order to update the\n    // evaluations counter.\n    final MultivariateFunction evalFunc\n        = new MultivariateFunction() {\n            public double value(double[] point) {\n                return computeObjectiveValue(point);\n            }\n        };\n\n    final boolean isMinim = getGoalType() == GoalType.MINIMIZE;\n    final Comparator<PointValuePair> comparator\n        = new Comparator<PointValuePair>() {\n        public int compare(final PointValuePair o1,\n                           final PointValuePair o2) {\n            final double v1 = o1.getValue();\n            final double v2 = o2.getValue();\n            return isMinim ? Double.compare(v1, v2) : Double.compare(v2, v1);\n        }\n    };\n\n    // Initialize search.\n    simplex.build(getStartPoint());\n    simplex.evaluate(evalFunc, comparator);\n\n    int iteration = 0;\n    final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n    while (true) {\n        PointValuePair[] previous = simplex.getPoints();\n\n        if (iteration > 0) {\n            boolean converged = true;\n            for (int i = 0; i < simplex.getSize(); i++) {\n                PointValuePair prev = previous[i];\n                converged = converged && \n                    checker.converged(iteration, prev, simplex.getPoint(i));\n            }\n            if (converged) {\n                // We have found an optimum.\n                return simplex.getPoint(0);\n            }\n        }\n\n        // We still need to search.\n        simplex.iterate(evalFunc, comparator);\n\n        ++iteration;\n    }\n}\n",
                            "\n@Override\nprotected PointValuePair doOptimize() {\n    checkParameters();\n\n    // Indirect call to \"computeObjectiveValue\" in order to update the\n    // evaluations counter.\n    final MultivariateFunction evalFunc = new MultivariateFunction() {\n        public double value(double[] point) {\n            return computeObjectiveValue(point);\n        }\n    };\n\n    final boolean isMinim = getGoalType() == GoalType.MINIMIZE;\n    final Comparator<PointValuePair> comparator = new Comparator<PointValuePair>() {\n        public int compare(final PointValuePair o1, final PointValuePair o2) {\n            final double v1 = o1.getValue();\n            final double v2 = o2.getValue();\n            return isMinim ? Double.compare(v1, v2) : Double.compare(v2, v1);\n        }\n    };\n\n    // Initialize search.\n    simplex.build(getStartPoint());\n    simplex.evaluate(evalFunc, comparator);\n\n    PointValuePair[] previous;\n    int iteration = 0;\n    final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n    while (true) {\n        previous = simplex.getPoints();\n        \n        if (iteration > 0) {\n            boolean converged = true;\n            for (int i = 0; i < simplex.getSize(); i++) {\n                PointValuePair prev = previous[i];\n                converged = converged && checker.converged(iteration, prev, simplex.getPoint(i));\n            }\n            if (converged) {\n                // We have found an optimum.\n                return simplex.getPoint(0);\n            }\n        }\n\n        // We still need to search.\n        simplex.iterate(evalFunc, comparator);\n\n        ++iteration;\n    }\n}\n",
                            "\n@Override\nprotected PointValuePair doOptimize() {\n    checkParameters();\n\n    final MultivariateFunction evalFunc = new MultivariateFunction() {\n        public double value(double[] point) {\n            return computeObjectiveValue(point);\n        }\n    };\n\n    final boolean isMinim = getGoalType() == GoalType.MINIMIZE;\n    final Comparator<PointValuePair> comparator = new Comparator<PointValuePair>() {\n        public int compare(final PointValuePair o1, final PointValuePair o2) {\n            final double v1 = o1.getValue();\n            final double v2 = o2.getValue();\n            return isMinim ? Double.compare(v1, v2) : Double.compare(v2, v1);\n        }\n    };\n\n    simplex.build(getStartPoint());\n    simplex.evaluate(evalFunc, comparator);\n\n    PointValuePair[] previous = null;\n    int iteration = 0;\n    final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n    while (true) {\n        if (iteration > 0) {\n            boolean converged = true;\n            for (int i = 0; i < simplex.getSize(); i++) {\n                PointValuePair prev = previous[i];\n                converged = converged && checker.converged(iteration, prev, simplex.getPoint(i));\n            }\n            if (converged) {\n                return simplex.getPoint(0);\n            }\n        }\n\n        previous = simplex.getPoints();\n        simplex.iterate(evalFunc, comparator);\n\n        ++iteration;\n    }\n}\n",
                            "\n@Override\nprotected PointValuePair doOptimize() {\n    checkParameters();\n\n    // Indirect call to \"computeObjectiveValue\" in order to update the\n    // evaluations counter.\n    final MultivariateFunction evalFunc\n        = new MultivariateFunction() {\n            public double value(double[] point) {\n                return computeObjectiveValue(point);\n            }\n        };\n\n    final boolean isMinim = getGoalType() == GoalType.MINIMIZE;\n    final Comparator<PointValuePair> comparator\n        = new Comparator<PointValuePair>() {\n        public int compare(final PointValuePair o1,\n                           final PointValuePair o2) {\n            final double v1 = o1.getValue();\n            final double v2 = o2.getValue();\n            return isMinim ? Double.compare(v1, v2) : Double.compare(v2, v1);\n        }\n    };\n\n    // Initialize search.\n    simplex.build(getStartPoint());\n    simplex.evaluate(evalFunc, comparator);\n\n    PointValuePair[] previous = null;\n    int iteration = 0;\n    final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n    while (true) {\n        if (iteration > 0) {\n            boolean converged = true;\n            for (int i = 0; i < simplex.getSize(); i++) {\n                PointValuePair prev = previous[i];\n                converged = converged &&\n                    checker.converged(iteration, prev, simplex.getPoint(i));\n            }\n            if (converged) {\n                // We have found an optimum.\n                return simplex.getPoint(0);\n            }\n        }\n\n        // We still need to search.\n        previous = simplex.getPoints().clone(); // Fix: Copy the array to ensure previous reflects before iteration starts\n        simplex.iterate(evalFunc, comparator);\n\n        ++iteration;\n    }\n}\n",
                            "\n@Override\nprotected PointValuePair doOptimize() {\n    checkParameters();\n\n    // Indirect call to \"computeObjectiveValue\" in order to update the\n    // evaluations counter.\n    final MultivariateFunction evalFunc\n        = new MultivariateFunction() {\n            public double value(double[] point) {\n                return computeObjectiveValue(point);\n            }\n        };\n\n    final boolean isMinim = getGoalType() == GoalType.MINIMIZE;\n    final Comparator<PointValuePair> comparator\n        = new Comparator<PointValuePair>() {\n        public int compare(final PointValuePair o1,\n                           final PointValuePair o2) {\n            final double v1 = o1.getValue();\n            final double v2 = o2.getValue();\n            return isMinim ? Double.compare(v1, v2) : Double.compare(v2, v1);\n        }\n    };\n\n    // Initialize search.\n    simplex.build(getStartPoint());\n    simplex.evaluate(evalFunc, comparator);\n\n    PointValuePair[] previous = null;\n    int iteration = 0;\n    final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n    while (true) {\n        if (iteration > 0) {\n            boolean converged = true;\n            for (int i = 0; i < simplex.getSize(); i++) {\n                PointValuePair prev = previous[i];\n                converged = converged &&\n                    checker.converged(iteration, prev, simplex.getPoint(i));\n            }\n            if (converged) {\n                // We have found an optimum.\n                return simplex.getPoint(0);\n            }\n        }\n\n        // We still need to search.\n        previous = simplex.getPoints();\n        simplex.iterate(evalFunc, comparator);\n\n        // Sort the simplex points to make sure the first point is the best\n        Arrays.sort(previous, comparator);\n\n        ++iteration;\n    }\n}\n",
                            "\n@Override\nprotected PointValuePair doOptimize() {\n    checkParameters();\n\n    final MultivariateFunction evalFunc\n        = new MultivariateFunction() {\n            public double value(double[] point) {\n                return computeObjectiveValue(point);\n            }\n        };\n\n    final boolean isMinim = getGoalType() == GoalType.MINIMIZE;\n    final Comparator<PointValuePair> comparator\n        = new Comparator<PointValuePair>() {\n        public int compare(final PointValuePair o1,\n                           final PointValuePair o2) {\n            final double v1 = o1.getValue();\n            final double v2 = o2.getValue();\n            return isMinim ? Double.compare(v1, v2) : Double.compare(v2, v1);\n        }\n    };\n\n    simplex.build(getStartPoint());\n    simplex.evaluate(evalFunc, comparator);\n\n    PointValuePair[] previous = simplex.getPoints();\n    int iteration = 0;\n    final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n    while (true) {\n        boolean converged = true;\n        for (int i = 0; i < simplex.getSize(); i++) {\n            PointValuePair prev = previous[i];\n            converged = converged &&\n                checker.converged(iteration, prev, simplex.getPoint(i));\n        }\n        if (converged) {\n            return simplex.getPoint(0);\n        }\n\n        previous = simplex.getPoints();\n        simplex.iterate(evalFunc, comparator);\n        ++iteration;\n    }\n}\n",
                            "\n@Override\nprotected PointValuePair doOptimize() {\n    checkParameters();\n\n    final MultivariateFunction evalFunc = new MultivariateFunction() {\n        public double value(double[] point) {\n            return computeObjectiveValue(point);\n        }\n    };\n\n    final boolean isMinim = getGoalType() == GoalType.MINIMIZE;\n    final Comparator<PointValuePair> comparator = new Comparator<PointValuePair>() {\n        public int compare(final PointValuePair o1, final PointValuePair o2) {\n            final double v1 = o1.getValue();\n            final double v2 = o2.getValue();\n            return isMinim ? Double.compare(v1, v2) : Double.compare(v2, v1);\n        }\n    };\n\n    // Initialize search.\n    simplex.build(getStartPoint());\n    simplex.evaluate(evalFunc, comparator);\n\n    PointValuePair[] previous = null;\n    int iteration = 0;\n    final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n    while (true) {\n        if (iteration > 0) {\n            boolean converged = true;\n            PointValuePair[] current = simplex.getPoints();\n            for (int i = 0; i < simplex.getSize(); i++) {\n                PointValuePair prev = previous[i];\n                PointValuePair curr = current[i];\n                converged = converged && checker.converged(iteration, prev, curr);\n            }\n            if (converged) {\n                return current[0];\n            }\n        }\n\n        previous = simplex.getPoints();\n        simplex.iterate(evalFunc, comparator);\n\n        ++iteration;\n    }\n}\n"
                        ]
                    }
                }
            },
            {
                "path": "src/main/java/org/apache/commons/math3/optim/nonlinear/vector/jacobian/GaussNewtonOptimizer.java",
                "start_loc": 79,
                "end_loc": 169,
                "buggy_function": "    @Override\n    public PointVectorValuePair doOptimize() {\n        checkParameters();\n\n        final ConvergenceChecker<PointVectorValuePair> checker\n            = getConvergenceChecker();\n\n        // Computation will be useless without a checker (see \"for-loop\").\n        if (checker == null) {\n            throw new NullArgumentException();\n        }\n\n        final double[] targetValues = getTarget();\n        final int nR = targetValues.length; // Number of observed data.\n\n        final RealMatrix weightMatrix = getWeight();\n        // Diagonal of the weight matrix.\n        final double[] residualsWeights = new double[nR];\n        for (int i = 0; i < nR; i++) {\n            residualsWeights[i] = weightMatrix.getEntry(i, i);\n        }\n\n        final double[] currentPoint = getStartPoint();\n        final int nC = currentPoint.length;\n\n        // iterate until convergence is reached\n        PointVectorValuePair current = null;\n        int iter = 0;\n        for (boolean converged = false; !converged;) {\n            ++iter;\n\n            // evaluate the objective function and its jacobian\n            PointVectorValuePair previous = current;\n            // Value of the objective function at \"currentPoint\".\n            final double[] currentObjective = computeObjectiveValue(currentPoint);\n            final double[] currentResiduals = computeResiduals(currentObjective);\n            final RealMatrix weightedJacobian = computeWeightedJacobian(currentPoint);\n            current = new PointVectorValuePair(currentPoint, currentObjective);\n\n            // build the linear problem\n            final double[]   b = new double[nC];\n            final double[][] a = new double[nC][nC];\n            for (int i = 0; i < nR; ++i) {\n\n                final double[] grad   = weightedJacobian.getRow(i);\n                final double weight   = residualsWeights[i];\n                final double residual = currentResiduals[i];\n\n                // compute the normal equation\n                final double wr = weight * residual;\n                for (int j = 0; j < nC; ++j) {\n                    b[j] += wr * grad[j];\n                }\n\n                // build the contribution matrix for measurement i\n                for (int k = 0; k < nC; ++k) {\n                    double[] ak = a[k];\n                    double wgk = weight * grad[k];\n                    for (int l = 0; l < nC; ++l) {\n                        ak[l] += wgk * grad[l];\n                    }\n                }\n            }\n\n            try {\n                // solve the linearized least squares problem\n                RealMatrix mA = new BlockRealMatrix(a);\n                DecompositionSolver solver = useLU ?\n                        new LUDecomposition(mA).getSolver() :\n                        new QRDecomposition(mA).getSolver();\n                final double[] dX = solver.solve(new ArrayRealVector(b, false)).toArray();\n                // update the estimated parameters\n                for (int i = 0; i < nC; ++i) {\n                    currentPoint[i] += dX[i];\n                }\n            } catch (SingularMatrixException e) {\n                throw new ConvergenceException(LocalizedFormats.UNABLE_TO_SOLVE_SINGULAR_PROBLEM);\n            }\n\n            // Check convergence.\n            if (previous != null) {\n                converged = checker.converged(iter, previous, current);\n                if (converged) {\n                    setCost(computeCost(currentResiduals));\n                    return current;\n                }\n            }\n        }\n        // Must never happen.\n        throw new MathInternalError();\n    }\n",
                "fixed_function": "    @Override\n    public PointVectorValuePair doOptimize() {\n        checkParameters();\n\n        final ConvergenceChecker<PointVectorValuePair> checker\n            = getConvergenceChecker();\n\n        // Computation will be useless without a checker (see \"for-loop\").\n        if (checker == null) {\n            throw new NullArgumentException();\n        }\n\n        final double[] targetValues = getTarget();\n        final int nR = targetValues.length; // Number of observed data.\n\n        final RealMatrix weightMatrix = getWeight();\n        // Diagonal of the weight matrix.\n        final double[] residualsWeights = new double[nR];\n        for (int i = 0; i < nR; i++) {\n            residualsWeights[i] = weightMatrix.getEntry(i, i);\n        }\n\n        final double[] currentPoint = getStartPoint();\n        final int nC = currentPoint.length;\n\n        // iterate until convergence is reached\n        PointVectorValuePair current = null;\n        for (boolean converged = false; !converged;) {\n            incrementIterationCount();\n\n            // evaluate the objective function and its jacobian\n            PointVectorValuePair previous = current;\n            // Value of the objective function at \"currentPoint\".\n            final double[] currentObjective = computeObjectiveValue(currentPoint);\n            final double[] currentResiduals = computeResiduals(currentObjective);\n            final RealMatrix weightedJacobian = computeWeightedJacobian(currentPoint);\n            current = new PointVectorValuePair(currentPoint, currentObjective);\n\n            // build the linear problem\n            final double[]   b = new double[nC];\n            final double[][] a = new double[nC][nC];\n            for (int i = 0; i < nR; ++i) {\n\n                final double[] grad   = weightedJacobian.getRow(i);\n                final double weight   = residualsWeights[i];\n                final double residual = currentResiduals[i];\n\n                // compute the normal equation\n                final double wr = weight * residual;\n                for (int j = 0; j < nC; ++j) {\n                    b[j] += wr * grad[j];\n                }\n\n                // build the contribution matrix for measurement i\n                for (int k = 0; k < nC; ++k) {\n                    double[] ak = a[k];\n                    double wgk = weight * grad[k];\n                    for (int l = 0; l < nC; ++l) {\n                        ak[l] += wgk * grad[l];\n                    }\n                }\n            }\n\n            try {\n                // solve the linearized least squares problem\n                RealMatrix mA = new BlockRealMatrix(a);\n                DecompositionSolver solver = useLU ?\n                        new LUDecomposition(mA).getSolver() :\n                        new QRDecomposition(mA).getSolver();\n                final double[] dX = solver.solve(new ArrayRealVector(b, false)).toArray();\n                // update the estimated parameters\n                for (int i = 0; i < nC; ++i) {\n                    currentPoint[i] += dX[i];\n                }\n            } catch (SingularMatrixException e) {\n                throw new ConvergenceException(LocalizedFormats.UNABLE_TO_SOLVE_SINGULAR_PROBLEM);\n            }\n\n            // Check convergence.\n            if (previous != null) {\n                converged = checker.converged(getIterations(), previous, current);\n                if (converged) {\n                    setCost(computeCost(currentResiduals));\n                    return current;\n                }\n            }\n        }\n        // Must never happen.\n        throw new MathInternalError();\n    }\n",
                "comment": "    /** {@inheritDoc} */\n",
                "normalized_body": [
                    "@Override public PointVectorValuePair doOptimize() { checkParameters(); final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker(); if (checker == null) { throw new NullArgumentException(); } final double[] targetValues = getTarget(); final int nR = targetValues.length; final RealMatrix weightMatrix = getWeight(); final double[] residualsWeights = new double[nR]; for (int i = 0; i < nR; i++) { residualsWeights[i] = weightMatrix.getEntry(i, i); } final double[] currentPoint = getStartPoint(); final int nC = currentPoint.length; PointVectorValuePair current = null; int iter = 0; for (boolean converged = false; !converged; ) { ++iter; PointVectorValuePair previous = current; final double[] currentObjective = computeObjectiveValue(currentPoint); final double[] currentResiduals = computeResiduals(currentObjective); final RealMatrix weightedJacobian = computeWeightedJacobian(currentPoint); current = new PointVectorValuePair(currentPoint, currentObjective); final double[] b = new double[nC]; final double[][] a = new double[nC][nC]; for (int i = 0; i < nR; ++i) { final double[] grad = weightedJacobian.getRow(i); final double weight = residualsWeights[i]; final double residual = currentResiduals[i]; final double wr = weight * residual; for (int j = 0; j < nC; ++j) { b[j] += wr * grad[j]; } for (int k = 0; k < nC; ++k) { double[] ak = a[k]; double wgk = weight * grad[k]; for (int l = 0; l < nC; ++l) { ak[l] += wgk * grad[l]; } } } try { RealMatrix mA = new BlockRealMatrix(a); DecompositionSolver solver = useLU ? new LUDecomposition(mA).getSolver() : new QRDecomposition(mA).getSolver(); final double[] dX = solver.solve(new ArrayRealVector(b, false)).toArray(); for (int i = 0; i < nC; ++i) { currentPoint[i] += dX[i]; } } catch (SingularMatrixException e) { throw new ConvergenceException(LocalizedFormats.UNABLE_TO_SOLVE_SINGULAR_PROBLEM); } if (previous != null) { converged = checker.converged(iter, previous, current); if (converged) { setCost(computeCost(currentResiduals)); return current; } } } throw new MathInternalError(); }"
                ],
                "top_similar_methods": [
                    [
                        1.0,
                        "@Override public PointVectorValuePair doOptimize() { checkParameters(); final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker(); if (checker == null) { throw new NullArgumentException(); } final double[] targetValues = getTarget(); final int nR = targetValues.length; final RealMatrix weightMatrix = getWeight(); final double[] residualsWeights = new double[nR]; for (int i = 0; i < nR; i++) { residualsWeights[i] = weightMatrix.getEntry(i, i); } final double[] currentPoint = getStartPoint(); final int nC = currentPoint.length; PointVectorValuePair current = null; int iter = 0; for (boolean converged = false; !converged; ) { ++iter; PointVectorValuePair previous = current; final double[] currentObjective = computeObjectiveValue(currentPoint); final double[] currentResiduals = computeResiduals(currentObjective); final RealMatrix weightedJacobian = computeWeightedJacobian(currentPoint); current = new PointVectorValuePair(currentPoint, currentObjective); final double[] b = new double[nC]; final double[][] a = new double[nC][nC]; for (int i = 0; i < nR; ++i) { final double[] grad = weightedJacobian.getRow(i); final double weight = residualsWeights[i]; final double residual = currentResiduals[i]; final double wr = weight * residual; for (int j = 0; j < nC; ++j) { b[j] += wr * grad[j]; } for (int k = 0; k < nC; ++k) { double[] ak = a[k]; double wgk = weight * grad[k]; for (int l = 0; l < nC; ++l) { ak[l] += wgk * grad[l]; } } } try { RealMatrix mA = new BlockRealMatrix(a); DecompositionSolver solver = useLU ? new LUDecomposition(mA).getSolver() : new QRDecomposition(mA).getSolver(); final double[] dX = solver.solve(new ArrayRealVector(b, false)).toArray(); for (int i = 0; i < nC; ++i) { currentPoint[i] += dX[i]; } } catch (SingularMatrixException e) { throw new ConvergenceException(LocalizedFormats.UNABLE_TO_SOLVE_SINGULAR_PROBLEM); } if (previous != null) { converged = checker.converged(iter, previous, current); if (converged) { setCost(computeCost(currentResiduals)); return current; } } } throw new MathInternalError(); }"
                    ],
                    [
                        0.9814519286155701,
                        "@Test public void testParametersErrorMonteCarloObservations() { final double yError = 15; final double slope = 123.456; final double offset = -98.765; final RandomStraightLinePointGenerator lineGenerator = new RandomStraightLinePointGenerator(slope, offset, yError, -1e3, 1e4, 138577L); final int numObs = 100; final int numParams = 2; final SummaryStatistics[] paramsFoundByDirectSolution = new SummaryStatistics[numParams]; final SummaryStatistics[] sigmaEstimate = new SummaryStatistics[numParams]; for (int i = 0; i < numParams; i++) { paramsFoundByDirectSolution[i] = new SummaryStatistics(); sigmaEstimate[i] = new SummaryStatistics(); } final AbstractLeastSquaresOptimizer optim = new DummyOptimizer(); final double[] init = { slope, offset }; final int mcRepeat = MONTE_CARLO_RUNS; int mcCount = 0; while (mcCount < mcRepeat) { final Point2D.Double[] obs = lineGenerator.generate(numObs); final StraightLineProblem problem = new StraightLineProblem(yError); for (int i = 0; i < numObs; i++) { final Point2D.Double p = obs[i]; problem.addPoint(p.x, p.y); } final double[] regress = problem.solve(); final PointVectorValuePair optimum = optim.optimize(new MaxEval(Integer.MAX_VALUE), problem.getModelFunction(), problem.getModelFunctionJacobian(), new Target(problem.target()), new Weight(problem.weight()), new InitialGuess(init)); final double[] sigma = optim.computeSigma(optimum.getPoint(), 1e-14); for (int i = 0; i < numParams; i++) { paramsFoundByDirectSolution[i].addValue(regress[i]); sigmaEstimate[i].addValue(sigma[i]); } ++mcCount; } final String line = \\\"--------------------------------------------------------------\\\"; System.out.println(\\\" True value Mean Std deviation\\\"); for (int i = 0; i < numParams; i++) { System.out.println(line); System.out.println(\\\"Parameter #\\\" + i); StatisticalSummary s = paramsFoundByDirectSolution[i].getSummary(); System.out.printf(\\\" %+.6e %+.6e %+.6e\\n\\\", init[i], s.getMean(), s.getStandardDeviation()); s = sigmaEstimate[i].getSummary(); System.out.printf(\\\"sigma: %+.6e (%+.6e)\\n\\\", s.getMean(), s.getStandardDeviation()); } System.out.println(line); for (int i = 0; i < numParams; i++) { Assert.assertEquals(paramsFoundByDirectSolution[i].getSummary().getStandardDeviation(), sigmaEstimate[i].getSummary().getMean(), 8e-2); } }"
                    ],
                    [
                        0.9812255799770355,
                        "@Override public PointVectorValuePair doOptimize() { final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker(); if (checker == null) { throw new NullArgumentException(); } final double[] targetValues = getTarget(); final int nR = targetValues.length; final RealMatrix weightMatrix = getWeight(); final double[] residualsWeights = new double[nR]; for (int i = 0; i < nR; i++) { residualsWeights[i] = weightMatrix.getEntry(i, i); } final double[] currentPoint = getStartPoint(); final int nC = currentPoint.length; PointVectorValuePair current = null; int iter = 0; for (boolean converged = false; !converged; ) { ++iter; PointVectorValuePair previous = current; final double[] currentObjective = computeObjectiveValue(currentPoint); final double[] currentResiduals = computeResiduals(currentObjective); final RealMatrix weightedJacobian = computeWeightedJacobian(currentPoint); current = new PointVectorValuePair(currentPoint, currentObjective); final double[] b = new double[nC]; final double[][] a = new double[nC][nC]; for (int i = 0; i < nR; ++i) { final double[] grad = weightedJacobian.getRow(i); final double weight = residualsWeights[i]; final double residual = currentResiduals[i]; final double wr = weight * residual; for (int j = 0; j < nC; ++j) { b[j] += wr * grad[j]; } for (int k = 0; k < nC; ++k) { double[] ak = a[k]; double wgk = weight * grad[k]; for (int l = 0; l < nC; ++l) { ak[l] += wgk * grad[l]; } } } try { RealMatrix mA = new BlockRealMatrix(a); DecompositionSolver solver = useLU ? new LUDecomposition(mA).getSolver() : new QRDecomposition(mA).getSolver(); final double[] dX = solver.solve(new ArrayRealVector(b, false)).toArray(); for (int i = 0; i < nC; ++i) { currentPoint[i] += dX[i]; } } catch (SingularMatrixException e) { throw new ConvergenceException(LocalizedFormats.UNABLE_TO_SOLVE_SINGULAR_PROBLEM); } if (previous != null) { converged = checker.converged(iter, previous, current); if (converged) { cost = computeCost(currentResiduals); point = current.getPoint(); return current; } } } throw new MathInternalError(); }"
                    ],
                    [
                        0.9809415638446808,
                        "private void doTestMath753(final double shape, final double meanNoOF, final double sdNoOF, final double meanOF, final double sdOF, final String resourceName) throws IOException { final GammaDistribution distribution = new GammaDistribution(shape, 1.0); final SummaryStatistics statOld = new SummaryStatistics(); final SummaryStatistics statNewNoOF = new SummaryStatistics(); final SummaryStatistics statNewOF = new SummaryStatistics(); final InputStream resourceAsStream; resourceAsStream = this.getClass().getResourceAsStream(resourceName); Assert.assertNotNull(\\\"Could not find resource \\\" + resourceName, resourceAsStream); final BufferedReader in; in = new BufferedReader(new InputStreamReader(resourceAsStream)); try { for (String line = in.readLine(); line != null; line = in.readLine()) { if (line.startsWith(\\\"#\\\")) { continue; } final String[] tokens = line.split(\\\", \\\"); Assert.assertTrue(\\\"expected two floating-point values\\\", tokens.length == 2); final double x = Double.parseDouble(tokens[0]); final String msg = \\\"x = \\\" + x + \\\", shape = \\\" + shape + \\\", scale = 1.0\\\"; final double expected = Double.parseDouble(tokens[1]); final double ulp = FastMath.ulp(expected); final double actualOld = density(x, shape, 1.0); final double actualNew = distribution.density(x); final double errOld, errNew; errOld = FastMath.abs((actualOld - expected) / ulp); errNew = FastMath.abs((actualNew - expected) / ulp); if (Double.isNaN(actualOld) || Double.isInfinite(actualOld)) { Assert.assertFalse(msg, Double.isNaN(actualNew)); Assert.assertFalse(msg, Double.isInfinite(actualNew)); statNewOF.addValue(errNew); } else { statOld.addValue(errOld); statNewNoOF.addValue(errNew); } } if (statOld.getN() != 0) { final StringBuilder sb = new StringBuilder(\\\"shape = \\\"); sb.append(shape); sb.append(\\\", scale = 1.0\\n\\\"); sb.append(\\\"Old implementation\\n\\\"); sb.append(\\\"------------------\\n\\\"); sb.append(statOld.toString()); sb.append(\\\"New implementation\\n\\\"); sb.append(\\\"------------------\\n\\\"); sb.append(statNewNoOF.toString()); final String msg = sb.toString(); final double oldMin = statOld.getMin(); final double newMin = statNewNoOF.getMin(); Assert.assertTrue(msg, newMin <= oldMin); final double oldMax = statOld.getMax(); final double newMax = statNewNoOF.getMax(); Assert.assertTrue(msg, newMax <= oldMax); final double oldMean = statOld.getMean(); final double newMean = statNewNoOF.getMean(); Assert.assertTrue(msg, newMean <= oldMean); final double oldSd = statOld.getStandardDeviation(); final double newSd = statNewNoOF.getStandardDeviation(); Assert.assertTrue(msg, newSd <= oldSd); Assert.assertTrue(msg, newMean <= meanNoOF); Assert.assertTrue(msg, newSd <= sdNoOF); } if (statNewOF.getN() != 0) { final double newMean = statNewOF.getMean(); final double newSd = statNewOF.getStandardDeviation(); final StringBuilder sb = new StringBuilder(\\\"shape = \\\"); sb.append(shape); sb.append(\\\", scale = 1.0\\\"); sb.append(\\\", max. mean error (ulps) = \\\"); sb.append(meanOF); sb.append(\\\", actual mean error (ulps) = \\\"); sb.append(newMean); sb.append(\\\", max. sd of error (ulps) = \\\"); sb.append(sdOF); sb.append(\\\", actual sd of error (ulps) = \\\"); sb.append(newSd); final String msg = sb.toString(); Assert.assertTrue(msg, newMean <= meanOF); Assert.assertTrue(msg, newSd <= sdOF); } } catch (IOException e) { Assert.fail(e.getMessage()); } finally { in.close(); } }"
                    ],
                    [
                        0.9807688593864441,
                        "@Test public void testParametersErrorMonteCarloObservations() { final double yError = 15; final double slope = 123.456; final double offset = -98.765; final RandomStraightLinePointGenerator lineGenerator = new RandomStraightLinePointGenerator(slope, offset, yError, -1e3, 1e4, 138577L); final int numObs = 100; final int numParams = 2; final SummaryStatistics[] paramsFoundByDirectSolution = new SummaryStatistics[numParams]; final SummaryStatistics[] sigmaEstimate = new SummaryStatistics[numParams]; for (int i = 0; i < numParams; i++) { paramsFoundByDirectSolution[i] = new SummaryStatistics(); sigmaEstimate[i] = new SummaryStatistics(); } final AbstractLeastSquaresOptimizer optim = new DummyOptimizer(); final double[] init = { slope, offset }; final int mcRepeat = MONTE_CARLO_RUNS; int mcCount = 0; while (mcCount < mcRepeat) { final Point2D.Double[] obs = lineGenerator.generate(numObs); final StraightLineProblem problem = new StraightLineProblem(yError); for (int i = 0; i < numObs; i++) { final Point2D.Double p = obs[i]; problem.addPoint(p.x, p.y); } final double[] regress = problem.solve(); final PointVectorValuePair optimum = optim.optimize(Integer.MAX_VALUE, problem, problem.target(), problem.weight(), init); final double[] sigma = optim.computeSigma(optimum.getPoint(), 1e-14); for (int i = 0; i < numParams; i++) { paramsFoundByDirectSolution[i].addValue(regress[i]); sigmaEstimate[i].addValue(sigma[i]); } ++mcCount; } final String line = \\\"--------------------------------------------------------------\\\"; System.out.println(\\\" True value Mean Std deviation\\\"); for (int i = 0; i < numParams; i++) { System.out.println(line); System.out.println(\\\"Parameter #\\\" + i); StatisticalSummary s = paramsFoundByDirectSolution[i].getSummary(); System.out.printf(\\\" %+.6e %+.6e %+.6e\\n\\\", init[i], s.getMean(), s.getStandardDeviation()); s = sigmaEstimate[i].getSummary(); System.out.printf(\\\"sigma: %+.6e (%+.6e)\\n\\\", s.getMean(), s.getStandardDeviation()); } System.out.println(line); for (int i = 0; i < numParams; i++) { Assert.assertEquals(paramsFoundByDirectSolution[i].getSummary().getStandardDeviation(), sigmaEstimate[i].getSummary().getMean(), 8e-2); } }"
                    ],
                    [
                        0.9801796674728394,
                        "@Test public void testParametersErrorMonteCarloParameters() { final double yError = 15; final double slope = 123.456; final double offset = -98.765; final RandomStraightLinePointGenerator lineGenerator = new RandomStraightLinePointGenerator(slope, offset, yError, -1e3, 1e4, 13839013L); final int numObs = 10; final int numParams = 2; final Point2D.Double[] obs = lineGenerator.generate(numObs); final StraightLineProblem problem = new StraightLineProblem(yError); for (int i = 0; i < numObs; i++) { final Point2D.Double p = obs[i]; problem.addPoint(p.x, p.y); } final double[] regress = problem.solve(); final AbstractLeastSquaresOptimizer optim = new DummyOptimizer(); final double[] init = { slope, offset }; final double bestChi2N = getChi2N(optim, problem, regress); final double[] sigma = optim.computeSigma(regress, 1e-14); final int mcRepeat = MONTE_CARLO_RUNS; final int gridSize = (int) FastMath.sqrt(mcRepeat); final List<double[]> paramsAndChi2 = new ArrayList<double[]>(gridSize * gridSize); final double slopeRange = 10 * sigma[0]; final double offsetRange = 10 * sigma[1]; final double minSlope = slope - 0.5 * slopeRange; final double minOffset = offset - 0.5 * offsetRange; final double deltaSlope = slopeRange / gridSize; final double deltaOffset = offsetRange / gridSize; for (int i = 0; i < gridSize; i++) { final double s = minSlope + i * deltaSlope; for (int j = 0; j < gridSize; j++) { final double o = minOffset + j * deltaOffset; final double chi2N = getChi2N(optim, problem, new double[] { s, o }); paramsAndChi2.add(new double[] { s, o, chi2N }); } } final double chi2NPlusOne = bestChi2N + 1; int numLarger = 0; final String lineFmt = \\\"%+.10e %+.10e %.8e\\n\\\"; System.out.printf(lineFmt, regress[0], regress[1], bestChi2N); System.out.println(); for (double[] d : paramsAndChi2) { if (d[2] <= chi2NPlusOne) { System.out.printf(lineFmt, d[0], d[1], d[2]); } } System.out.println(); for (double[] d : paramsAndChi2) { if (d[2] > chi2NPlusOne) { ++numLarger; System.out.printf(lineFmt, d[0], d[1], d[2]); } } System.out.println(); System.out.println(\\\"# sigma=\\\" + Arrays.toString(sigma)); System.out.println(\\\"# \\\" + numLarger + \\\" sets filtered out\\\"); }"
                    ],
                    [
                        0.9801796674728394,
                        "@Test public void testParametersErrorMonteCarloParameters() { final double yError = 15; final double slope = 123.456; final double offset = -98.765; final RandomStraightLinePointGenerator lineGenerator = new RandomStraightLinePointGenerator(slope, offset, yError, -1e3, 1e4, 13839013L); final int numObs = 10; final int numParams = 2; final Point2D.Double[] obs = lineGenerator.generate(numObs); final StraightLineProblem problem = new StraightLineProblem(yError); for (int i = 0; i < numObs; i++) { final Point2D.Double p = obs[i]; problem.addPoint(p.x, p.y); } final double[] regress = problem.solve(); final AbstractLeastSquaresOptimizer optim = new DummyOptimizer(); final double[] init = { slope, offset }; final double bestChi2N = getChi2N(optim, problem, regress); final double[] sigma = optim.computeSigma(regress, 1e-14); final int mcRepeat = MONTE_CARLO_RUNS; final int gridSize = (int) FastMath.sqrt(mcRepeat); final List<double[]> paramsAndChi2 = new ArrayList<double[]>(gridSize * gridSize); final double slopeRange = 10 * sigma[0]; final double offsetRange = 10 * sigma[1]; final double minSlope = slope - 0.5 * slopeRange; final double minOffset = offset - 0.5 * offsetRange; final double deltaSlope = slopeRange / gridSize; final double deltaOffset = offsetRange / gridSize; for (int i = 0; i < gridSize; i++) { final double s = minSlope + i * deltaSlope; for (int j = 0; j < gridSize; j++) { final double o = minOffset + j * deltaOffset; final double chi2N = getChi2N(optim, problem, new double[] { s, o }); paramsAndChi2.add(new double[] { s, o, chi2N }); } } final double chi2NPlusOne = bestChi2N + 1; int numLarger = 0; final String lineFmt = \\\"%+.10e %+.10e %.8e\\n\\\"; System.out.printf(lineFmt, regress[0], regress[1], bestChi2N); System.out.println(); for (double[] d : paramsAndChi2) { if (d[2] <= chi2NPlusOne) { System.out.printf(lineFmt, d[0], d[1], d[2]); } } System.out.println(); for (double[] d : paramsAndChi2) { if (d[2] > chi2NPlusOne) { ++numLarger; System.out.printf(lineFmt, d[0], d[1], d[2]); } } System.out.println(); System.out.println(\\\"# sigma=\\\" + Arrays.toString(sigma)); System.out.println(\\\"# \\\" + numLarger + \\\" sets filtered out\\\"); }"
                    ],
                    [
                        0.9789253771305084,
                        "public double evaluate(final double[] values, final int begin, final int length, final double p) throws MathIllegalArgumentException { test(values, begin, length); if ((p > 100) || (p <= 0)) { throw new OutOfRangeException(LocalizedFormats.OUT_OF_BOUNDS_QUANTILE_VALUE, p, 0, 100); } if (length == 0) { return Double.NaN; } if (length == 1) { return values[begin]; } double n = length; double pos = p * (n + 1) / 100; double fpos = FastMath.floor(pos); int intPos = (int) fpos; double dif = pos - fpos; double[] work; int[] pivotsHeap; if (values == getDataRef()) { work = getDataRef(); pivotsHeap = cachedPivots; } else { work = new double[length]; System.arraycopy(values, begin, work, 0, length); pivotsHeap = new int[(0x1 << MAX_CACHED_LEVELS) - 1]; Arrays.fill(pivotsHeap, -1); } if (pos < 1) { return select(work, pivotsHeap, 0); } if (pos >= n) { return select(work, pivotsHeap, length - 1); } double lower = select(work, pivotsHeap, intPos - 1); double upper = select(work, pivotsHeap, intPos); return lower + dif * (upper - lower); }"
                    ],
                    [
                        0.9785684943199158,
                        "public void addSamplePoint(final T x, final T[]... value) throws ZeroException, MathArithmeticException, DimensionMismatchException, NullArgumentException { MathUtils.checkNotNull(x); T factorial = x.getField().getOne(); for (int i = 0; i < value.length; ++i) { final T[] y = value[i].clone(); if (i > 1) { factorial = factorial.multiply(i); final T inv = factorial.reciprocal(); for (int j = 0; j < y.length; ++j) { y[j] = y[j].multiply(inv); } } final int n = abscissae.size(); bottomDiagonal.add(n - i, y); T[] bottom0 = y; for (int j = i; j < n; ++j) { final T[] bottom1 = bottomDiagonal.get(n - (j + 1)); if (x.equals(abscissae.get(n - (j + 1)))) { throw new ZeroException(LocalizedFormats.DUPLICATED_ABSCISSA_DIVISION_BY_ZERO, x); } final T inv = x.subtract(abscissae.get(n - (j + 1))).reciprocal(); for (int k = 0; k < y.length; ++k) { bottom1[k] = inv.multiply(bottom0[k].subtract(bottom1[k])); } bottom0 = bottom1; } topDiagonal.add(bottom0.clone()); abscissae.add(x); } }"
                    ],
                    [
                        0.9785278141498566,
                        "@Test public void testMatrixFunction() { FiniteDifferencesDifferentiator differentiator = new FiniteDifferencesDifferentiator(7, 0.01); UnivariateDifferentiableMatrixFunction f = differentiator.differentiate(new UnivariateMatrixFunction() { public double[][] value(double x) { return new double[][] { { FastMath.cos(x), FastMath.sin(x) }, { FastMath.cosh(x), FastMath.sinh(x) } }; } }); for (double x = -1; x < 1; x += 0.02) { DerivativeStructure dsX = new DerivativeStructure(1, 2, 0, x); DerivativeStructure[][] y = f.value(dsX); double cos = FastMath.cos(x); double sin = FastMath.sin(x); double cosh = FastMath.cosh(x); double sinh = FastMath.sinh(x); double[][] f1 = f.value(dsX.getValue()); DerivativeStructure[][] f2 = f.value(dsX); Assert.assertEquals(f1.length, f2.length); for (int i = 0; i < f1.length; ++i) { Assert.assertEquals(f1[i].length, f2[i].length); for (int j = 0; j < f1[i].length; ++j) { Assert.assertEquals(f1[i][j], f2[i][j].getValue(), 1.0e-15); } } Assert.assertEquals(cos, y[0][0].getValue(), 7.0e-18); Assert.assertEquals(sin, y[0][1].getValue(), 6.0e-17); Assert.assertEquals(cosh, y[1][0].getValue(), 3.0e-16); Assert.assertEquals(sinh, y[1][1].getValue(), 3.0e-16); Assert.assertEquals(-sin, y[0][0].getPartialDerivative(1), 2.0e-14); Assert.assertEquals(cos, y[0][1].getPartialDerivative(1), 2.0e-14); Assert.assertEquals(sinh, y[1][0].getPartialDerivative(1), 3.0e-14); Assert.assertEquals(cosh, y[1][1].getPartialDerivative(1), 3.0e-14); Assert.assertEquals(-cos, y[0][0].getPartialDerivative(2), 3.0e-12); Assert.assertEquals(-sin, y[0][1].getPartialDerivative(2), 3.0e-12); Assert.assertEquals(cosh, y[1][0].getPartialDerivative(2), 6.0e-12); Assert.assertEquals(sinh, y[1][1].getPartialDerivative(2), 6.0e-12); } }"
                    ],
                    [
                        0.9784102141857147,
                        "@Test public void testUnpreconditionedSolutionWithInitialGuess() { final int n = 5; final int maxIterations = 100; final RealLinearOperator a = new HilbertMatrix(n); final InverseHilbertMatrix ainv = new InverseHilbertMatrix(n); final IterativeLinearSolver solver; solver = new ConjugateGradient(maxIterations, 1E-10, true); final RealVector b = new ArrayRealVector(n); for (int j = 0; j < n; j++) { b.set(0.); b.setEntry(j, 1.); final RealVector x0 = new ArrayRealVector(n); x0.set(1.); final RealVector x = solver.solve(a, b, x0); Assert.assertNotSame(\\\"x should not be a reference to x0\\\", x0, x); for (int i = 0; i < n; i++) { final double actual = x.getEntry(i); final double expected = ainv.getEntry(i, j); final double delta = 1E-10 * Math.abs(expected); final String msg = String.format(\\\"entry[%d][%d]\\\", i, j); Assert.assertEquals(msg, expected, actual, delta); Assert.assertEquals(msg, x0.getEntry(i), 1., Math.ulp(1.)); } } }"
                    ],
                    [
                        0.9774422943592072,
                        "@Override protected double doIntegrate() throws TooManyEvaluationsException, MaxCountExceededException { final int m = iterations.getMaximalCount() + 1; double[] previousRow = new double[m]; double[] currentRow = new double[m]; TrapezoidIntegrator qtrap = new TrapezoidIntegrator(); currentRow[0] = qtrap.stage(this, 0); iterations.incrementCount(); double olds = currentRow[0]; while (true) { final int i = iterations.getCount(); final double[] tmpRow = previousRow; previousRow = currentRow; currentRow = tmpRow; currentRow[0] = qtrap.stage(this, i); iterations.incrementCount(); for (int j = 1; j <= i; j++) { final double r = (1L << (2 * j)) - 1; final double tIJm1 = currentRow[j - 1]; currentRow[j] = tIJm1 + (tIJm1 - previousRow[j - 1]) / r; } final double s = currentRow[i]; if (i >= getMinimalIterationCount()) { final double delta = FastMath.abs(s - olds); final double rLimit = getRelativeAccuracy() * (FastMath.abs(olds) + FastMath.abs(s)) * 0.5; if ((delta <= rLimit) || (delta <= getAbsoluteAccuracy())) { return s; } } olds = s; } }"
                    ]
                ],
                "directly_related_tests": [
                    "org.apache.commons.math3.genetics.FixedElapsedTimeTest::testIsSatisfied"
                ],
                "generation": {
                    "1": {
                        "prompt": "You are an expert Java developer. Below is a buggy method from a large Java project. This method causes one or more test failures.\nYour task is to fix the bug in this method. Use the provided test failure(s) to guide your reasoning.\n\n---\n\n## Buggy Method\n\n```java\n    /** {@inheritDoc} */\n\n    @Override\n    public PointVectorValuePair doOptimize() {\n        checkParameters();\n\n        final ConvergenceChecker<PointVectorValuePair> checker\n            = getConvergenceChecker();\n\n        // Computation will be useless without a checker (see \"for-loop\").\n        if (checker == null) {\n            throw new NullArgumentException();\n        }\n\n        final double[] targetValues = getTarget();\n        final int nR = targetValues.length; // Number of observed data.\n\n        final RealMatrix weightMatrix = getWeight();\n        // Diagonal of the weight matrix.\n        final double[] residualsWeights = new double[nR];\n        for (int i = 0; i < nR; i++) {\n            residualsWeights[i] = weightMatrix.getEntry(i, i);\n        }\n\n        final double[] currentPoint = getStartPoint();\n        final int nC = currentPoint.length;\n\n        // iterate until convergence is reached\n        PointVectorValuePair current = null;\n        int iter = 0;\n        for (boolean converged = false; !converged;) {\n            ++iter;\n\n            // evaluate the objective function and its jacobian\n            PointVectorValuePair previous = current;\n            // Value of the objective function at \"currentPoint\".\n            final double[] currentObjective = computeObjectiveValue(currentPoint);\n            final double[] currentResiduals = computeResiduals(currentObjective);\n            final RealMatrix weightedJacobian = computeWeightedJacobian(currentPoint);\n            current = new PointVectorValuePair(currentPoint, currentObjective);\n\n            // build the linear problem\n            final double[]   b = new double[nC];\n            final double[][] a = new double[nC][nC];\n            for (int i = 0; i < nR; ++i) {\n\n                final double[] grad   = weightedJacobian.getRow(i);\n                final double weight   = residualsWeights[i];\n                final double residual = currentResiduals[i];\n\n                // compute the normal equation\n                final double wr = weight * residual;\n                for (int j = 0; j < nC; ++j) {\n                    b[j] += wr * grad[j];\n                }\n\n                // build the contribution matrix for measurement i\n                for (int k = 0; k < nC; ++k) {\n                    double[] ak = a[k];\n                    double wgk = weight * grad[k];\n                    for (int l = 0; l < nC; ++l) {\n                        ak[l] += wgk * grad[l];\n                    }\n                }\n            }\n\n            try {\n                // solve the linearized least squares problem\n                RealMatrix mA = new BlockRealMatrix(a);\n                DecompositionSolver solver = useLU ?\n                        new LUDecomposition(mA).getSolver() :\n                        new QRDecomposition(mA).getSolver();\n                final double[] dX = solver.solve(new ArrayRealVector(b, false)).toArray();\n                // update the estimated parameters\n                for (int i = 0; i < nC; ++i) {\n                    currentPoint[i] += dX[i];\n                }\n            } catch (SingularMatrixException e) {\n                throw new ConvergenceException(LocalizedFormats.UNABLE_TO_SOLVE_SINGULAR_PROBLEM);\n            }\n\n            // Check convergence.\n            if (previous != null) {\n                converged = checker.converged(iter, previous, current);\n                if (converged) {\n                    setCost(computeCost(currentResiduals));\n                    return current;\n                }\n            }\n        }\n        // Must never happen.\n        throw new MathInternalError();\n    }\n\n```\n---\n\n## Failing Test Case(s)\n\n\n---\n\n## Your Goal\n\nFix the buggy method. Return only the fixed Java method. Do not include explanations or other text.\n",
                        "patches": [
                            "\n/** {@inheritDoc} */\n\n@Override\npublic PointVectorValuePair doOptimize() {\n    checkParameters();\n\n    final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n\n    // Computation will be useless without a checker (see \"for-loop\").\n    if (checker == null) {\n        throw new NullArgumentException();\n    }\n\n    final double[] targetValues = getTarget();\n    final int nR = targetValues.length; // Number of observed data.\n\n    final RealMatrix weightMatrix = getWeight();\n    // Diagonal of the weight matrix.\n    final double[] residualsWeights = new double[nR];\n    for (int i = 0; i < nR; i++) {\n        residualsWeights[i] = weightMatrix.getEntry(i, i);\n    }\n\n    final double[] currentPoint = getStartPoint();\n    final int nC = currentPoint.length;\n\n    // iterate until convergence is reached\n    PointVectorValuePair current = null;\n    int iter = 0;\n    for (boolean converged = false; !converged;) {\n        ++iter;\n\n        // evaluate the objective function and its jacobian\n        PointVectorValuePair previous = current;\n        // Value of the objective function at \"currentPoint\".\n        final double[] currentObjective = computeObjectiveValue(currentPoint);\n        final double[] currentResiduals = computeResiduals(currentObjective);\n        final RealMatrix weightedJacobian = computeWeightedJacobian(currentPoint);\n        current = new PointVectorValuePair(currentPoint.clone(), currentObjective);\n\n        // build the linear problem\n        final double[] b = new double[nC];\n        final double[][] a = new double[nC][nC];\n        for (int i = 0; i < nR; ++i) {\n\n            final double[] grad = weightedJacobian.getRow(i);\n            final double weight = residualsWeights[i];\n            final double residual = currentResiduals[i];\n\n            // compute the normal equation\n            final double wr = weight * residual;\n            for (int j = 0; j < nC; ++j) {\n                b[j] += wr * grad[j];\n            }\n\n            // build the contribution matrix for measurement i\n            for (int k = 0; k < nC; ++k) {\n                double[] ak = a[k];\n                double wgk = weight * grad[k];\n                for (int l = 0; l < nC; ++l) {\n                    ak[l] += wgk * grad[l];\n                }\n            }\n        }\n\n        try {\n            // solve the linearized least squares problem\n            RealMatrix mA = new BlockRealMatrix(a);\n            DecompositionSolver solver = useLU ?\n                    new LUDecomposition(mA).getSolver() :\n                    new QRDecomposition(mA).getSolver();\n            final double[] dX = solver.solve(new ArrayRealVector(b, false)).toArray();\n            // update the estimated parameters\n            for (int i = 0; i < nC; ++i) {\n                currentPoint[i] += dX[i];\n            }\n        } catch (SingularMatrixException e) {\n            throw new ConvergenceException(LocalizedFormats.UNABLE_TO_SOLVE_SINGULAR_PROBLEM);\n        }\n\n        // Check convergence.\n        if (previous != null) {\n            converged = checker.converged(iter, previous, current);\n            if (converged) {\n                setCost(computeCost(currentResiduals));\n                return current;\n            }\n        }\n    }\n    // Must never happen.\n    throw new MathInternalError();\n}\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    public PointVectorValuePair doOptimize() {\n        checkParameters();\n\n        final ConvergenceChecker<PointVectorValuePair> checker\n            = getConvergenceChecker();\n\n        // Computation will be useless without a checker (see \"for-loop\").\n        if (checker == null) {\n            throw new NullArgumentException();\n        }\n\n        final double[] targetValues = getTarget();\n        final int nR = targetValues.length; // Number of observed data.\n\n        final RealMatrix weightMatrix = getWeight();\n        // Diagonal of the weight matrix.\n        final double[] residualsWeights = new double[nR];\n        for (int i = 0; i < nR; i++) {\n            residualsWeights[i] = weightMatrix.getEntry(i, i);\n        }\n\n        final double[] currentPoint = getStartPoint();\n        final int nC = currentPoint.length;\n\n        // iterate until convergence is reached\n        PointVectorValuePair current = null;\n        int iter = 0;\n        for (boolean converged = false; !converged;) {\n            ++iter;\n\n            // evaluate the objective function and its jacobian\n            PointVectorValuePair previous = current;\n            // Value of the objective function at \"currentPoint\".\n            final double[] currentObjective = computeObjectiveValue(currentPoint);\n            final double[] currentResiduals = computeResiduals(currentObjective);\n            final RealMatrix weightedJacobian = computeWeightedJacobian(currentPoint);\n            current = new PointVectorValuePair(currentPoint.clone(), currentObjective);\n\n            // build the linear problem\n            final double[]   b = new double[nC];\n            final double[][] a = new double[nC][nC];\n            for (int i = 0; i < nR; ++i) {\n\n                final double[] grad   = weightedJacobian.getRow(i);\n                final double weight   = residualsWeights[i];\n                final double residual = currentResiduals[i];\n\n                // compute the normal equation\n                final double wr = weight * residual;\n                for (int j = 0; j < nC; ++j) {\n                    b[j] += wr * grad[j];\n                }\n\n                // build the contribution matrix for measurement i\n                for (int k = 0; k < nC; ++k) {\n                    double[] ak = a[k];\n                    double wgk = weight * grad[k];\n                    for (int l = 0; l < nC; ++l) {\n                        ak[l] += wgk * grad[l];\n                    }\n                }\n            }\n\n            try {\n                // solve the linearized least squares problem\n                RealMatrix mA = new BlockRealMatrix(a);\n                DecompositionSolver solver = useLU ?\n                        new LUDecomposition(mA).getSolver() :\n                        new QRDecomposition(mA).getSolver();\n                final double[] dX = solver.solve(new ArrayRealVector(b, false)).toArray();\n                // update the estimated parameters\n                for (int i = 0; i < nC; ++i) {\n                    currentPoint[i] += dX[i];\n                }\n            } catch (SingularMatrixException e) {\n                throw new ConvergenceException(LocalizedFormats.UNABLE_TO_SOLVE_SINGULAR_PROBLEM);\n            }\n\n            // Check convergence.\n            if (previous != null) {\n                converged = checker.converged(iter, previous, current);\n                if (converged) {\n                    setCost(computeCost(currentResiduals));\n                    return current;\n                }\n            }\n        }\n        // Must never happen.\n        throw new MathInternalError();\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    public PointVectorValuePair doOptimize() {\n        checkParameters();\n\n        final ConvergenceChecker<PointVectorValuePair> checker\n            = getConvergenceChecker();\n\n        // Computation will be useless without a checker (see \"for-loop\").\n        if (checker == null) {\n            throw new NullArgumentException();\n        }\n\n        final double[] targetValues = getTarget();\n        final int nR = targetValues.length; // Number of observed data.\n\n        final RealMatrix weightMatrix = getWeight();\n        // Diagonal of the weight matrix.\n        final double[] residualsWeights = new double[nR];\n        for (int i = 0; i < nR; i++) {\n            residualsWeights[i] = weightMatrix.getEntry(i, i);\n        }\n\n        final double[] currentPoint = getStartPoint();\n        final int nC = currentPoint.length;\n\n        // iterate until convergence is reached\n        PointVectorValuePair current = null;\n        int iter = 0;\n        for (boolean converged = false; !converged;) {\n            ++iter;\n\n            // evaluate the objective function and its jacobian\n            PointVectorValuePair previous = current;\n            // Value of the objective function at \"currentPoint\".\n            final double[] currentObjective = computeObjectiveValue(currentPoint);\n            final double[] currentResiduals = computeResiduals(currentObjective);\n            final RealMatrix weightedJacobian = computeWeightedJacobian(currentPoint);\n            current = new PointVectorValuePair(currentPoint.clone(), currentObjective);\n\n            // build the linear problem\n            final double[]   b = new double[nC];\n            final double[][] a = new double[nC][nC];\n            for (int i = 0; i < nR; ++i) {\n\n                final double[] grad   = weightedJacobian.getRow(i);\n                final double weight   = residualsWeights[i];\n                final double residual = currentResiduals[i];\n\n                // compute the normal equation\n                final double wr = weight * residual;\n                for (int j = 0; j < nC; ++j) {\n                    b[j] += wr * grad[j];\n                }\n\n                // build the contribution matrix for measurement i\n                for (int k = 0; k < nC; ++k) {\n                    double[] ak = a[k];\n                    double wgk = weight * grad[k];\n                    for (int l = 0; l < nC; ++l) {\n                        ak[l] += wgk * grad[l];\n                    }\n                }\n            }\n\n            try {\n                // solve the linearized least squares problem\n                RealMatrix mA = new BlockRealMatrix(a);\n                DecompositionSolver solver = useLU ?\n                        new LUDecomposition(mA).getSolver() :\n                        new QRDecomposition(mA).getSolver();\n                final double[] dX = solver.solve(new ArrayRealVector(b, false)).toArray();\n                // update the estimated parameters\n                for (int i = 0; i < nC; ++i) {\n                    currentPoint[i] += dX[i];\n                }\n            } catch (SingularMatrixException e) {\n                throw new ConvergenceException(LocalizedFormats.UNABLE_TO_SOLVE_SINGULAR_PROBLEM);\n            }\n\n            // Check convergence.\n            if (previous != null) {\n                converged = checker.converged(iter, previous, current);\n                if (converged) {\n                    setCost(computeCost(currentResiduals));\n                    return current;\n                }\n            }\n        }\n        // Must never happen.\n        throw new MathInternalError();\n    }\n",
                            "\n@Override\npublic PointVectorValuePair doOptimize() {\n    checkParameters();\n\n    final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n\n    // Computation will be useless without a checker (see \"for-loop\").\n    if (checker == null) {\n        throw new NullArgumentException();\n    }\n\n    final double[] targetValues = getTarget();\n    final int nR = targetValues.length; // Number of observed data.\n\n    final RealMatrix weightMatrix = getWeight();\n    // Diagonal of the weight matrix.\n    final double[] residualsWeights = new double[nR];\n    for (int i = 0; i < nR; i++) {\n        residualsWeights[i] = weightMatrix.getEntry(i, i);\n    }\n\n    final double[] currentPoint = getStartPoint();\n    final int nC = currentPoint.length;\n\n    // iterate until convergence is reached\n    PointVectorValuePair current = null;\n    int iter = 0;\n    for (boolean converged = false; !converged;) {\n        ++iter;\n\n        // evaluate the objective function and its jacobian\n        PointVectorValuePair previous = current;\n        // Value of the objective function at \"currentPoint\".\n        final double[] currentObjective = computeObjectiveValue(currentPoint);\n        final double[] currentResiduals = computeResiduals(currentObjective);\n        final RealMatrix weightedJacobian = computeWeightedJacobian(currentPoint);\n        current = new PointVectorValuePair(currentPoint.clone(), currentObjective);\n\n        // build the linear problem\n        final double[] b = new double[nC];\n        final double[][] a = new double[nC][nC];\n        for (int i = 0; i < nR; ++i) {\n\n            final double[] grad = weightedJacobian.getRow(i);\n            final double weight = residualsWeights[i];\n            final double residual = currentResiduals[i];\n\n            // compute the normal equation\n            final double wr = weight * residual;\n            for (int j = 0; j < nC; ++j) {\n                b[j] += wr * grad[j];\n            }\n\n            // build the contribution matrix for measurement i\n            for (int k = 0; k < nC; ++k) {\n                double[] ak = a[k];\n                double wgk = weight * grad[k];\n                for (int l = 0; l < nC; ++l) {\n                    ak[l] += wgk * grad[l];\n                }\n            }\n        }\n\n        try {\n            // solve the linearized least squares problem\n            RealMatrix mA = new BlockRealMatrix(a);\n            DecompositionSolver solver = useLU ?\n                    new LUDecomposition(mA).getSolver() :\n                    new QRDecomposition(mA).getSolver();\n            final double[] dX = solver.solve(new ArrayRealVector(b, false)).toArray();\n            // update the estimated parameters\n            for (int i = 0; i < nC; ++i) {\n                currentPoint[i] += dX[i];\n            }\n        } catch (SingularMatrixException e) {\n            throw new ConvergenceException(LocalizedFormats.UNABLE_TO_SOLVE_SINGULAR_PROBLEM);\n        }\n\n        // Check convergence.\n        if (previous != null) {\n            converged = checker.converged(iter, previous, current);\n            if (converged) {\n                setCost(computeCost(currentResiduals));\n                return current;\n            }\n        }\n    }\n    // Must never happen.\n    throw new MathInternalError();\n}\n",
                            "\n@Override\npublic PointVectorValuePair doOptimize() {\n    checkParameters();\n\n    final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n\n    // Computation will be useless without a checker (see \"for-loop\").\n    if (checker == null) {\n        throw new NullArgumentException();\n    }\n\n    final double[] targetValues = getTarget();\n    final int nR = targetValues.length; // Number of observed data.\n\n    final RealMatrix weightMatrix = getWeight();\n    // Diagonal of the weight matrix.\n    final double[] residualsWeights = new double[nR];\n    for (int i = 0; i < nR; i++) {\n        residualsWeights[i] = weightMatrix.getEntry(i, i);\n    }\n\n    final double[] currentPoint = getStartPoint();\n    final int nC = currentPoint.length;\n\n    // iterate until convergence is reached\n    PointVectorValuePair current = null;\n    int iter = 0;\n    for (boolean converged = false; !converged;) {\n        ++iter;\n\n        // evaluate the objective function and its jacobian\n        PointVectorValuePair previous = current;\n        // Value of the objective function at \"currentPoint\".\n        final double[] currentObjective = computeObjectiveValue(currentPoint);\n        final double[] currentResiduals = computeResiduals(currentObjective);\n        final RealMatrix weightedJacobian = computeWeightedJacobian(currentPoint);\n        current = new PointVectorValuePair(currentPoint.clone(), currentObjective);\n\n        // build the linear problem\n        final double[] b = new double[nC];\n        final double[][] a = new double[nC][nC];\n        for (int i = 0; i < nR; ++i) {\n\n            final double[] grad = weightedJacobian.getRow(i);\n            final double weight = residualsWeights[i];\n            final double residual = currentResiduals[i];\n\n            // compute the normal equation\n            final double wr = weight * residual;\n            for (int j = 0; j < nC; ++j) {\n                b[j] += wr * grad[j];\n            }\n\n            // build the contribution matrix for measurement i\n            for (int k = 0; k < nC; ++k) {\n                double[] ak = a[k];\n                double wgk = weight * grad[k];\n                for (int l = 0; l < nC; ++l) {\n                    ak[l] += wgk * grad[l];\n                }\n            }\n        }\n\n        try {\n            // solve the linearized least squares problem\n            RealMatrix mA = new BlockRealMatrix(a);\n            DecompositionSolver solver = useLU ?\n                    new LUDecomposition(mA).getSolver() :\n                    new QRDecomposition(mA).getSolver();\n            final double[] dX = solver.solve(new ArrayRealVector(b, false)).toArray();\n            // update the estimated parameters\n            for (int i = 0; i < nC; ++i) {\n                currentPoint[i] += dX[i];\n            }\n        } catch (SingularMatrixException e) {\n            throw new ConvergenceException(LocalizedFormats.UNABLE_TO_SOLVE_SINGULAR_PROBLEM);\n        }\n\n        // Check convergence.\n        if (previous != null) {\n            converged = checker.converged(iter, previous, current);\n            if (converged) {\n                setCost(computeCost(currentResiduals));\n                return current;\n            }\n        }\n    }\n    // Must never happen.\n    throw new MathInternalError();\n}\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    public PointVectorValuePair doOptimize() {\n        checkParameters();\n\n        final ConvergenceChecker<PointVectorValuePair> checker\n            = getConvergenceChecker();\n\n        // Computation will be useless without a checker (see \"for-loop\").\n        if (checker == null) {\n            throw new NullArgumentException();\n        }\n\n        final double[] targetValues = getTarget();\n        final int nR = targetValues.length; // Number of observed data.\n\n        final RealMatrix weightMatrix = getWeight();\n        // Diagonal of the weight matrix.\n        final double[] residualsWeights = new double[nR];\n        for (int i = 0; i < nR; i++) {\n            residualsWeights[i] = weightMatrix.getEntry(i, i);\n        }\n\n        final double[] currentPoint = getStartPoint();\n        final int nC = currentPoint.length;\n\n        // iterate until convergence is reached\n        PointVectorValuePair current = null;\n        int iter = 0;\n        for (boolean converged = false; !converged;) {\n            ++iter;\n\n            // evaluate the objective function and its jacobian\n            PointVectorValuePair previous = current;\n            // Value of the objective function at \"currentPoint\".\n            final double[] currentObjective = computeObjectiveValue(currentPoint);\n            final double[] currentResiduals = computeResiduals(currentObjective);\n            final RealMatrix weightedJacobian = computeWeightedJacobian(currentPoint);\n            current = new PointVectorValuePair(currentPoint.clone(), currentObjective);\n\n            // build the linear problem\n            final double[]   b = new double[nC];\n            final double[][] a = new double[nC][nC];\n            for (int i = 0; i < nR; ++i) {\n\n                final double[] grad   = weightedJacobian.getRow(i);\n                final double weight   = residualsWeights[i];\n                final double residual = currentResiduals[i];\n\n                // compute the normal equation\n                final double wr = weight * residual;\n                for (int j = 0; j < nC; ++j) {\n                    b[j] += wr * grad[j];\n                }\n\n                // build the contribution matrix for measurement i\n                for (int k = 0; k < nC; ++k) {\n                    double[] ak = a[k];\n                    double wgk = weight * grad[k];\n                    for (int l = 0; l < nC; ++l) {\n                        ak[l] += wgk * grad[l];\n                    }\n                }\n            }\n\n            try {\n                // solve the linearized least squares problem\n                RealMatrix mA = new BlockRealMatrix(a);\n                DecompositionSolver solver = useLU ?\n                        new LUDecomposition(mA).getSolver() :\n                        new QRDecomposition(mA).getSolver();\n                final double[] dX = solver.solve(new ArrayRealVector(b, false)).toArray();\n                // update the estimated parameters\n                for (int i = 0; i < nC; ++i) {\n                    currentPoint[i] += dX[i];\n                }\n            } catch (SingularMatrixException e) {\n                throw new ConvergenceException(LocalizedFormats.UNABLE_TO_SOLVE_SINGULAR_PROBLEM);\n            }\n\n            // Check convergence.\n            if (previous != null) {\n                converged = checker.converged(iter, previous, current);\n                if (converged) {\n                    setCost(computeCost(currentResiduals));\n                    return current;\n                }\n            }\n        }\n        // Must never happen.\n        throw new MathInternalError();\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    public PointVectorValuePair doOptimize() {\n        checkParameters();\n\n        final ConvergenceChecker<PointVectorValuePair> checker\n            = getConvergenceChecker();\n\n        // Computation will be useless without a checker (see \"for-loop\").\n        if (checker == null) {\n            throw new NullArgumentException();\n        }\n\n        final double[] targetValues = getTarget();\n        final int nR = targetValues.length; // Number of observed data.\n\n        final RealMatrix weightMatrix = getWeight();\n        // Diagonal of the weight matrix.\n        final double[] residualsWeights = new double[nR];\n        for (int i = 0; i < nR; i++) {\n            residualsWeights[i] = weightMatrix.getEntry(i, i);\n        }\n\n        final double[] currentPoint = getStartPoint();\n        final int nC = currentPoint.length;\n\n        // iterate until convergence is reached\n        PointVectorValuePair current = null;\n        int iter = 0;\n        for (boolean converged = false; !converged;) {\n            ++iter;\n\n            // evaluate the objective function and its jacobian\n            PointVectorValuePair previous = current;\n            // Value of the objective function at \"currentPoint\".\n            final double[] currentObjective = computeObjectiveValue(currentPoint);\n            final double[] currentResiduals = computeResiduals(currentObjective);\n            final RealMatrix weightedJacobian = computeWeightedJacobian(currentPoint);\n            current = new PointVectorValuePair(currentPoint.clone(), currentObjective);\n\n            // build the linear problem\n            final double[]   b = new double[nC];\n            final double[][] a = new double[nC][nC];\n            for (int i = 0; i < nR; ++i) {\n\n                final double[] grad   = weightedJacobian.getRow(i);\n                final double weight   = residualsWeights[i];\n                final double residual = currentResiduals[i];\n\n                // compute the normal equation\n                final double wr = weight * residual;\n                for (int j = 0; j < nC; ++j) {\n                    b[j] += wr * grad[j];\n                }\n\n                // build the contribution matrix for measurement i\n                for (int k = 0; k < nC; ++k) {\n                    double[] ak = a[k];\n                    double wgk = weight * grad[k];\n                    for (int l = 0; l < nC; ++l) {\n                        ak[l] += wgk * grad[l];\n                    }\n                }\n            }\n\n            try {\n                // solve the linearized least squares problem\n                RealMatrix mA = new BlockRealMatrix(a);\n                DecompositionSolver solver = useLU ?\n                        new LUDecomposition(mA).getSolver() :\n                        new QRDecomposition(mA).getSolver();\n                final double[] dX = solver.solve(new ArrayRealVector(b, false)).toArray();\n                // update the estimated parameters\n                for (int i = 0; i < nC; ++i) {\n                    currentPoint[i] += dX[i];\n                }\n            } catch (SingularMatrixException e) {\n                throw new ConvergenceException(LocalizedFormats.UNABLE_TO_SOLVE_SINGULAR_PROBLEM);\n            }\n\n            // Check convergence.\n            if (previous != null) {\n                converged = checker.converged(iter, previous, current);\n                if (converged) {\n                    setCost(computeCost(currentResiduals));\n                    return current;\n                }\n            }\n        }\n        // Must never happen.\n        throw new MathInternalError();\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    public PointVectorValuePair doOptimize() {\n        checkParameters();\n\n        final ConvergenceChecker<PointVectorValuePair> checker\n            = getConvergenceChecker();\n\n        // Computation will be useless without a checker (see \"for-loop\").\n        if (checker == null) {\n            throw new NullArgumentException();\n        }\n\n        final double[] targetValues = getTarget();\n        final int nR = targetValues.length; // Number of observed data.\n\n        final RealMatrix weightMatrix = getWeight();\n        // Diagonal of the weight matrix.\n        final double[] residualsWeights = new double[nR];\n        for (int i = 0; i < nR; i++) {\n            residualsWeights[i] = weightMatrix.getEntry(i, i);\n        }\n\n        final double[] currentPoint = getStartPoint();\n        final int nC = currentPoint.length;\n\n        // iterate until convergence is reached\n        PointVectorValuePair current = null;\n        int iter = 0;\n        for (boolean converged = false; !converged;) {\n            ++iter;\n\n            // evaluate the objective function and its jacobian\n            PointVectorValuePair previous = current;\n            // Value of the objective function at \"currentPoint\".\n            final double[] currentObjective = computeObjectiveValue(currentPoint);\n            final double[] currentResiduals = computeResiduals(currentObjective);\n            final RealMatrix weightedJacobian = computeWeightedJacobian(currentPoint);\n            current = new PointVectorValuePair(currentPoint.clone(), currentObjective);\n\n            // build the linear problem\n            final double[]   b = new double[nC];\n            final double[][] a = new double[nC][nC];\n            for (int i = 0; i < nR; ++i) {\n\n                final double[] grad   = weightedJacobian.getRow(i);\n                final double weight   = residualsWeights[i];\n                final double residual = currentResiduals[i];\n\n                // compute the normal equation\n                final double wr = weight * residual;\n                for (int j = 0; j < nC; ++j) {\n                    b[j] += wr * grad[j];\n                }\n\n                // build the contribution matrix for measurement i\n                for (int k = 0; k < nC; ++k) {\n                    double[] ak = a[k];\n                    double wgk = weight * grad[k];\n                    for (int l = 0; l < nC; ++l) {\n                        ak[l] += wgk * grad[l];\n                    }\n                }\n            }\n\n            try {\n                // solve the linearized least squares problem\n                RealMatrix mA = new BlockRealMatrix(a);\n                DecompositionSolver solver = useLU ?\n                        new LUDecomposition(mA).getSolver() :\n                        new QRDecomposition(mA).getSolver();\n                final double[] dX = solver.solve(new ArrayRealVector(b, false)).toArray();\n                // update the estimated parameters\n                for (int i = 0; i < nC; ++i) {\n                    currentPoint[i] += dX[i];\n                }\n            } catch (SingularMatrixException e) {\n                throw new ConvergenceException(LocalizedFormats.UNABLE_TO_SOLVE_SINGULAR_PROBLEM);\n            }\n\n            // Check convergence.\n            if (previous != null) {\n                converged = checker.converged(iter, previous, current);\n                if (converged) {\n                    setCost(computeCost(currentResiduals));\n                    return current;\n                }\n            }\n        }\n        // Must never happen.\n        throw new MathInternalError();\n    }\n",
                            "\n/** {@inheritDoc} */\n\n@Override\npublic PointVectorValuePair doOptimize() {\n    checkParameters();\n\n    final ConvergenceChecker<PointVectorValuePair> checker\n        = getConvergenceChecker();\n\n    // Computation will be useless without a checker (see \"for-loop\").\n    if (checker == null) {\n        throw new NullArgumentException();\n    }\n\n    final double[] targetValues = getTarget();\n    final int nR = targetValues.length; // Number of observed data.\n\n    final RealMatrix weightMatrix = getWeight();\n    // Diagonal of the weight matrix.\n    final double[] residualsWeights = new double[nR];\n    for (int i = 0; i < nR; i++) {\n        residualsWeights[i] = weightMatrix.getEntry(i, i);\n    }\n\n    final double[] currentPoint = getStartPoint();\n    final int nC = currentPoint.length;\n\n    // iterate until convergence is reached\n    PointVectorValuePair current = null;\n    int iter = 0;\n    for (boolean converged = false; !converged;) {\n        ++iter;\n\n        // evaluate the objective function and its jacobian\n        PointVectorValuePair previous = current;\n        // Value of the objective function at \"currentPoint\".\n        final double[] currentObjective = computeObjectiveValue(currentPoint);\n        final double[] currentResiduals = computeResiduals(currentObjective);\n        final RealMatrix weightedJacobian = computeWeightedJacobian(currentPoint);\n        current = new PointVectorValuePair(currentPoint.clone(), currentObjective);\n\n        // build the linear problem\n        final double[]   b = new double[nC];\n        final double[][] a = new double[nC][nC];\n        for (int i = 0; i < nR; ++i) {\n\n            final double[] grad   = weightedJacobian.getRow(i);\n            final double weight   = residualsWeights[i];\n            final double residual = currentResiduals[i];\n\n            // compute the normal equation\n            final double wr = weight * residual;\n            for (int j = 0; j < nC; ++j) {\n                b[j] += wr * grad[j];\n            }\n\n            // build the contribution matrix for measurement i\n            for (int k = 0; k < nC; ++k) {\n                double[] ak = a[k];\n                double wgk = weight * grad[k];\n                for (int l = 0; l < nC; ++l) {\n                    ak[l] += wgk * grad[l];\n                }\n            }\n        }\n\n        try {\n            // solve the linearized least squares problem\n            RealMatrix mA = new BlockRealMatrix(a);\n            DecompositionSolver solver = useLU ?\n                    new LUDecomposition(mA).getSolver() :\n                    new QRDecomposition(mA).getSolver();\n            final double[] dX = solver.solve(new ArrayRealVector(b, false)).toArray();\n            // update the estimated parameters\n            for (int i = 0; i < nC; ++i) {\n                currentPoint[i] += dX[i];\n            }\n        } catch (SingularMatrixException e) {\n            throw new ConvergenceException(LocalizedFormats.UNABLE_TO_SOLVE_SINGULAR_PROBLEM);\n        }\n\n        // Check convergence.\n        if (previous != null) {\n            converged = checker.converged(iter, previous, current);\n            if (converged) {\n                setCost(computeCost(currentResiduals));\n                return current;\n            }\n        }\n    }\n    // Must never happen.\n    throw new MathInternalError();\n}\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    public PointVectorValuePair doOptimize() {\n        checkParameters();\n\n        final ConvergenceChecker<PointVectorValuePair> checker\n            = getConvergenceChecker();\n\n        // Computation will be useless without a checker (see \"for-loop\").\n        if (checker == null) {\n            throw new NullArgumentException();\n        }\n\n        final double[] targetValues = getTarget();\n        final int nR = targetValues.length; // Number of observed data.\n\n        final RealMatrix weightMatrix = getWeight();\n        // Diagonal of the weight matrix.\n        final double[] residualsWeights = new double[nR];\n        for (int i = 0; i < nR; i++) {\n            residualsWeights[i] = weightMatrix.getEntry(i, i);\n        }\n\n        final double[] currentPoint = getStartPoint();\n        final int nC = currentPoint.length;\n\n        // iterate until convergence is reached\n        PointVectorValuePair current = null;\n        int iter = 0;\n        for (boolean converged = false; !converged;) {\n            ++iter;\n\n            // evaluate the objective function and its jacobian\n            PointVectorValuePair previous = current;\n            // Value of the objective function at \"currentPoint\".\n            final double[] currentObjective = computeObjectiveValue(currentPoint);\n            final double[] currentResiduals = computeResiduals(currentObjective);\n            final RealMatrix weightedJacobian = computeWeightedJacobian(currentPoint);\n            current = new PointVectorValuePair(currentPoint.clone(), currentObjective);\n\n            // build the linear problem\n            final double[]   b = new double[nC];\n            final double[][] a = new double[nC][nC];\n            for (int i = 0; i < nR; ++i) {\n\n                final double[] grad   = weightedJacobian.getRow(i);\n                final double weight   = residualsWeights[i];\n                final double residual = currentResiduals[i];\n\n                // compute the normal equation\n                final double wr = weight * residual;\n                for (int j = 0; j < nC; ++j) {\n                    b[j] += wr * grad[j];\n                }\n\n                // build the contribution matrix for measurement i\n                for (int k = 0; k < nC; ++k) {\n                    double[] ak = a[k];\n                    double wgk = weight * grad[k];\n                    for (int l = 0; l < nC; ++l) {\n                        ak[l] += wgk * grad[l];\n                    }\n                }\n            }\n\n            try {\n                // solve the linearized least squares problem\n                RealMatrix mA = new BlockRealMatrix(a);\n                DecompositionSolver solver = useLU ?\n                        new LUDecomposition(mA).getSolver() :\n                        new QRDecomposition(mA).getSolver();\n                final double[] dX = solver.solve(new ArrayRealVector(b, false)).toArray();\n                // update the estimated parameters\n                for (int i = 0; i < nC; ++i) {\n                    currentPoint[i] += dX[i];\n                }\n            } catch (SingularMatrixException e) {\n                throw new ConvergenceException(LocalizedFormats.UNABLE_TO_SOLVE_SINGULAR_PROBLEM);\n            }\n\n            // Check convergence.\n            if (previous != null) {\n                converged = checker.converged(iter, previous, current);\n                if (converged) {\n                    setCost(computeCost(currentResiduals));\n                    return current;\n                }\n            }\n        }\n        // Must never happen.\n        throw new MathInternalError();\n    }\n\n"
                        ]
                    },
                    "2": {
                        "prompt": "You are an expert Java developer. Below is a buggy method from a large Java project. This method causes one or more test failures.\nYour task is to fix the bug in this method. Use the provided test failure(s) and relevant context to guide your reasoning.\n\n---\n\n## Buggy Method\n\n```java\n    /** {@inheritDoc} */\n\n    @Override\n    public PointVectorValuePair doOptimize() {\n        checkParameters();\n\n        final ConvergenceChecker<PointVectorValuePair> checker\n            = getConvergenceChecker();\n\n        // Computation will be useless without a checker (see \"for-loop\").\n        if (checker == null) {\n            throw new NullArgumentException();\n        }\n\n        final double[] targetValues = getTarget();\n        final int nR = targetValues.length; // Number of observed data.\n\n        final RealMatrix weightMatrix = getWeight();\n        // Diagonal of the weight matrix.\n        final double[] residualsWeights = new double[nR];\n        for (int i = 0; i < nR; i++) {\n            residualsWeights[i] = weightMatrix.getEntry(i, i);\n        }\n\n        final double[] currentPoint = getStartPoint();\n        final int nC = currentPoint.length;\n\n        // iterate until convergence is reached\n        PointVectorValuePair current = null;\n        int iter = 0;\n        for (boolean converged = false; !converged;) {\n            ++iter;\n\n            // evaluate the objective function and its jacobian\n            PointVectorValuePair previous = current;\n            // Value of the objective function at \"currentPoint\".\n            final double[] currentObjective = computeObjectiveValue(currentPoint);\n            final double[] currentResiduals = computeResiduals(currentObjective);\n            final RealMatrix weightedJacobian = computeWeightedJacobian(currentPoint);\n            current = new PointVectorValuePair(currentPoint, currentObjective);\n\n            // build the linear problem\n            final double[]   b = new double[nC];\n            final double[][] a = new double[nC][nC];\n            for (int i = 0; i < nR; ++i) {\n\n                final double[] grad   = weightedJacobian.getRow(i);\n                final double weight   = residualsWeights[i];\n                final double residual = currentResiduals[i];\n\n                // compute the normal equation\n                final double wr = weight * residual;\n                for (int j = 0; j < nC; ++j) {\n                    b[j] += wr * grad[j];\n                }\n\n                // build the contribution matrix for measurement i\n                for (int k = 0; k < nC; ++k) {\n                    double[] ak = a[k];\n                    double wgk = weight * grad[k];\n                    for (int l = 0; l < nC; ++l) {\n                        ak[l] += wgk * grad[l];\n                    }\n                }\n            }\n\n            try {\n                // solve the linearized least squares problem\n                RealMatrix mA = new BlockRealMatrix(a);\n                DecompositionSolver solver = useLU ?\n                        new LUDecomposition(mA).getSolver() :\n                        new QRDecomposition(mA).getSolver();\n                final double[] dX = solver.solve(new ArrayRealVector(b, false)).toArray();\n                // update the estimated parameters\n                for (int i = 0; i < nC; ++i) {\n                    currentPoint[i] += dX[i];\n                }\n            } catch (SingularMatrixException e) {\n                throw new ConvergenceException(LocalizedFormats.UNABLE_TO_SOLVE_SINGULAR_PROBLEM);\n            }\n\n            // Check convergence.\n            if (previous != null) {\n                converged = checker.converged(iter, previous, current);\n                if (converged) {\n                    setCost(computeCost(currentResiduals));\n                    return current;\n                }\n            }\n        }\n        // Must never happen.\n        throw new MathInternalError();\n    }\n\n```\n---\n\n## Failing Test Case(s)\n\n\n\n## \ud83d\udca1 Context from Project\n\n            \n```java\n@Test public void testParametersErrorMonteCarloObservations() { final double yError = 15; final double slope = 123.456; final double offset = -98.765; final RandomStraightLinePointGenerator lineGenerator = new RandomStraightLinePointGenerator(slope, offset, yError, -1e3, 1e4, 138577L); final int numObs = 100; final int numParams = 2; final SummaryStatistics[] paramsFoundByDirectSolution = new SummaryStatistics[numParams]; final SummaryStatistics[] sigmaEstimate = new SummaryStatistics[numParams]; for (int i = 0; i < numParams; i++) { paramsFoundByDirectSolution[i] = new SummaryStatistics(); sigmaEstimate[i] = new SummaryStatistics(); } final AbstractLeastSquaresOptimizer optim = new DummyOptimizer(); final double[] init = { slope, offset }; final int mcRepeat = MONTE_CARLO_RUNS; int mcCount = 0; while (mcCount < mcRepeat) { final Point2D.Double[] obs = lineGenerator.generate(numObs); final StraightLineProblem problem = new StraightLineProblem(yError); for (int i = 0; i < numObs; i++) { final Point2D.Double p = obs[i]; problem.addPoint(p.x, p.y); } final double[] regress = problem.solve(); final PointVectorValuePair optimum = optim.optimize(new MaxEval(Integer.MAX_VALUE), problem.getModelFunction(), problem.getModelFunctionJacobian(), new Target(problem.target()), new Weight(problem.weight()), new InitialGuess(init)); final double[] sigma = optim.computeSigma(optimum.getPoint(), 1e-14); for (int i = 0; i < numParams; i++) { paramsFoundByDirectSolution[i].addValue(regress[i]); sigmaEstimate[i].addValue(sigma[i]); } ++mcCount; } final String line = \\\"--------------------------------------------------------------\\\"; System.out.println(\\\" True value Mean Std deviation\\\"); for (int i = 0; i < numParams; i++) { System.out.println(line); System.out.println(\\\"Parameter #\\\" + i); StatisticalSummary s = paramsFoundByDirectSolution[i].getSummary(); System.out.printf(\\\" %+.6e %+.6e %+.6e\\n\\\", init[i], s.getMean(), s.getStandardDeviation()); s = sigmaEstimate[i].getSummary(); System.out.printf(\\\"sigma: %+.6e (%+.6e)\\n\\\", s.getMean(), s.getStandardDeviation()); } System.out.println(line); for (int i = 0; i < numParams; i++) { Assert.assertEquals(paramsFoundByDirectSolution[i].getSummary().getStandardDeviation(), sigmaEstimate[i].getSummary().getMean(), 8e-2); } }\n```\n\n            \n```java\n@Override public PointVectorValuePair doOptimize() { final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker(); if (checker == null) { throw new NullArgumentException(); } final double[] targetValues = getTarget(); final int nR = targetValues.length; final RealMatrix weightMatrix = getWeight(); final double[] residualsWeights = new double[nR]; for (int i = 0; i < nR; i++) { residualsWeights[i] = weightMatrix.getEntry(i, i); } final double[] currentPoint = getStartPoint(); final int nC = currentPoint.length; PointVectorValuePair current = null; int iter = 0; for (boolean converged = false; !converged; ) { ++iter; PointVectorValuePair previous = current; final double[] currentObjective = computeObjectiveValue(currentPoint); final double[] currentResiduals = computeResiduals(currentObjective); final RealMatrix weightedJacobian = computeWeightedJacobian(currentPoint); current = new PointVectorValuePair(currentPoint, currentObjective); final double[] b = new double[nC]; final double[][] a = new double[nC][nC]; for (int i = 0; i < nR; ++i) { final double[] grad = weightedJacobian.getRow(i); final double weight = residualsWeights[i]; final double residual = currentResiduals[i]; final double wr = weight * residual; for (int j = 0; j < nC; ++j) { b[j] += wr * grad[j]; } for (int k = 0; k < nC; ++k) { double[] ak = a[k]; double wgk = weight * grad[k]; for (int l = 0; l < nC; ++l) { ak[l] += wgk * grad[l]; } } } try { RealMatrix mA = new BlockRealMatrix(a); DecompositionSolver solver = useLU ? new LUDecomposition(mA).getSolver() : new QRDecomposition(mA).getSolver(); final double[] dX = solver.solve(new ArrayRealVector(b, false)).toArray(); for (int i = 0; i < nC; ++i) { currentPoint[i] += dX[i]; } } catch (SingularMatrixException e) { throw new ConvergenceException(LocalizedFormats.UNABLE_TO_SOLVE_SINGULAR_PROBLEM); } if (previous != null) { converged = checker.converged(iter, previous, current); if (converged) { cost = computeCost(currentResiduals); point = current.getPoint(); return current; } } } throw new MathInternalError(); }\n```\n\n            \n```java\nprivate void doTestMath753(final double shape, final double meanNoOF, final double sdNoOF, final double meanOF, final double sdOF, final String resourceName) throws IOException { final GammaDistribution distribution = new GammaDistribution(shape, 1.0); final SummaryStatistics statOld = new SummaryStatistics(); final SummaryStatistics statNewNoOF = new SummaryStatistics(); final SummaryStatistics statNewOF = new SummaryStatistics(); final InputStream resourceAsStream; resourceAsStream = this.getClass().getResourceAsStream(resourceName); Assert.assertNotNull(\\\"Could not find resource \\\" + resourceName, resourceAsStream); final BufferedReader in; in = new BufferedReader(new InputStreamReader(resourceAsStream)); try { for (String line = in.readLine(); line != null; line = in.readLine()) { if (line.startsWith(\\\"#\\\")) { continue; } final String[] tokens = line.split(\\\", \\\"); Assert.assertTrue(\\\"expected two floating-point values\\\", tokens.length == 2); final double x = Double.parseDouble(tokens[0]); final String msg = \\\"x = \\\" + x + \\\", shape = \\\" + shape + \\\", scale = 1.0\\\"; final double expected = Double.parseDouble(tokens[1]); final double ulp = FastMath.ulp(expected); final double actualOld = density(x, shape, 1.0); final double actualNew = distribution.density(x); final double errOld, errNew; errOld = FastMath.abs((actualOld - expected) / ulp); errNew = FastMath.abs((actualNew - expected) / ulp); if (Double.isNaN(actualOld) || Double.isInfinite(actualOld)) { Assert.assertFalse(msg, Double.isNaN(actualNew)); Assert.assertFalse(msg, Double.isInfinite(actualNew)); statNewOF.addValue(errNew); } else { statOld.addValue(errOld); statNewNoOF.addValue(errNew); } } if (statOld.getN() != 0) { final StringBuilder sb = new StringBuilder(\\\"shape = \\\"); sb.append(shape); sb.append(\\\", scale = 1.0\\n\\\"); sb.append(\\\"Old implementation\\n\\\"); sb.append(\\\"------------------\\n\\\"); sb.append(statOld.toString()); sb.append(\\\"New implementation\\n\\\"); sb.append(\\\"------------------\\n\\\"); sb.append(statNewNoOF.toString()); final String msg = sb.toString(); final double oldMin = statOld.getMin(); final double newMin = statNewNoOF.getMin(); Assert.assertTrue(msg, newMin <= oldMin); final double oldMax = statOld.getMax(); final double newMax = statNewNoOF.getMax(); Assert.assertTrue(msg, newMax <= oldMax); final double oldMean = statOld.getMean(); final double newMean = statNewNoOF.getMean(); Assert.assertTrue(msg, newMean <= oldMean); final double oldSd = statOld.getStandardDeviation(); final double newSd = statNewNoOF.getStandardDeviation(); Assert.assertTrue(msg, newSd <= oldSd); Assert.assertTrue(msg, newMean <= meanNoOF); Assert.assertTrue(msg, newSd <= sdNoOF); } if (statNewOF.getN() != 0) { final double newMean = statNewOF.getMean(); final double newSd = statNewOF.getStandardDeviation(); final StringBuilder sb = new StringBuilder(\\\"shape = \\\"); sb.append(shape); sb.append(\\\", scale = 1.0\\\"); sb.append(\\\", max. mean error (ulps) = \\\"); sb.append(meanOF); sb.append(\\\", actual mean error (ulps) = \\\"); sb.append(newMean); sb.append(\\\", max. sd of error (ulps) = \\\"); sb.append(sdOF); sb.append(\\\", actual sd of error (ulps) = \\\"); sb.append(newSd); final String msg = sb.toString(); Assert.assertTrue(msg, newMean <= meanOF); Assert.assertTrue(msg, newSd <= sdOF); } } catch (IOException e) { Assert.fail(e.getMessage()); } finally { in.close(); } }\n```\n\n            \n```java\n@Test public void testParametersErrorMonteCarloObservations() { final double yError = 15; final double slope = 123.456; final double offset = -98.765; final RandomStraightLinePointGenerator lineGenerator = new RandomStraightLinePointGenerator(slope, offset, yError, -1e3, 1e4, 138577L); final int numObs = 100; final int numParams = 2; final SummaryStatistics[] paramsFoundByDirectSolution = new SummaryStatistics[numParams]; final SummaryStatistics[] sigmaEstimate = new SummaryStatistics[numParams]; for (int i = 0; i < numParams; i++) { paramsFoundByDirectSolution[i] = new SummaryStatistics(); sigmaEstimate[i] = new SummaryStatistics(); } final AbstractLeastSquaresOptimizer optim = new DummyOptimizer(); final double[] init = { slope, offset }; final int mcRepeat = MONTE_CARLO_RUNS; int mcCount = 0; while (mcCount < mcRepeat) { final Point2D.Double[] obs = lineGenerator.generate(numObs); final StraightLineProblem problem = new StraightLineProblem(yError); for (int i = 0; i < numObs; i++) { final Point2D.Double p = obs[i]; problem.addPoint(p.x, p.y); } final double[] regress = problem.solve(); final PointVectorValuePair optimum = optim.optimize(Integer.MAX_VALUE, problem, problem.target(), problem.weight(), init); final double[] sigma = optim.computeSigma(optimum.getPoint(), 1e-14); for (int i = 0; i < numParams; i++) { paramsFoundByDirectSolution[i].addValue(regress[i]); sigmaEstimate[i].addValue(sigma[i]); } ++mcCount; } final String line = \\\"--------------------------------------------------------------\\\"; System.out.println(\\\" True value Mean Std deviation\\\"); for (int i = 0; i < numParams; i++) { System.out.println(line); System.out.println(\\\"Parameter #\\\" + i); StatisticalSummary s = paramsFoundByDirectSolution[i].getSummary(); System.out.printf(\\\" %+.6e %+.6e %+.6e\\n\\\", init[i], s.getMean(), s.getStandardDeviation()); s = sigmaEstimate[i].getSummary(); System.out.printf(\\\"sigma: %+.6e (%+.6e)\\n\\\", s.getMean(), s.getStandardDeviation()); } System.out.println(line); for (int i = 0; i < numParams; i++) { Assert.assertEquals(paramsFoundByDirectSolution[i].getSummary().getStandardDeviation(), sigmaEstimate[i].getSummary().getMean(), 8e-2); } }\n```\n\n            \n```java\n@Test public void testParametersErrorMonteCarloParameters() { final double yError = 15; final double slope = 123.456; final double offset = -98.765; final RandomStraightLinePointGenerator lineGenerator = new RandomStraightLinePointGenerator(slope, offset, yError, -1e3, 1e4, 13839013L); final int numObs = 10; final int numParams = 2; final Point2D.Double[] obs = lineGenerator.generate(numObs); final StraightLineProblem problem = new StraightLineProblem(yError); for (int i = 0; i < numObs; i++) { final Point2D.Double p = obs[i]; problem.addPoint(p.x, p.y); } final double[] regress = problem.solve(); final AbstractLeastSquaresOptimizer optim = new DummyOptimizer(); final double[] init = { slope, offset }; final double bestChi2N = getChi2N(optim, problem, regress); final double[] sigma = optim.computeSigma(regress, 1e-14); final int mcRepeat = MONTE_CARLO_RUNS; final int gridSize = (int) FastMath.sqrt(mcRepeat); final List<double[]> paramsAndChi2 = new ArrayList<double[]>(gridSize * gridSize); final double slopeRange = 10 * sigma[0]; final double offsetRange = 10 * sigma[1]; final double minSlope = slope - 0.5 * slopeRange; final double minOffset = offset - 0.5 * offsetRange; final double deltaSlope = slopeRange / gridSize; final double deltaOffset = offsetRange / gridSize; for (int i = 0; i < gridSize; i++) { final double s = minSlope + i * deltaSlope; for (int j = 0; j < gridSize; j++) { final double o = minOffset + j * deltaOffset; final double chi2N = getChi2N(optim, problem, new double[] { s, o }); paramsAndChi2.add(new double[] { s, o, chi2N }); } } final double chi2NPlusOne = bestChi2N + 1; int numLarger = 0; final String lineFmt = \\\"%+.10e %+.10e %.8e\\n\\\"; System.out.printf(lineFmt, regress[0], regress[1], bestChi2N); System.out.println(); for (double[] d : paramsAndChi2) { if (d[2] <= chi2NPlusOne) { System.out.printf(lineFmt, d[0], d[1], d[2]); } } System.out.println(); for (double[] d : paramsAndChi2) { if (d[2] > chi2NPlusOne) { ++numLarger; System.out.printf(lineFmt, d[0], d[1], d[2]); } } System.out.println(); System.out.println(\\\"# sigma=\\\" + Arrays.toString(sigma)); System.out.println(\\\"# \\\" + numLarger + \\\" sets filtered out\\\"); }\n```\n\n---\n\n## Your Goal\n\nFix the buggy method. Return only the fixed Java method. Do not include explanations or other text.\n",
                        "patches": [
                            "\n@Override\npublic PointVectorValuePair doOptimize() {\n    checkParameters();\n\n    final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n    if (checker == null) {\n        throw new NullArgumentException();\n    }\n\n    final double[] targetValues = getTarget();\n    final int nR = targetValues.length;\n\n    final RealMatrix weightMatrix = getWeight();\n    final double[] residualsWeights = new double[nR];\n    for (int i = 0; i < nR; i++) {\n        residualsWeights[i] = weightMatrix.getEntry(i, i);\n    }\n\n    final double[] currentPoint = getStartPoint();\n    final int nC = currentPoint.length;\n\n    PointVectorValuePair current = null;\n    int iter = 0;\n    for (boolean converged = false; !converged;) {\n        ++iter;\n\n        PointVectorValuePair previous = current;\n        final double[] currentObjective = computeObjectiveValue(currentPoint);\n        final double[] currentResiduals = computeResiduals(currentObjective);\n        final RealMatrix weightedJacobian = computeWeightedJacobian(currentPoint);\n        current = new PointVectorValuePair(currentPoint.clone(), currentObjective);\n\n        final double[] b = new double[nC];\n        final double[][] a = new double[nC][nC];\n        for (int i = 0; i < nR; ++i) {\n\n            final double[] grad = weightedJacobian.getRow(i);\n            final double weight = residualsWeights[i];\n            final double residual = currentResiduals[i];\n\n            final double wr = weight * residual;\n            for (int j = 0; j < nC; ++j) {\n                b[j] += wr * grad[j];\n            }\n\n            for (int k = 0; k < nC; ++k) {\n                double[] ak = a[k];\n                double wgk = weight * grad[k];\n                for (int l = 0; l < nC; ++l) {\n                    ak[l] += wgk * grad[l];\n                }\n            }\n        }\n\n        try {\n            RealMatrix mA = new BlockRealMatrix(a);\n            DecompositionSolver solver = useLU ?\n                    new LUDecomposition(mA).getSolver() :\n                    new QRDecomposition(mA).getSolver();\n            final double[] dX = solver.solve(new ArrayRealVector(b, false)).toArray();\n            for (int i = 0; i < nC; ++i) {\n                currentPoint[i] += dX[i];\n            }\n        } catch (SingularMatrixException e) {\n            throw new ConvergenceException(LocalizedFormats.UNABLE_TO_SOLVE_SINGULAR_PROBLEM);\n        }\n\n        if (previous != null) {\n            converged = checker.converged(iter, previous, current);\n            if (converged) {\n                setCost(computeCost(currentResiduals));\n                return current;\n            }\n        }\n    }\n    throw new MathInternalError();\n}\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    public PointVectorValuePair doOptimize() {\n        checkParameters();\n\n        final ConvergenceChecker<PointVectorValuePair> checker\n            = getConvergenceChecker();\n\n        // Computation will be useless without a checker (see \"for-loop\").\n        if (checker == null) {\n            throw new NullArgumentException();\n        }\n\n        final double[] targetValues = getTarget();\n        final int nR = targetValues.length; // Number of observed data.\n\n        final RealMatrix weightMatrix = getWeight();\n        // Diagonal of the weight matrix.\n        final double[] residualsWeights = new double[nR];\n        for (int i = 0; i < nR; i++) {\n            residualsWeights[i] = weightMatrix.getEntry(i, i);\n        }\n\n        final double[] currentPoint = getStartPoint();\n        final int nC = currentPoint.length;\n\n        // iterate until convergence is reached\n        PointVectorValuePair current = null;\n        int iter = 0;\n        for (boolean converged = false; !converged;) {\n            ++iter;\n\n            // evaluate the objective function and its jacobian\n            PointVectorValuePair previous = current;\n            // Value of the objective function at \"currentPoint\".\n            final double[] currentObjective = computeObjectiveValue(currentPoint);\n            final double[] currentResiduals = computeResiduals(currentObjective);\n            final RealMatrix weightedJacobian = computeWeightedJacobian(currentPoint);\n            current = new PointVectorValuePair(currentPoint.clone(), currentObjective);\n\n            // build the linear problem\n            final double[]   b = new double[nC];\n            final double[][] a = new double[nC][nC];\n            for (int i = 0; i < nR; ++i) {\n\n                final double[] grad   = weightedJacobian.getRow(i);\n                final double weight   = residualsWeights[i];\n                final double residual = currentResiduals[i];\n\n                // compute the normal equation\n                final double wr = weight * residual;\n                for (int j = 0; j < nC; ++j) {\n                    b[j] += wr * grad[j];\n                }\n\n                // build the contribution matrix for measurement i\n                for (int k = 0; k < nC; ++k) {\n                    double[] ak = a[k];\n                    double wgk = weight * grad[k];\n                    for (int l = 0; l < nC; ++l) {\n                        ak[l] += wgk * grad[l];\n                    }\n                }\n            }\n\n            try {\n                // solve the linearized least squares problem\n                RealMatrix mA = new BlockRealMatrix(a);\n                DecompositionSolver solver = useLU ?\n                        new LUDecomposition(mA).getSolver() :\n                        new QRDecomposition(mA).getSolver();\n                final double[] dX = solver.solve(new ArrayRealVector(b, false)).toArray();\n                // update the estimated parameters\n                for (int i = 0; i < nC; ++i) {\n                    currentPoint[i] += dX[i];\n                }\n            } catch (SingularMatrixException e) {\n                throw new ConvergenceException(LocalizedFormats.UNABLE_TO_SOLVE_SINGULAR_PROBLEM);\n            }\n\n            // Check convergence.\n            if (previous != null) {\n                converged = checker.converged(iter, previous, current);\n                if (converged) {\n                    setCost(computeCost(currentResiduals));\n                    return current;\n                }\n            }\n        }\n        // Must never happen.\n        throw new MathInternalError();\n    }\n",
                            "\n@Override\npublic PointVectorValuePair doOptimize() {\n    checkParameters();\n\n    final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n\n    if (checker == null) {\n        throw new NullArgumentException();\n    }\n\n    final double[] targetValues = getTarget();\n    final int nR = targetValues.length;\n\n    final RealMatrix weightMatrix = getWeight();\n    final double[] residualsWeights = new double[nR];\n    for (int i = 0; i < nR; i++) {\n        residualsWeights[i] = weightMatrix.getEntry(i, i);\n    }\n\n    final double[] currentPoint = getStartPoint();\n    final int nC = currentPoint.length;\n\n    PointVectorValuePair current = null;\n    int iter = 0;\n    for (boolean converged = false; !converged;) {\n        ++iter;\n        PointVectorValuePair previous = current;\n        final double[] currentObjective = computeObjectiveValue(currentPoint);\n        final double[] currentResiduals = computeResiduals(currentObjective);\n        final RealMatrix weightedJacobian = computeWeightedJacobian(currentPoint);\n        current = new PointVectorValuePair(currentPoint.clone(), currentObjective);\n\n        final double[] b = new double[nC];\n        final double[][] a = new double[nC][nC];\n        for (int i = 0; i < nR; ++i) {\n            final double[] grad = weightedJacobian.getRow(i);\n            final double weight = residualsWeights[i];\n            final double residual = currentResiduals[i];\n\n            final double wr = weight * residual;\n            for (int j = 0; j < nC; ++j) {\n                b[j] += wr * grad[j];\n            }\n            \n            for (int k = 0; k < nC; ++k) {\n                double[] ak = a[k];\n                double wgk = weight * grad[k];\n                for (int l = 0; l < nC; ++l) {\n                    ak[l] += wgk * grad[l];\n                }\n            }\n        }\n\n        try {\n            RealMatrix mA = new BlockRealMatrix(a);\n            DecompositionSolver solver = useLU ? \n                    new LUDecomposition(mA).getSolver() : \n                    new QRDecomposition(mA).getSolver();\n            final double[] dX = solver.solve(new ArrayRealVector(b, false)).toArray();\n            for (int i = 0; i < nC; ++i) {\n                currentPoint[i] += dX[i];\n            }\n        } catch (SingularMatrixException e) {\n            throw new ConvergenceException(LocalizedFormats.UNABLE_TO_SOLVE_SINGULAR_PROBLEM);\n        }\n\n        if (previous != null) {\n            converged = checker.converged(iter, previous, current);\n            if (converged) {\n                setCost(computeCost(currentResiduals));\n                return current;\n            }\n        }\n    }\n\n    throw new MathInternalError();\n}\n",
                            "\n@Override\npublic PointVectorValuePair doOptimize() {\n    checkParameters();\n\n    final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n\n    // Computation will be useless without a checker (see \"for-loop\").\n    if (checker == null) {\n        throw new NullArgumentException();\n    }\n\n    final double[] targetValues = getTarget();\n    final int nR = targetValues.length; // Number of observed data.\n\n    final RealMatrix weightMatrix = getWeight();\n    // Diagonal of the weight matrix.\n    final double[] residualsWeights = new double[nR];\n    for (int i = 0; i < nR; i++) {\n        residualsWeights[i] = weightMatrix.getEntry(i, i);\n    }\n\n    final double[] currentPoint = getStartPoint();\n    final int nC = currentPoint.length;\n\n    // iterate until convergence is reached\n    PointVectorValuePair current = null;\n    int iter = 0;\n    for (boolean converged = false; !converged;) {\n        ++iter;\n\n        // evaluate the objective function and its jacobian\n        PointVectorValuePair previous = current;\n        // Value of the objective function at \"currentPoint\".\n        final double[] currentObjective = computeObjectiveValue(currentPoint);\n        final double[] currentResiduals = computeResiduals(currentObjective);\n        final RealMatrix weightedJacobian = computeWeightedJacobian(currentPoint);\n        current = new PointVectorValuePair(currentPoint.clone(), currentObjective);\n\n        // build the linear problem\n        final double[] b = new double[nC];\n        final double[][] a = new double[nC][nC];\n        for (int i = 0; i < nR; ++i) {\n\n            final double[] grad = weightedJacobian.getRow(i);\n            final double weight = residualsWeights[i];\n            final double residual = currentResiduals[i];\n\n            // compute the normal equation\n            final double wr = weight * residual;\n            for (int j = 0; j < nC; ++j) {\n                b[j] += wr * grad[j];\n            }\n\n            // build the contribution matrix for measurement i\n            for (int k = 0; k < nC; ++k) {\n                double[] ak = a[k];\n                double wgk = weight * grad[k];\n                for (int l = 0; l < nC; ++l) {\n                    ak[l] += wgk * grad[l];\n                }\n            }\n        }\n\n        try {\n            // solve the linearized least squares problem\n            RealMatrix mA = new BlockRealMatrix(a);\n            DecompositionSolver solver = useLU ?\n                    new LUDecomposition(mA).getSolver() :\n                    new QRDecomposition(mA).getSolver();\n            final double[] dX = solver.solve(new ArrayRealVector(b, false)).toArray();\n            // update the estimated parameters\n            for (int i = 0; i < nC; ++i) {\n                currentPoint[i] += dX[i];\n            }\n        } catch (SingularMatrixException e) {\n            throw new ConvergenceException(LocalizedFormats.UNABLE_TO_SOLVE_SINGULAR_PROBLEM);\n        }\n\n        // Check convergence.\n        if (previous != null) {\n            converged = checker.converged(iter, previous, current);\n            if (converged) {\n                setCost(computeCost(currentResiduals));\n                return current;\n            }\n        }\n    }\n    // Must never happen.\n    throw new MathInternalError();\n}\n",
                            "\n@Override\npublic PointVectorValuePair doOptimize() {\n    checkParameters();\n\n    final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n\n    if (checker == null) {\n        throw new NullArgumentException();\n    }\n\n    final double[] targetValues = getTarget();\n    final int nR = targetValues.length;\n\n    final RealMatrix weightMatrix = getWeight();\n    final double[] residualsWeights = new double[nR];\n    for (int i = 0; i < nR; i++) {\n        residualsWeights[i] = weightMatrix.getEntry(i, i);\n    }\n\n    final double[] currentPoint = getStartPoint();\n    final int nC = currentPoint.length;\n\n    PointVectorValuePair current = null;\n    int iter = 0;\n    for (boolean converged = false; !converged;) {\n        ++iter;\n\n        PointVectorValuePair previous = current;\n        final double[] currentObjective = computeObjectiveValue(currentPoint);\n        final double[] currentResiduals = computeResiduals(currentObjective);\n        final RealMatrix weightedJacobian = computeWeightedJacobian(currentPoint);\n        current = new PointVectorValuePair(currentPoint.clone(), currentObjective);\n\n        final double[] b = new double[nC];\n        final double[][] a = new double[nC][nC];\n        for (int i = 0; i < nR; ++i) {\n            final double[] grad = weightedJacobian.getRow(i);\n            final double weight = residualsWeights[i];\n            final double residual = currentResiduals[i];\n\n            final double wr = weight * residual;\n            for (int j = 0; j < nC; ++j) {\n                b[j] += wr * grad[j];\n            }\n\n            for (int k = 0; k < nC; ++k) {\n                double[] ak = a[k];\n                double wgk = weight * grad[k];\n                for (int l = 0; l < nC; ++l) {\n                    ak[l] += wgk * grad[l];\n                }\n            }\n        }\n\n        try {\n            RealMatrix mA = new BlockRealMatrix(a);\n            DecompositionSolver solver = useLU ? \n                    new LUDecomposition(mA).getSolver() : \n                    new QRDecomposition(mA).getSolver();\n            final double[] dX = solver.solve(new ArrayRealVector(b, false)).toArray();\n            for (int i = 0; i < nC; ++i) {\n                currentPoint[i] += dX[i];\n            }\n        } catch (SingularMatrixException e) {\n            throw new ConvergenceException(LocalizedFormats.UNABLE_TO_SOLVE_SINGULAR_PROBLEM);\n        }\n\n        if (previous != null) {\n            converged = checker.converged(iter, previous, current);\n            if (converged) {\n                setCost(computeCost(currentResiduals));\n                return new PointVectorValuePair(current.getPoint(), current.getValue());\n            }\n        }\n    }\n\n    throw new MathInternalError();\n}\n",
                            "\n@Override\npublic PointVectorValuePair doOptimize() {\n    checkParameters();\n\n    final ConvergenceChecker<PointVectorValuePair> checker\n        = getConvergenceChecker();\n\n    // Computation will be useless without a checker (see \"for-loop\").\n    if (checker == null) {\n        throw new NullArgumentException();\n    }\n\n    final double[] targetValues = getTarget();\n    final int nR = targetValues.length; // Number of observed data.\n\n    final RealMatrix weightMatrix = getWeight();\n    // Diagonal of the weight matrix.\n    final double[] residualsWeights = new double[nR];\n    for (int i = 0; i < nR; i++) {\n        residualsWeights[i] = weightMatrix.getEntry(i, i);\n    }\n\n    final double[] currentPoint = getStartPoint();\n    final int nC = currentPoint.length;\n\n    // iterate until convergence is reached\n    PointVectorValuePair current = null;\n    int iter = 0;\n    for (boolean converged = false; !converged;) {\n        ++iter;\n\n        // evaluate the objective function and its jacobian\n        PointVectorValuePair previous = current;\n        // Value of the objective function at \"currentPoint\".\n        final double[] currentObjective = computeObjectiveValue(currentPoint);\n        final double[] currentResiduals = computeResiduals(currentObjective);\n        final RealMatrix weightedJacobian = computeWeightedJacobian(currentPoint);\n        current = new PointVectorValuePair(currentPoint.clone(), currentObjective);\n\n        // build the linear problem\n        final double[]   b = new double[nC];\n        final double[][] a = new double[nC][nC];\n        for (int i = 0; i < nR; ++i) {\n\n            final double[] grad   = weightedJacobian.getRow(i);\n            final double weight   = residualsWeights[i];\n            final double residual = currentResiduals[i];\n\n            // compute the normal equation\n            final double wr = weight * residual;\n            for (int j = 0; j < nC; ++j) {\n                b[j] += wr * grad[j];\n            }\n\n            // build the contribution matrix for measurement i\n            for (int k = 0; k < nC; ++k) {\n                double[] ak = a[k];\n                double wgk = weight * grad[k];\n                for (int l = 0; l < nC; ++l) {\n                    ak[l] += wgk * grad[l];\n                }\n            }\n        }\n\n        try {\n            // solve the linearized least squares problem\n            RealMatrix mA = new BlockRealMatrix(a);\n            DecompositionSolver solver = useLU ?\n                    new LUDecomposition(mA).getSolver() :\n                    new QRDecomposition(mA).getSolver();\n            final double[] dX = solver.solve(new ArrayRealVector(b, false)).toArray();\n            // update the estimated parameters\n            for (int i = 0; i < nC; ++i) {\n                currentPoint[i] += dX[i];\n            }\n        } catch (SingularMatrixException e) {\n            throw new ConvergenceException(LocalizedFormats.UNABLE_TO_SOLVE_SINGULAR_PROBLEM);\n        }\n\n        // Check convergence.\n        if (previous != null) {\n            converged = checker.converged(iter, previous, current);\n            if (converged) {\n                setCost(computeCost(currentResiduals));\n                return current;\n            }\n        }\n    }\n    // Must never happen.\n    throw new MathInternalError();\n}\n",
                            "\n@Override\npublic PointVectorValuePair doOptimize() {\n    checkParameters();\n\n    final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n\n    if (checker == null) {\n        throw new NullArgumentException();\n    }\n\n    final double[] targetValues = getTarget();\n    final int nR = targetValues.length; // Number of observed data.\n    final RealMatrix weightMatrix = getWeight();\n    final double[] residualsWeights = new double[nR];\n    for (int i = 0; i < nR; i++) {\n        residualsWeights[i] = weightMatrix.getEntry(i, i);\n    }\n\n    final double[] currentPoint = getStartPoint();\n    final int nC = currentPoint.length;\n\n    PointVectorValuePair current = null;\n    int iter = 0;\n    for (boolean converged = false; !converged;) {\n        ++iter;\n\n        PointVectorValuePair previous = current;\n        final double[] currentObjective = computeObjectiveValue(currentPoint);\n        final double[] currentResiduals = computeResiduals(currentObjective);\n        final RealMatrix weightedJacobian = computeWeightedJacobian(currentPoint);\n        current = new PointVectorValuePair(currentPoint.clone(), currentObjective);\n\n        final double[] b = new double[nC];\n        final double[][] a = new double[nC][nC];\n        for (int i = 0; i < nR; ++i) {\n            final double[] grad = weightedJacobian.getRow(i);\n            final double weight = residualsWeights[i];\n            final double residual = currentResiduals[i];\n\n            final double wr = weight * residual;\n            for (int j = 0; j < nC; ++j) {\n                b[j] += wr * grad[j];\n            }\n\n            for (int k = 0; k < nC; ++k) {\n                double[] ak = a[k];\n                double wgk = weight * grad[k];\n                for (int l = 0; l < nC; ++l) {\n                    ak[l] += wgk * grad[l];\n                }\n            }\n        }\n\n        try {\n            RealMatrix mA = new BlockRealMatrix(a);\n            DecompositionSolver solver = useLU ?\n                new LUDecomposition(mA).getSolver() :\n                new QRDecomposition(mA).getSolver();\n            final double[] dX = solver.solve(new ArrayRealVector(b, false)).toArray();\n            for (int i = 0; i < nC; ++i) {\n                currentPoint[i] += dX[i];\n            }\n        } catch (SingularMatrixException e) {\n            throw new ConvergenceException(LocalizedFormats.UNABLE_TO_SOLVE_SINGULAR_PROBLEM);\n        }\n\n        if (previous != null) {\n            converged = checker.converged(iter, previous, current);\n            if (converged) {\n                setCost(computeCost(currentResiduals));\n                return current;\n            }\n        }\n    }\n    throw new MathInternalError();\n}\n",
                            "\n@Override\npublic PointVectorValuePair doOptimize() {\n    checkParameters();\n\n    final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n\n    // Computation will be useless without a checker (see \"for-loop\").\n    if (checker == null) {\n        throw new NullArgumentException();\n    }\n\n    final double[] targetValues = getTarget();\n    final int nR = targetValues.length; // Number of observed data.\n\n    final RealMatrix weightMatrix = getWeight();\n    // Diagonal of the weight matrix.\n    final double[] residualsWeights = new double[nR];\n    for (int i = 0; i < nR; i++) {\n        residualsWeights[i] = weightMatrix.getEntry(i, i);\n    }\n\n    final double[] currentPoint = getStartPoint();\n    final int nC = currentPoint.length;\n\n    // iterate until convergence is reached\n    PointVectorValuePair current = null;\n    int iter = 0;\n    for (boolean converged = false; !converged; ) {\n        ++iter;\n\n        // evaluate the objective function and its jacobian\n        PointVectorValuePair previous = current;\n        // Value of the objective function at \"currentPoint\".\n        final double[] currentObjective = computeObjectiveValue(currentPoint);\n        final double[] currentResiduals = computeResiduals(currentObjective);\n        final RealMatrix weightedJacobian = computeWeightedJacobian(currentPoint);\n        current = new PointVectorValuePair(currentPoint.clone(), currentObjective);\n\n        // Build the linear problem\n        final double[] b = new double[nC];\n        final double[][] a = new double[nC][nC];\n        for (int i = 0; i < nR; ++i) {\n\n            final double[] grad = weightedJacobian.getRow(i);\n            final double weight = residualsWeights[i];\n            final double residual = currentResiduals[i];\n\n            // Compute the normal equation\n            final double wr = weight * residual;\n            for (int j = 0; j < nC; ++j) {\n                b[j] += wr * grad[j];\n            }\n\n            // Build the contribution matrix for measurement i\n            for (int k = 0; k < nC; ++k) {\n                double[] ak = a[k];\n                double wgk = weight * grad[k];\n                for (int l = 0; l < nC; ++l) {\n                    ak[l] += wgk * grad[l];\n                }\n            }\n        }\n\n        try {\n            // Solve the linearized least squares problem\n            RealMatrix mA = new BlockRealMatrix(a);\n            DecompositionSolver solver = useLU ?\n                    new LUDecomposition(mA).getSolver() :\n                    new QRDecomposition(mA).getSolver();\n            final double[] dX = solver.solve(new ArrayRealVector(b, false)).toArray();\n            // Update the estimated parameters\n            for (int i = 0; i < nC; ++i) {\n                currentPoint[i] += dX[i];\n            }\n        } catch (SingularMatrixException e) {\n            throw new ConvergenceException(LocalizedFormats.UNABLE_TO_SOLVE_SINGULAR_PROBLEM);\n        }\n\n        // Check convergence.\n        if (previous != null) {\n            converged = checker.converged(iter, previous, current);\n            if (converged) {\n                setCost(computeCost(currentResiduals));\n                return current;\n            }\n        }\n    }\n    // Must never happen.\n    throw new MathInternalError();\n}\n",
                            "\n@Override\npublic PointVectorValuePair doOptimize() {\n    checkParameters();\n\n    final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n\n    // Computation will be useless without a checker (see \"for-loop\").\n    if (checker == null) {\n        throw new NullArgumentException();\n    }\n\n    final double[] targetValues = getTarget();\n    final int nR = targetValues.length; // Number of observed data.\n\n    final RealMatrix weightMatrix = getWeight();\n    // Diagonal of the weight matrix.\n    final double[] residualsWeights = new double[nR];\n    for (int i = 0; i < nR; i++) {\n        residualsWeights[i] = weightMatrix.getEntry(i, i);\n    }\n\n    final double[] currentPoint = getStartPoint();\n    final int nC = currentPoint.length;\n\n    // iterate until convergence is reached\n    PointVectorValuePair current = null;\n    int iter = 0;\n    for (boolean converged = false; !converged;) {\n        ++iter;\n\n        // evaluate the objective function and its jacobian\n        PointVectorValuePair previous = current;\n        // Value of the objective function at \"currentPoint\".\n        final double[] currentObjective = computeObjectiveValue(currentPoint);\n        final double[] currentResiduals = computeResiduals(currentObjective);\n        final RealMatrix weightedJacobian = computeWeightedJacobian(currentPoint);\n        current = new PointVectorValuePair(currentPoint.clone(), currentObjective);\n        \n        // build the linear problem\n        final double[] b = new double[nC];\n        final double[][] a = new double[nC][nC];\n        for (int i = 0; i < nR; ++i) {\n\n            final double[] grad = weightedJacobian.getRow(i);\n            final double weight = residualsWeights[i];\n            final double residual = currentResiduals[i];\n\n            // compute the normal equation\n            final double wr = weight * residual;\n            for (int j = 0; j < nC; ++j) {\n                b[j] += wr * grad[j];\n            }\n\n            // build the contribution matrix for measurement i\n            for (int k = 0; k < nC; ++k) {\n                double[] ak = a[k];\n                double wgk = weight * grad[k];\n                for (int l = 0; l < nC; ++l) {\n                    ak[l] += wgk * grad[l];\n                }\n            }\n        }\n\n        try {\n            // solve the linearized least squares problem\n            RealMatrix mA = new BlockRealMatrix(a);\n            DecompositionSolver solver = useLU ?\n                    new LUDecomposition(mA).getSolver() :\n                    new QRDecomposition(mA).getSolver();\n            final double[] dX = solver.solve(new ArrayRealVector(b, false)).toArray();\n            // update the estimated parameters\n            for (int i = 0; i < nC; ++i) {\n                currentPoint[i] += dX[i];\n            }\n        } catch (SingularMatrixException e) {\n            throw new ConvergenceException(LocalizedFormats.UNABLE_TO_SOLVE_SINGULAR_PROBLEM);\n        }\n\n        // Check convergence.\n        if (previous != null) {\n            converged = checker.converged(iter, previous, current);\n            if (converged) {\n                setCost(computeCost(currentResiduals));\n                return current;\n            }\n        }\n    }\n    // Must never happen.\n    throw new MathInternalError();\n}\n",
                            "\n/** {@inheritDoc} */\n\n@Override\npublic PointVectorValuePair doOptimize() {\n    checkParameters();\n\n    final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n\n    if (checker == null) {\n        throw new NullArgumentException();\n    }\n\n    final double[] targetValues = getTarget();\n    final int nR = targetValues.length;\n\n    final RealMatrix weightMatrix = getWeight();\n    final double[] residualsWeights = new double[nR];\n    for (int i = 0; i < nR; i++) {\n        residualsWeights[i] = weightMatrix.getEntry(i, i);\n    }\n\n    final double[] currentPoint = getStartPoint();\n    final int nC = currentPoint.length;\n\n    PointVectorValuePair current = null;\n    int iter = 0;\n    for (boolean converged = false; !converged;) {\n        ++iter;\n\n        PointVectorValuePair previous = current;\n        final double[] currentObjective = computeObjectiveValue(currentPoint);\n        final double[] currentResiduals = computeResiduals(currentObjective);\n        final RealMatrix weightedJacobian = computeWeightedJacobian(currentPoint);\n        current = new PointVectorValuePair(currentPoint.clone(), currentObjective);\n\n        final double[] b = new double[nC];\n        final double[][] a = new double[nC][nC];\n        for (int i = 0; i < nR; ++i) {\n            final double[] grad = weightedJacobian.getRow(i);\n            final double weight = residualsWeights[i];\n            final double residual = currentResiduals[i];\n\n            final double wr = weight * residual;\n            for (int j = 0; j < nC; ++j) {\n                b[j] += wr * grad[j];\n            }\n\n            for (int k = 0; k < nC; ++k) {\n                double[] ak = a[k];\n                double wgk = weight * grad[k];\n                for (int l = 0; l < nC; ++l) {\n                    ak[l] += wgk * grad[l];\n                }\n            }\n        }\n\n        try {\n            RealMatrix mA = new BlockRealMatrix(a);\n            DecompositionSolver solver = useLU ?\n                    new LUDecomposition(mA).getSolver() :\n                    new QRDecomposition(mA).getSolver();\n            final double[] dX = solver.solve(new ArrayRealVector(b, false)).toArray();\n            for (int i = 0; i < nC; ++i) {\n                currentPoint[i] += dX[i];\n            }\n        } catch (SingularMatrixException e) {\n            throw new ConvergenceException(LocalizedFormats.UNABLE_TO_SOLVE_SINGULAR_PROBLEM);\n        }\n\n        if (previous != null) {\n            converged = checker.converged(iter, previous, current);\n            if (converged) {\n                setCost(computeCost(currentResiduals));\n                return current;\n            }\n        }\n    }\n    throw new MathInternalError();\n}\n"
                        ]
                    }
                }
            },
            {
                "path": "src/main/java/org/apache/commons/math3/optim/nonlinear/vector/jacobian/LevenbergMarquardtOptimizer.java",
                "start_loc": 283,
                "end_loc": 534,
                "buggy_function": "    @Override\n    protected PointVectorValuePair doOptimize() {\n        checkParameters();\n\n        final int nR = getTarget().length; // Number of observed data.\n        final double[] currentPoint = getStartPoint();\n        final int nC = currentPoint.length; // Number of parameters.\n\n        // arrays shared with the other private methods\n        solvedCols  = FastMath.min(nR, nC);\n        diagR       = new double[nC];\n        jacNorm     = new double[nC];\n        beta        = new double[nC];\n        permutation = new int[nC];\n        lmDir       = new double[nC];\n\n        // local point\n        double   delta   = 0;\n        double   xNorm   = 0;\n        double[] diag    = new double[nC];\n        double[] oldX    = new double[nC];\n        double[] oldRes  = new double[nR];\n        double[] oldObj  = new double[nR];\n        double[] qtf     = new double[nR];\n        double[] work1   = new double[nC];\n        double[] work2   = new double[nC];\n        double[] work3   = new double[nC];\n\n        final RealMatrix weightMatrixSqrt = getWeightSquareRoot();\n\n        // Evaluate the function at the starting point and calculate its norm.\n        double[] currentObjective = computeObjectiveValue(currentPoint);\n        double[] currentResiduals = computeResiduals(currentObjective);\n        PointVectorValuePair current = new PointVectorValuePair(currentPoint, currentObjective);\n        double currentCost = computeCost(currentResiduals);\n\n        // Outer loop.\n        lmPar = 0;\n        boolean firstIteration = true;\n        int iter = 0;\n        final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n        while (true) {\n            ++iter;\n            final PointVectorValuePair previous = current;\n\n            // QR decomposition of the jacobian matrix\n            qrDecomposition(computeWeightedJacobian(currentPoint));\n\n            weightedResidual = weightMatrixSqrt.operate(currentResiduals);\n            for (int i = 0; i < nR; i++) {\n                qtf[i] = weightedResidual[i];\n            }\n\n            // compute Qt.res\n            qTy(qtf);\n\n            // now we don't need Q anymore,\n            // so let jacobian contain the R matrix with its diagonal elements\n            for (int k = 0; k < solvedCols; ++k) {\n                int pk = permutation[k];\n                weightedJacobian[k][pk] = diagR[pk];\n            }\n\n            if (firstIteration) {\n                // scale the point according to the norms of the columns\n                // of the initial jacobian\n                xNorm = 0;\n                for (int k = 0; k < nC; ++k) {\n                    double dk = jacNorm[k];\n                    if (dk == 0) {\n                        dk = 1.0;\n                    }\n                    double xk = dk * currentPoint[k];\n                    xNorm  += xk * xk;\n                    diag[k] = dk;\n                }\n                xNorm = FastMath.sqrt(xNorm);\n\n                // initialize the step bound delta\n                delta = (xNorm == 0) ? initialStepBoundFactor : (initialStepBoundFactor * xNorm);\n            }\n\n            // check orthogonality between function vector and jacobian columns\n            double maxCosine = 0;\n            if (currentCost != 0) {\n                for (int j = 0; j < solvedCols; ++j) {\n                    int    pj = permutation[j];\n                    double s  = jacNorm[pj];\n                    if (s != 0) {\n                        double sum = 0;\n                        for (int i = 0; i <= j; ++i) {\n                            sum += weightedJacobian[i][pj] * qtf[i];\n                        }\n                        maxCosine = FastMath.max(maxCosine, FastMath.abs(sum) / (s * currentCost));\n                    }\n                }\n            }\n            if (maxCosine <= orthoTolerance) {\n                // Convergence has been reached.\n                setCost(currentCost);\n                return current;\n            }\n\n            // rescale if necessary\n            for (int j = 0; j < nC; ++j) {\n                diag[j] = FastMath.max(diag[j], jacNorm[j]);\n            }\n\n            // Inner loop.\n            for (double ratio = 0; ratio < 1.0e-4;) {\n\n                // save the state\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    oldX[pj] = currentPoint[pj];\n                }\n                final double previousCost = currentCost;\n                double[] tmpVec = weightedResidual;\n                weightedResidual = oldRes;\n                oldRes    = tmpVec;\n                tmpVec    = currentObjective;\n                currentObjective = oldObj;\n                oldObj    = tmpVec;\n\n                // determine the Levenberg-Marquardt parameter\n                determineLMParameter(qtf, delta, diag, work1, work2, work3);\n\n                // compute the new point and the norm of the evolution direction\n                double lmNorm = 0;\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    lmDir[pj] = -lmDir[pj];\n                    currentPoint[pj] = oldX[pj] + lmDir[pj];\n                    double s = diag[pj] * lmDir[pj];\n                    lmNorm  += s * s;\n                }\n                lmNorm = FastMath.sqrt(lmNorm);\n                // on the first iteration, adjust the initial step bound.\n                if (firstIteration) {\n                    delta = FastMath.min(delta, lmNorm);\n                }\n\n                // Evaluate the function at x + p and calculate its norm.\n                currentObjective = computeObjectiveValue(currentPoint);\n                currentResiduals = computeResiduals(currentObjective);\n                current = new PointVectorValuePair(currentPoint, currentObjective);\n                currentCost = computeCost(currentResiduals);\n\n                // compute the scaled actual reduction\n                double actRed = -1.0;\n                if (0.1 * currentCost < previousCost) {\n                    double r = currentCost / previousCost;\n                    actRed = 1.0 - r * r;\n                }\n\n                // compute the scaled predicted reduction\n                // and the scaled directional derivative\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    double dirJ = lmDir[pj];\n                    work1[j] = 0;\n                    for (int i = 0; i <= j; ++i) {\n                        work1[i] += weightedJacobian[i][pj] * dirJ;\n                    }\n                }\n                double coeff1 = 0;\n                for (int j = 0; j < solvedCols; ++j) {\n                    coeff1 += work1[j] * work1[j];\n                }\n                double pc2 = previousCost * previousCost;\n                coeff1 = coeff1 / pc2;\n                double coeff2 = lmPar * lmNorm * lmNorm / pc2;\n                double preRed = coeff1 + 2 * coeff2;\n                double dirDer = -(coeff1 + coeff2);\n\n                // ratio of the actual to the predicted reduction\n                ratio = (preRed == 0) ? 0 : (actRed / preRed);\n\n                // update the step bound\n                if (ratio <= 0.25) {\n                    double tmp =\n                        (actRed < 0) ? (0.5 * dirDer / (dirDer + 0.5 * actRed)) : 0.5;\n                        if ((0.1 * currentCost >= previousCost) || (tmp < 0.1)) {\n                            tmp = 0.1;\n                        }\n                        delta = tmp * FastMath.min(delta, 10.0 * lmNorm);\n                        lmPar /= tmp;\n                } else if ((lmPar == 0) || (ratio >= 0.75)) {\n                    delta = 2 * lmNorm;\n                    lmPar *= 0.5;\n                }\n\n                // test for successful iteration.\n                if (ratio >= 1.0e-4) {\n                    // successful iteration, update the norm\n                    firstIteration = false;\n                    xNorm = 0;\n                    for (int k = 0; k < nC; ++k) {\n                        double xK = diag[k] * currentPoint[k];\n                        xNorm += xK * xK;\n                    }\n                    xNorm = FastMath.sqrt(xNorm);\n\n                    // tests for convergence.\n                    if (checker != null) {\n                        // we use the vectorial convergence checker\n                        if (checker.converged(iter, previous, current)) {\n                            setCost(currentCost);\n                            return current;\n                        }\n                    }\n                } else {\n                    // failed iteration, reset the previous values\n                    currentCost = previousCost;\n                    for (int j = 0; j < solvedCols; ++j) {\n                        int pj = permutation[j];\n                        currentPoint[pj] = oldX[pj];\n                    }\n                    tmpVec    = weightedResidual;\n                    weightedResidual = oldRes;\n                    oldRes    = tmpVec;\n                    tmpVec    = currentObjective;\n                    currentObjective = oldObj;\n                    oldObj    = tmpVec;\n                    // Reset \"current\" to previous values.\n                    current = new PointVectorValuePair(currentPoint, currentObjective);\n                }\n\n                // Default convergence criteria.\n                if ((FastMath.abs(actRed) <= costRelativeTolerance &&\n                     preRed <= costRelativeTolerance &&\n                     ratio <= 2.0) ||\n                    delta <= parRelativeTolerance * xNorm) {\n                    setCost(currentCost);\n                    return current;\n                }\n\n                // tests for termination and stringent tolerances\n                // (2.2204e-16 is the machine epsilon for IEEE754)\n                if ((FastMath.abs(actRed) <= 2.2204e-16) && (preRed <= 2.2204e-16) && (ratio <= 2.0)) {\n                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_COST_RELATIVE_TOLERANCE,\n                                                   costRelativeTolerance);\n                } else if (delta <= 2.2204e-16 * xNorm) {\n                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE,\n                                                   parRelativeTolerance);\n                } else if (maxCosine <= 2.2204e-16)  {\n                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_ORTHOGONALITY_TOLERANCE,\n                                                   orthoTolerance);\n                }\n            }\n        }\n    }\n",
                "fixed_function": "    @Override\n    protected PointVectorValuePair doOptimize() {\n        checkParameters();\n\n        final int nR = getTarget().length; // Number of observed data.\n        final double[] currentPoint = getStartPoint();\n        final int nC = currentPoint.length; // Number of parameters.\n\n        // arrays shared with the other private methods\n        solvedCols  = FastMath.min(nR, nC);\n        diagR       = new double[nC];\n        jacNorm     = new double[nC];\n        beta        = new double[nC];\n        permutation = new int[nC];\n        lmDir       = new double[nC];\n\n        // local point\n        double   delta   = 0;\n        double   xNorm   = 0;\n        double[] diag    = new double[nC];\n        double[] oldX    = new double[nC];\n        double[] oldRes  = new double[nR];\n        double[] oldObj  = new double[nR];\n        double[] qtf     = new double[nR];\n        double[] work1   = new double[nC];\n        double[] work2   = new double[nC];\n        double[] work3   = new double[nC];\n\n        final RealMatrix weightMatrixSqrt = getWeightSquareRoot();\n\n        // Evaluate the function at the starting point and calculate its norm.\n        double[] currentObjective = computeObjectiveValue(currentPoint);\n        double[] currentResiduals = computeResiduals(currentObjective);\n        PointVectorValuePair current = new PointVectorValuePair(currentPoint, currentObjective);\n        double currentCost = computeCost(currentResiduals);\n\n        // Outer loop.\n        lmPar = 0;\n        boolean firstIteration = true;\n        final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n        while (true) {\n            incrementIterationCount();\n\n            final PointVectorValuePair previous = current;\n\n            // QR decomposition of the jacobian matrix\n            qrDecomposition(computeWeightedJacobian(currentPoint));\n\n            weightedResidual = weightMatrixSqrt.operate(currentResiduals);\n            for (int i = 0; i < nR; i++) {\n                qtf[i] = weightedResidual[i];\n            }\n\n            // compute Qt.res\n            qTy(qtf);\n\n            // now we don't need Q anymore,\n            // so let jacobian contain the R matrix with its diagonal elements\n            for (int k = 0; k < solvedCols; ++k) {\n                int pk = permutation[k];\n                weightedJacobian[k][pk] = diagR[pk];\n            }\n\n            if (firstIteration) {\n                // scale the point according to the norms of the columns\n                // of the initial jacobian\n                xNorm = 0;\n                for (int k = 0; k < nC; ++k) {\n                    double dk = jacNorm[k];\n                    if (dk == 0) {\n                        dk = 1.0;\n                    }\n                    double xk = dk * currentPoint[k];\n                    xNorm  += xk * xk;\n                    diag[k] = dk;\n                }\n                xNorm = FastMath.sqrt(xNorm);\n\n                // initialize the step bound delta\n                delta = (xNorm == 0) ? initialStepBoundFactor : (initialStepBoundFactor * xNorm);\n            }\n\n            // check orthogonality between function vector and jacobian columns\n            double maxCosine = 0;\n            if (currentCost != 0) {\n                for (int j = 0; j < solvedCols; ++j) {\n                    int    pj = permutation[j];\n                    double s  = jacNorm[pj];\n                    if (s != 0) {\n                        double sum = 0;\n                        for (int i = 0; i <= j; ++i) {\n                            sum += weightedJacobian[i][pj] * qtf[i];\n                        }\n                        maxCosine = FastMath.max(maxCosine, FastMath.abs(sum) / (s * currentCost));\n                    }\n                }\n            }\n            if (maxCosine <= orthoTolerance) {\n                // Convergence has been reached.\n                setCost(currentCost);\n                return current;\n            }\n\n            // rescale if necessary\n            for (int j = 0; j < nC; ++j) {\n                diag[j] = FastMath.max(diag[j], jacNorm[j]);\n            }\n\n            // Inner loop.\n            for (double ratio = 0; ratio < 1.0e-4;) {\n\n                // save the state\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    oldX[pj] = currentPoint[pj];\n                }\n                final double previousCost = currentCost;\n                double[] tmpVec = weightedResidual;\n                weightedResidual = oldRes;\n                oldRes    = tmpVec;\n                tmpVec    = currentObjective;\n                currentObjective = oldObj;\n                oldObj    = tmpVec;\n\n                // determine the Levenberg-Marquardt parameter\n                determineLMParameter(qtf, delta, diag, work1, work2, work3);\n\n                // compute the new point and the norm of the evolution direction\n                double lmNorm = 0;\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    lmDir[pj] = -lmDir[pj];\n                    currentPoint[pj] = oldX[pj] + lmDir[pj];\n                    double s = diag[pj] * lmDir[pj];\n                    lmNorm  += s * s;\n                }\n                lmNorm = FastMath.sqrt(lmNorm);\n                // on the first iteration, adjust the initial step bound.\n                if (firstIteration) {\n                    delta = FastMath.min(delta, lmNorm);\n                }\n\n                // Evaluate the function at x + p and calculate its norm.\n                currentObjective = computeObjectiveValue(currentPoint);\n                currentResiduals = computeResiduals(currentObjective);\n                current = new PointVectorValuePair(currentPoint, currentObjective);\n                currentCost = computeCost(currentResiduals);\n\n                // compute the scaled actual reduction\n                double actRed = -1.0;\n                if (0.1 * currentCost < previousCost) {\n                    double r = currentCost / previousCost;\n                    actRed = 1.0 - r * r;\n                }\n\n                // compute the scaled predicted reduction\n                // and the scaled directional derivative\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    double dirJ = lmDir[pj];\n                    work1[j] = 0;\n                    for (int i = 0; i <= j; ++i) {\n                        work1[i] += weightedJacobian[i][pj] * dirJ;\n                    }\n                }\n                double coeff1 = 0;\n                for (int j = 0; j < solvedCols; ++j) {\n                    coeff1 += work1[j] * work1[j];\n                }\n                double pc2 = previousCost * previousCost;\n                coeff1 = coeff1 / pc2;\n                double coeff2 = lmPar * lmNorm * lmNorm / pc2;\n                double preRed = coeff1 + 2 * coeff2;\n                double dirDer = -(coeff1 + coeff2);\n\n                // ratio of the actual to the predicted reduction\n                ratio = (preRed == 0) ? 0 : (actRed / preRed);\n\n                // update the step bound\n                if (ratio <= 0.25) {\n                    double tmp =\n                        (actRed < 0) ? (0.5 * dirDer / (dirDer + 0.5 * actRed)) : 0.5;\n                        if ((0.1 * currentCost >= previousCost) || (tmp < 0.1)) {\n                            tmp = 0.1;\n                        }\n                        delta = tmp * FastMath.min(delta, 10.0 * lmNorm);\n                        lmPar /= tmp;\n                } else if ((lmPar == 0) || (ratio >= 0.75)) {\n                    delta = 2 * lmNorm;\n                    lmPar *= 0.5;\n                }\n\n                // test for successful iteration.\n                if (ratio >= 1.0e-4) {\n                    // successful iteration, update the norm\n                    firstIteration = false;\n                    xNorm = 0;\n                    for (int k = 0; k < nC; ++k) {\n                        double xK = diag[k] * currentPoint[k];\n                        xNorm += xK * xK;\n                    }\n                    xNorm = FastMath.sqrt(xNorm);\n\n                    // tests for convergence.\n                    if (checker != null) {\n                        // we use the vectorial convergence checker\n                        if (checker.converged(getIterations(), previous, current)) {\n                            setCost(currentCost);\n                            return current;\n                        }\n                    }\n                } else {\n                    // failed iteration, reset the previous values\n                    currentCost = previousCost;\n                    for (int j = 0; j < solvedCols; ++j) {\n                        int pj = permutation[j];\n                        currentPoint[pj] = oldX[pj];\n                    }\n                    tmpVec    = weightedResidual;\n                    weightedResidual = oldRes;\n                    oldRes    = tmpVec;\n                    tmpVec    = currentObjective;\n                    currentObjective = oldObj;\n                    oldObj    = tmpVec;\n                    // Reset \"current\" to previous values.\n                    current = new PointVectorValuePair(currentPoint, currentObjective);\n                }\n\n                // Default convergence criteria.\n                if ((FastMath.abs(actRed) <= costRelativeTolerance &&\n                     preRed <= costRelativeTolerance &&\n                     ratio <= 2.0) ||\n                    delta <= parRelativeTolerance * xNorm) {\n                    setCost(currentCost);\n                    return current;\n                }\n\n                // tests for termination and stringent tolerances\n                // (2.2204e-16 is the machine epsilon for IEEE754)\n                if ((FastMath.abs(actRed) <= 2.2204e-16) && (preRed <= 2.2204e-16) && (ratio <= 2.0)) {\n                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_COST_RELATIVE_TOLERANCE,\n                                                   costRelativeTolerance);\n                } else if (delta <= 2.2204e-16 * xNorm) {\n                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE,\n                                                   parRelativeTolerance);\n                } else if (maxCosine <= 2.2204e-16)  {\n                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_ORTHOGONALITY_TOLERANCE,\n                                                   orthoTolerance);\n                }\n            }\n        }\n    }\n",
                "comment": "    /** {@inheritDoc} */\n",
                "normalized_body": [
                    "@Override protected PointVectorValuePair doOptimize() { checkParameters(); final int nR = getTarget().length; final double[] currentPoint = getStartPoint(); final int nC = currentPoint.length; solvedCols = FastMath.min(nR, nC); diagR = new double[nC]; jacNorm = new double[nC]; beta = new double[nC]; permutation = new int[nC]; lmDir = new double[nC]; double delta = 0; double xNorm = 0; double[] diag = new double[nC]; double[] oldX = new double[nC]; double[] oldRes = new double[nR]; double[] oldObj = new double[nR]; double[] qtf = new double[nR]; double[] work1 = new double[nC]; double[] work2 = new double[nC]; double[] work3 = new double[nC]; final RealMatrix weightMatrixSqrt = getWeightSquareRoot(); double[] currentObjective = computeObjectiveValue(currentPoint); double[] currentResiduals = computeResiduals(currentObjective); PointVectorValuePair current = new PointVectorValuePair(currentPoint, currentObjective); double currentCost = computeCost(currentResiduals); lmPar = 0; boolean firstIteration = true; int iter = 0; final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker(); while (true) { ++iter; final PointVectorValuePair previous = current; qrDecomposition(computeWeightedJacobian(currentPoint)); weightedResidual = weightMatrixSqrt.operate(currentResiduals); for (int i = 0; i < nR; i++) { qtf[i] = weightedResidual[i]; } qTy(qtf); for (int k = 0; k < solvedCols; ++k) { int pk = permutation[k]; weightedJacobian[k][pk] = diagR[pk]; } if (firstIteration) { xNorm = 0; for (int k = 0; k < nC; ++k) { double dk = jacNorm[k]; if (dk == 0) { dk = 1.0; } double xk = dk * currentPoint[k]; xNorm += xk * xk; diag[k] = dk; } xNorm = FastMath.sqrt(xNorm); delta = (xNorm == 0) ? initialStepBoundFactor : (initialStepBoundFactor * xNorm); } double maxCosine = 0; if (currentCost != 0) { for (int j = 0; j < solvedCols; ++j) { int pj = permutation[j]; double s = jacNorm[pj]; if (s != 0) { double sum = 0; for (int i = 0; i <= j; ++i) { sum += weightedJacobian[i][pj] * qtf[i]; } maxCosine = FastMath.max(maxCosine, FastMath.abs(sum) / (s * currentCost)); } } } if (maxCosine <= orthoTolerance) { setCost(currentCost); return current; } for (int j = 0; j < nC; ++j) { diag[j] = FastMath.max(diag[j], jacNorm[j]); } for (double ratio = 0; ratio < 1.0e-4; ) { for (int j = 0; j < solvedCols; ++j) { int pj = permutation[j]; oldX[pj] = currentPoint[pj]; } final double previousCost = currentCost; double[] tmpVec = weightedResidual; weightedResidual = oldRes; oldRes = tmpVec; tmpVec = currentObjective; currentObjective = oldObj; oldObj = tmpVec; determineLMParameter(qtf, delta, diag, work1, work2, work3); double lmNorm = 0; for (int j = 0; j < solvedCols; ++j) { int pj = permutation[j]; lmDir[pj] = -lmDir[pj]; currentPoint[pj] = oldX[pj] + lmDir[pj]; double s = diag[pj] * lmDir[pj]; lmNorm += s * s; } lmNorm = FastMath.sqrt(lmNorm); if (firstIteration) { delta = FastMath.min(delta, lmNorm); } currentObjective = computeObjectiveValue(currentPoint); currentResiduals = computeResiduals(currentObjective); current = new PointVectorValuePair(currentPoint, currentObjective); currentCost = computeCost(currentResiduals); double actRed = -1.0; if (0.1 * currentCost < previousCost) { double r = currentCost / previousCost; actRed = 1.0 - r * r; } for (int j = 0; j < solvedCols; ++j) { int pj = permutation[j]; double dirJ = lmDir[pj]; work1[j] = 0; for (int i = 0; i <= j; ++i) { work1[i] += weightedJacobian[i][pj] * dirJ; } } double coeff1 = 0; for (int j = 0; j < solvedCols; ++j) { coeff1 += work1[j] * work1[j]; } double pc2 = previousCost * previousCost; coeff1 = coeff1 / pc2; double coeff2 = lmPar * lmNorm * lmNorm / pc2; double preRed = coeff1 + 2 * coeff2; double dirDer = -(coeff1 + coeff2); ratio = (preRed == 0) ? 0 : (actRed / preRed); if (ratio <= 0.25) { double tmp = (actRed < 0) ? (0.5 * dirDer / (dirDer + 0.5 * actRed)) : 0.5; if ((0.1 * currentCost >= previousCost) || (tmp < 0.1)) { tmp = 0.1; } delta = tmp * FastMath.min(delta, 10.0 * lmNorm); lmPar /= tmp; } else if ((lmPar == 0) || (ratio >= 0.75)) { delta = 2 * lmNorm; lmPar *= 0.5; } if (ratio >= 1.0e-4) { firstIteration = false; xNorm = 0; for (int k = 0; k < nC; ++k) { double xK = diag[k] * currentPoint[k]; xNorm += xK * xK; } xNorm = FastMath.sqrt(xNorm); if (checker != null) { if (checker.converged(iter, previous, current)) { setCost(currentCost); return current; } } } else { currentCost = previousCost; for (int j = 0; j < solvedCols; ++j) { int pj = permutation[j]; currentPoint[pj] = oldX[pj]; } tmpVec = weightedResidual; weightedResidual = oldRes; oldRes = tmpVec; tmpVec = currentObjective; currentObjective = oldObj; oldObj = tmpVec; current = new PointVectorValuePair(currentPoint, currentObjective); } if ((FastMath.abs(actRed) <= costRelativeTolerance && preRed <= costRelativeTolerance && ratio <= 2.0) || delta <= parRelativeTolerance * xNorm) { setCost(currentCost); return current; } if ((FastMath.abs(actRed) <= 2.2204e-16) && (preRed <= 2.2204e-16) && (ratio <= 2.0)) { throw new ConvergenceException(LocalizedFormats.TOO_SMALL_COST_RELATIVE_TOLERANCE, costRelativeTolerance); } else if (delta <= 2.2204e-16 * xNorm) { throw new ConvergenceException(LocalizedFormats.TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE, parRelativeTolerance); } else if (maxCosine <= 2.2204e-16) { throw new ConvergenceException(LocalizedFormats.TOO_SMALL_ORTHOGONALITY_TOLERANCE, orthoTolerance); } } } }"
                ],
                "top_similar_methods": [
                    [
                        1.0000000149011612,
                        "@Override protected PointVectorValuePair doOptimize() { checkParameters(); final int nR = getTarget().length; final double[] currentPoint = getStartPoint(); final int nC = currentPoint.length; solvedCols = FastMath.min(nR, nC); diagR = new double[nC]; jacNorm = new double[nC]; beta = new double[nC]; permutation = new int[nC]; lmDir = new double[nC]; double delta = 0; double xNorm = 0; double[] diag = new double[nC]; double[] oldX = new double[nC]; double[] oldRes = new double[nR]; double[] oldObj = new double[nR]; double[] qtf = new double[nR]; double[] work1 = new double[nC]; double[] work2 = new double[nC]; double[] work3 = new double[nC]; final RealMatrix weightMatrixSqrt = getWeightSquareRoot(); double[] currentObjective = computeObjectiveValue(currentPoint); double[] currentResiduals = computeResiduals(currentObjective); PointVectorValuePair current = new PointVectorValuePair(currentPoint, currentObjective); double currentCost = computeCost(currentResiduals); lmPar = 0; boolean firstIteration = true; int iter = 0; final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker(); while (true) { ++iter; final PointVectorValuePair previous = current; qrDecomposition(computeWeightedJacobian(currentPoint)); weightedResidual = weightMatrixSqrt.operate(currentResiduals); for (int i = 0; i < nR; i++) { qtf[i] = weightedResidual[i]; } qTy(qtf); for (int k = 0; k < solvedCols; ++k) { int pk = permutation[k]; weightedJacobian[k][pk] = diagR[pk]; } if (firstIteration) { xNorm = 0; for (int k = 0; k < nC; ++k) { double dk = jacNorm[k]; if (dk == 0) { dk = 1.0; } double xk = dk * currentPoint[k]; xNorm += xk * xk; diag[k] = dk; } xNorm = FastMath.sqrt(xNorm); delta = (xNorm == 0) ? initialStepBoundFactor : (initialStepBoundFactor * xNorm); } double maxCosine = 0; if (currentCost != 0) { for (int j = 0; j < solvedCols; ++j) { int pj = permutation[j]; double s = jacNorm[pj]; if (s != 0) { double sum = 0; for (int i = 0; i <= j; ++i) { sum += weightedJacobian[i][pj] * qtf[i]; } maxCosine = FastMath.max(maxCosine, FastMath.abs(sum) / (s * currentCost)); } } } if (maxCosine <= orthoTolerance) { setCost(currentCost); return current; } for (int j = 0; j < nC; ++j) { diag[j] = FastMath.max(diag[j], jacNorm[j]); } for (double ratio = 0; ratio < 1.0e-4; ) { for (int j = 0; j < solvedCols; ++j) { int pj = permutation[j]; oldX[pj] = currentPoint[pj]; } final double previousCost = currentCost; double[] tmpVec = weightedResidual; weightedResidual = oldRes; oldRes = tmpVec; tmpVec = currentObjective; currentObjective = oldObj; oldObj = tmpVec; determineLMParameter(qtf, delta, diag, work1, work2, work3); double lmNorm = 0; for (int j = 0; j < solvedCols; ++j) { int pj = permutation[j]; lmDir[pj] = -lmDir[pj]; currentPoint[pj] = oldX[pj] + lmDir[pj]; double s = diag[pj] * lmDir[pj]; lmNorm += s * s; } lmNorm = FastMath.sqrt(lmNorm); if (firstIteration) { delta = FastMath.min(delta, lmNorm); } currentObjective = computeObjectiveValue(currentPoint); currentResiduals = computeResiduals(currentObjective); current = new PointVectorValuePair(currentPoint, currentObjective); currentCost = computeCost(currentResiduals); double actRed = -1.0; if (0.1 * currentCost < previousCost) { double r = currentCost / previousCost; actRed = 1.0 - r * r; } for (int j = 0; j < solvedCols; ++j) { int pj = permutation[j]; double dirJ = lmDir[pj]; work1[j] = 0; for (int i = 0; i <= j; ++i) { work1[i] += weightedJacobian[i][pj] * dirJ; } } double coeff1 = 0; for (int j = 0; j < solvedCols; ++j) { coeff1 += work1[j] * work1[j]; } double pc2 = previousCost * previousCost; coeff1 = coeff1 / pc2; double coeff2 = lmPar * lmNorm * lmNorm / pc2; double preRed = coeff1 + 2 * coeff2; double dirDer = -(coeff1 + coeff2); ratio = (preRed == 0) ? 0 : (actRed / preRed); if (ratio <= 0.25) { double tmp = (actRed < 0) ? (0.5 * dirDer / (dirDer + 0.5 * actRed)) : 0.5; if ((0.1 * currentCost >= previousCost) || (tmp < 0.1)) { tmp = 0.1; } delta = tmp * FastMath.min(delta, 10.0 * lmNorm); lmPar /= tmp; } else if ((lmPar == 0) || (ratio >= 0.75)) { delta = 2 * lmNorm; lmPar *= 0.5; } if (ratio >= 1.0e-4) { firstIteration = false; xNorm = 0; for (int k = 0; k < nC; ++k) { double xK = diag[k] * currentPoint[k]; xNorm += xK * xK; } xNorm = FastMath.sqrt(xNorm); if (checker != null) { if (checker.converged(iter, previous, current)) { setCost(currentCost); return current; } } } else { currentCost = previousCost; for (int j = 0; j < solvedCols; ++j) { int pj = permutation[j]; currentPoint[pj] = oldX[pj]; } tmpVec = weightedResidual; weightedResidual = oldRes; oldRes = tmpVec; tmpVec = currentObjective; currentObjective = oldObj; oldObj = tmpVec; current = new PointVectorValuePair(currentPoint, currentObjective); } if ((FastMath.abs(actRed) <= costRelativeTolerance && preRed <= costRelativeTolerance && ratio <= 2.0) || delta <= parRelativeTolerance * xNorm) { setCost(currentCost); return current; } if ((FastMath.abs(actRed) <= 2.2204e-16) && (preRed <= 2.2204e-16) && (ratio <= 2.0)) { throw new ConvergenceException(LocalizedFormats.TOO_SMALL_COST_RELATIVE_TOLERANCE, costRelativeTolerance); } else if (delta <= 2.2204e-16 * xNorm) { throw new ConvergenceException(LocalizedFormats.TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE, parRelativeTolerance); } else if (maxCosine <= 2.2204e-16) { throw new ConvergenceException(LocalizedFormats.TOO_SMALL_ORTHOGONALITY_TOLERANCE, orthoTolerance); } } } }"
                    ],
                    [
                        0.999730110168457,
                        "@Override protected PointVectorValuePair doOptimize() { final int nR = getTarget().length; final double[] currentPoint = getStartPoint(); final int nC = currentPoint.length; solvedCols = FastMath.min(nR, nC); diagR = new double[nC]; jacNorm = new double[nC]; beta = new double[nC]; permutation = new int[nC]; lmDir = new double[nC]; double delta = 0; double xNorm = 0; double[] diag = new double[nC]; double[] oldX = new double[nC]; double[] oldRes = new double[nR]; double[] oldObj = new double[nR]; double[] qtf = new double[nR]; double[] work1 = new double[nC]; double[] work2 = new double[nC]; double[] work3 = new double[nC]; final RealMatrix weightMatrixSqrt = getWeightSquareRoot(); double[] currentObjective = computeObjectiveValue(currentPoint); double[] currentResiduals = computeResiduals(currentObjective); PointVectorValuePair current = new PointVectorValuePair(currentPoint, currentObjective); double currentCost = computeCost(currentResiduals); lmPar = 0; boolean firstIteration = true; int iter = 0; final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker(); while (true) { ++iter; final PointVectorValuePair previous = current; qrDecomposition(computeWeightedJacobian(currentPoint)); weightedResidual = weightMatrixSqrt.operate(currentResiduals); for (int i = 0; i < nR; i++) { qtf[i] = weightedResidual[i]; } qTy(qtf); for (int k = 0; k < solvedCols; ++k) { int pk = permutation[k]; weightedJacobian[k][pk] = diagR[pk]; } if (firstIteration) { xNorm = 0; for (int k = 0; k < nC; ++k) { double dk = jacNorm[k]; if (dk == 0) { dk = 1.0; } double xk = dk * currentPoint[k]; xNorm += xk * xk; diag[k] = dk; } xNorm = FastMath.sqrt(xNorm); delta = (xNorm == 0) ? initialStepBoundFactor : (initialStepBoundFactor * xNorm); } double maxCosine = 0; if (currentCost != 0) { for (int j = 0; j < solvedCols; ++j) { int pj = permutation[j]; double s = jacNorm[pj]; if (s != 0) { double sum = 0; for (int i = 0; i <= j; ++i) { sum += weightedJacobian[i][pj] * qtf[i]; } maxCosine = FastMath.max(maxCosine, FastMath.abs(sum) / (s * currentCost)); } } } if (maxCosine <= orthoTolerance) { setCost(currentCost); point = current.getPoint(); return current; } for (int j = 0; j < nC; ++j) { diag[j] = FastMath.max(diag[j], jacNorm[j]); } for (double ratio = 0; ratio < 1.0e-4; ) { for (int j = 0; j < solvedCols; ++j) { int pj = permutation[j]; oldX[pj] = currentPoint[pj]; } final double previousCost = currentCost; double[] tmpVec = weightedResidual; weightedResidual = oldRes; oldRes = tmpVec; tmpVec = currentObjective; currentObjective = oldObj; oldObj = tmpVec; determineLMParameter(qtf, delta, diag, work1, work2, work3); double lmNorm = 0; for (int j = 0; j < solvedCols; ++j) { int pj = permutation[j]; lmDir[pj] = -lmDir[pj]; currentPoint[pj] = oldX[pj] + lmDir[pj]; double s = diag[pj] * lmDir[pj]; lmNorm += s * s; } lmNorm = FastMath.sqrt(lmNorm); if (firstIteration) { delta = FastMath.min(delta, lmNorm); } currentObjective = computeObjectiveValue(currentPoint); currentResiduals = computeResiduals(currentObjective); current = new PointVectorValuePair(currentPoint, currentObjective); currentCost = computeCost(currentResiduals); double actRed = -1.0; if (0.1 * currentCost < previousCost) { double r = currentCost / previousCost; actRed = 1.0 - r * r; } for (int j = 0; j < solvedCols; ++j) { int pj = permutation[j]; double dirJ = lmDir[pj]; work1[j] = 0; for (int i = 0; i <= j; ++i) { work1[i] += weightedJacobian[i][pj] * dirJ; } } double coeff1 = 0; for (int j = 0; j < solvedCols; ++j) { coeff1 += work1[j] * work1[j]; } double pc2 = previousCost * previousCost; coeff1 = coeff1 / pc2; double coeff2 = lmPar * lmNorm * lmNorm / pc2; double preRed = coeff1 + 2 * coeff2; double dirDer = -(coeff1 + coeff2); ratio = (preRed == 0) ? 0 : (actRed / preRed); if (ratio <= 0.25) { double tmp = (actRed < 0) ? (0.5 * dirDer / (dirDer + 0.5 * actRed)) : 0.5; if ((0.1 * currentCost >= previousCost) || (tmp < 0.1)) { tmp = 0.1; } delta = tmp * FastMath.min(delta, 10.0 * lmNorm); lmPar /= tmp; } else if ((lmPar == 0) || (ratio >= 0.75)) { delta = 2 * lmNorm; lmPar *= 0.5; } if (ratio >= 1.0e-4) { firstIteration = false; xNorm = 0; for (int k = 0; k < nC; ++k) { double xK = diag[k] * currentPoint[k]; xNorm += xK * xK; } xNorm = FastMath.sqrt(xNorm); if (checker != null) { if (checker.converged(iter, previous, current)) { setCost(currentCost); point = current.getPoint(); return current; } } } else { currentCost = previousCost; for (int j = 0; j < solvedCols; ++j) { int pj = permutation[j]; currentPoint[pj] = oldX[pj]; } tmpVec = weightedResidual; weightedResidual = oldRes; oldRes = tmpVec; tmpVec = currentObjective; currentObjective = oldObj; oldObj = tmpVec; current = new PointVectorValuePair(currentPoint, currentObjective); } if ((FastMath.abs(actRed) <= costRelativeTolerance && preRed <= costRelativeTolerance && ratio <= 2.0) || delta <= parRelativeTolerance * xNorm) { setCost(currentCost); point = current.getPoint(); return current; } if ((FastMath.abs(actRed) <= 2.2204e-16) && (preRed <= 2.2204e-16) && (ratio <= 2.0)) { throw new ConvergenceException(LocalizedFormats.TOO_SMALL_COST_RELATIVE_TOLERANCE, costRelativeTolerance); } else if (delta <= 2.2204e-16 * xNorm) { throw new ConvergenceException(LocalizedFormats.TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE, parRelativeTolerance); } else if (maxCosine <= 2.2204e-16) { throw new ConvergenceException(LocalizedFormats.TOO_SMALL_ORTHOGONALITY_TOLERANCE, orthoTolerance); } } } }"
                    ],
                    [
                        0.9936806112527847,
                        "private double bobyqb(double[] lowerBound, double[] upperBound) { printMethod(); final int n = currentBest.getDimension(); final int npt = numberOfInterpolationPoints; final int np = n + 1; final int nptm = npt - np; final int nh = n * np / 2; final ArrayRealVector work1 = new ArrayRealVector(n); final ArrayRealVector work2 = new ArrayRealVector(npt); final ArrayRealVector work3 = new ArrayRealVector(npt); double cauchy = Double.NaN; double alpha = Double.NaN; double dsq = Double.NaN; double crvmin = Double.NaN; trustRegionCenterInterpolationPointIndex = 0; prelim(lowerBound, upperBound); double xoptsq = ZERO; for (int i = 0; i < n; i++) { trustRegionCenterOffset.setEntry(i, interpolationPoints.getEntry(trustRegionCenterInterpolationPointIndex, i)); final double deltaOne = trustRegionCenterOffset.getEntry(i); xoptsq += deltaOne * deltaOne; } double fsave = fAtInterpolationPoints.getEntry(0); final int kbase = 0; int ntrits = 0; int itest = 0; int knew = 0; int nfsav = getEvaluations(); double rho = initialTrustRegionRadius; double delta = rho; double diffa = ZERO; double diffb = ZERO; double diffc = ZERO; double f = ZERO; double beta = ZERO; double adelt = ZERO; double denom = ZERO; double ratio = ZERO; double dnorm = ZERO; double scaden = ZERO; double biglsq = ZERO; double distsq = ZERO; int state = 20; for (; ; ) switch(state) { case 20: { printState(20); if (trustRegionCenterInterpolationPointIndex != kbase) { int ih = 0; for (int j = 0; j < n; j++) { for (int i = 0; i <= j; i++) { if (i < j) { gradientAtTrustRegionCenter.setEntry(j, gradientAtTrustRegionCenter.getEntry(j) + modelSecondDerivativesValues.getEntry(ih) * trustRegionCenterOffset.getEntry(i)); } gradientAtTrustRegionCenter.setEntry(i, gradientAtTrustRegionCenter.getEntry(i) + modelSecondDerivativesValues.getEntry(ih) * trustRegionCenterOffset.getEntry(j)); ih++; } } if (getEvaluations() > npt) { for (int k = 0; k < npt; k++) { double temp = ZERO; for (int j = 0; j < n; j++) { temp += interpolationPoints.getEntry(k, j) * trustRegionCenterOffset.getEntry(j); } temp *= modelSecondDerivativesParameters.getEntry(k); for (int i = 0; i < n; i++) { gradientAtTrustRegionCenter.setEntry(i, gradientAtTrustRegionCenter.getEntry(i) + temp * interpolationPoints.getEntry(k, i)); } } } } } case 60: { printState(60); final ArrayRealVector gnew = new ArrayRealVector(n); final ArrayRealVector xbdi = new ArrayRealVector(n); final ArrayRealVector s = new ArrayRealVector(n); final ArrayRealVector hs = new ArrayRealVector(n); final ArrayRealVector hred = new ArrayRealVector(n); final double[] dsqCrvmin = trsbox(delta, gnew, xbdi, s, hs, hred); dsq = dsqCrvmin[0]; crvmin = dsqCrvmin[1]; double deltaOne = delta; double deltaTwo = Math.sqrt(dsq); dnorm = Math.min(deltaOne, deltaTwo); if (dnorm < HALF * rho) { ntrits = -1; deltaOne = TEN * rho; distsq = deltaOne * deltaOne; if (getEvaluations() <= nfsav + 2) { state = 650; break; } deltaOne = Math.max(diffa, diffb); final double errbig = Math.max(deltaOne, diffc); final double frhosq = rho * ONE_OVER_EIGHT * rho; if (crvmin > ZERO && errbig > frhosq * crvmin) { state = 650; break; } final double bdtol = errbig / rho; for (int j = 0; j < n; j++) { double bdtest = bdtol; if (newPoint.getEntry(j) == lowerDifference.getEntry(j)) { bdtest = work1.getEntry(j); } if (newPoint.getEntry(j) == upperDifference.getEntry(j)) { bdtest = -work1.getEntry(j); } if (bdtest < bdtol) { double curv = modelSecondDerivativesValues.getEntry((j + j * j) / 2); for (int k = 0; k < npt; k++) { final double d1 = interpolationPoints.getEntry(k, j); curv += modelSecondDerivativesParameters.getEntry(k) * (d1 * d1); } bdtest += HALF * curv * rho; if (bdtest < bdtol) { state = 650; break; } } } state = 680; break; } ++ntrits; } case 90: { printState(90); if (dsq <= xoptsq * ONE_OVER_A_THOUSAND) { final double fracsq = xoptsq * ONE_OVER_FOUR; double sumpq = ZERO; for (int k = 0; k < npt; k++) { sumpq += modelSecondDerivativesParameters.getEntry(k); double sum = -HALF * xoptsq; for (int i = 0; i < n; i++) { sum += interpolationPoints.getEntry(k, i) * trustRegionCenterOffset.getEntry(i); } work2.setEntry(k, sum); final double temp = fracsq - HALF * sum; for (int i = 0; i < n; i++) { work1.setEntry(i, bMatrix.getEntry(k, i)); lagrangeValuesAtNewPoint.setEntry(i, sum * interpolationPoints.getEntry(k, i) + temp * trustRegionCenterOffset.getEntry(i)); final int ip = npt + i; for (int j = 0; j <= i; j++) { bMatrix.setEntry(ip, j, bMatrix.getEntry(ip, j) + work1.getEntry(i) * lagrangeValuesAtNewPoint.getEntry(j) + lagrangeValuesAtNewPoint.getEntry(i) * work1.getEntry(j)); } } } for (int m = 0; m < nptm; m++) { double sumz = ZERO; double sumw = ZERO; for (int k = 0; k < npt; k++) { sumz += zMatrix.getEntry(k, m); lagrangeValuesAtNewPoint.setEntry(k, work2.getEntry(k) * zMatrix.getEntry(k, m)); sumw += lagrangeValuesAtNewPoint.getEntry(k); } for (int j = 0; j < n; j++) { double sum = (fracsq * sumz - HALF * sumw) * trustRegionCenterOffset.getEntry(j); for (int k = 0; k < npt; k++) { sum += lagrangeValuesAtNewPoint.getEntry(k) * interpolationPoints.getEntry(k, j); } work1.setEntry(j, sum); for (int k = 0; k < npt; k++) { bMatrix.setEntry(k, j, bMatrix.getEntry(k, j) + sum * zMatrix.getEntry(k, m)); } } for (int i = 0; i < n; i++) { final int ip = i + npt; final double temp = work1.getEntry(i); for (int j = 0; j <= i; j++) { bMatrix.setEntry(ip, j, bMatrix.getEntry(ip, j) + temp * work1.getEntry(j)); } } } int ih = 0; for (int j = 0; j < n; j++) { work1.setEntry(j, -HALF * sumpq * trustRegionCenterOffset.getEntry(j)); for (int k = 0; k < npt; k++) { work1.setEntry(j, work1.getEntry(j) + modelSecondDerivativesParameters.getEntry(k) * interpolationPoints.getEntry(k, j)); interpolationPoints.setEntry(k, j, interpolationPoints.getEntry(k, j) - trustRegionCenterOffset.getEntry(j)); } for (int i = 0; i <= j; i++) { modelSecondDerivativesValues.setEntry(ih, modelSecondDerivativesValues.getEntry(ih) + work1.getEntry(i) * trustRegionCenterOffset.getEntry(j) + trustRegionCenterOffset.getEntry(i) * work1.getEntry(j)); bMatrix.setEntry(npt + i, j, bMatrix.getEntry(npt + j, i)); ih++; } } for (int i = 0; i < n; i++) { originShift.setEntry(i, originShift.getEntry(i) + trustRegionCenterOffset.getEntry(i)); newPoint.setEntry(i, newPoint.getEntry(i) - trustRegionCenterOffset.getEntry(i)); lowerDifference.setEntry(i, lowerDifference.getEntry(i) - trustRegionCenterOffset.getEntry(i)); upperDifference.setEntry(i, upperDifference.getEntry(i) - trustRegionCenterOffset.getEntry(i)); trustRegionCenterOffset.setEntry(i, ZERO); } xoptsq = ZERO; } if (ntrits == 0) { state = 210; break; } state = 230; break; } case 210: { printState(210); final double[] alphaCauchy = altmov(knew, adelt); alpha = alphaCauchy[0]; cauchy = alphaCauchy[1]; for (int i = 0; i < n; i++) { trialStepPoint.setEntry(i, newPoint.getEntry(i) - trustRegionCenterOffset.getEntry(i)); } } case 230: { printState(230); for (int k = 0; k < npt; k++) { double suma = ZERO; double sumb = ZERO; double sum = ZERO; for (int j = 0; j < n; j++) { suma += interpolationPoints.getEntry(k, j) * trialStepPoint.getEntry(j); sumb += interpolationPoints.getEntry(k, j) * trustRegionCenterOffset.getEntry(j); sum += bMatrix.getEntry(k, j) * trialStepPoint.getEntry(j); } work3.setEntry(k, suma * (HALF * suma + sumb)); lagrangeValuesAtNewPoint.setEntry(k, sum); work2.setEntry(k, suma); } beta = ZERO; for (int m = 0; m < nptm; m++) { double sum = ZERO; for (int k = 0; k < npt; k++) { sum += zMatrix.getEntry(k, m) * work3.getEntry(k); } beta -= sum * sum; for (int k = 0; k < npt; k++) { lagrangeValuesAtNewPoint.setEntry(k, lagrangeValuesAtNewPoint.getEntry(k) + sum * zMatrix.getEntry(k, m)); } } dsq = ZERO; double bsum = ZERO; double dx = ZERO; for (int j = 0; j < n; j++) { final double d1 = trialStepPoint.getEntry(j); dsq += d1 * d1; double sum = ZERO; for (int k = 0; k < npt; k++) { sum += work3.getEntry(k) * bMatrix.getEntry(k, j); } bsum += sum * trialStepPoint.getEntry(j); final int jp = npt + j; for (int i = 0; i < n; i++) { sum += bMatrix.getEntry(jp, i) * trialStepPoint.getEntry(i); } lagrangeValuesAtNewPoint.setEntry(jp, sum); bsum += sum * trialStepPoint.getEntry(j); dx += trialStepPoint.getEntry(j) * trustRegionCenterOffset.getEntry(j); } beta = dx * dx + dsq * (xoptsq + dx + dx + HALF * dsq) + beta - bsum; lagrangeValuesAtNewPoint.setEntry(trustRegionCenterInterpolationPointIndex, lagrangeValuesAtNewPoint.getEntry(trustRegionCenterInterpolationPointIndex) + ONE); if (ntrits == 0) { final double d1 = lagrangeValuesAtNewPoint.getEntry(knew); denom = d1 * d1 + alpha * beta; if (denom < cauchy && cauchy > ZERO) { for (int i = 0; i < n; i++) { newPoint.setEntry(i, alternativeNewPoint.getEntry(i)); trialStepPoint.setEntry(i, newPoint.getEntry(i) - trustRegionCenterOffset.getEntry(i)); } cauchy = ZERO; state = 230; break; } } else { final double delsq = delta * delta; scaden = ZERO; biglsq = ZERO; knew = 0; for (int k = 0; k < npt; k++) { if (k == trustRegionCenterInterpolationPointIndex) { continue; } double hdiag = ZERO; for (int m = 0; m < nptm; m++) { final double d1 = zMatrix.getEntry(k, m); hdiag += d1 * d1; } final double d2 = lagrangeValuesAtNewPoint.getEntry(k); final double den = beta * hdiag + d2 * d2; distsq = ZERO; for (int j = 0; j < n; j++) { final double d3 = interpolationPoints.getEntry(k, j) - trustRegionCenterOffset.getEntry(j); distsq += d3 * d3; } final double d4 = distsq / delsq; final double temp = Math.max(ONE, d4 * d4); if (temp * den > scaden) { scaden = temp * den; knew = k; denom = den; } final double d5 = lagrangeValuesAtNewPoint.getEntry(k); biglsq = Math.max(biglsq, temp * (d5 * d5)); } } } case 360: { printState(360); for (int i = 0; i < n; i++) { final double d3 = lowerBound[i]; final double d4 = originShift.getEntry(i) + newPoint.getEntry(i); final double d1 = Math.max(d3, d4); final double d2 = upperBound[i]; currentBest.setEntry(i, Math.min(d1, d2)); if (newPoint.getEntry(i) == lowerDifference.getEntry(i)) { currentBest.setEntry(i, lowerBound[i]); } if (newPoint.getEntry(i) == upperDifference.getEntry(i)) { currentBest.setEntry(i, upperBound[i]); } } f = computeObjectiveValue(currentBest.toArray()); if (!isMinimize) f = -f; if (ntrits == -1) { fsave = f; state = 720; break; } final double fopt = fAtInterpolationPoints.getEntry(trustRegionCenterInterpolationPointIndex); double vquad = ZERO; int ih = 0; for (int j = 0; j < n; j++) { vquad += trialStepPoint.getEntry(j) * gradientAtTrustRegionCenter.getEntry(j); for (int i = 0; i <= j; i++) { double temp = trialStepPoint.getEntry(i) * trialStepPoint.getEntry(j); if (i == j) { temp *= HALF; } vquad += modelSecondDerivativesValues.getEntry(ih) * temp; ih++; } } for (int k = 0; k < npt; k++) { final double d1 = work2.getEntry(k); final double d2 = d1 * d1; vquad += HALF * modelSecondDerivativesParameters.getEntry(k) * d2; } final double diff = f - fopt - vquad; diffc = diffb; diffb = diffa; diffa = Math.abs(diff); if (dnorm > rho) { nfsav = getEvaluations(); } if (ntrits > 0) { if (vquad >= ZERO) { throw new MathIllegalStateException(LocalizedFormats.TRUST_REGION_STEP_FAILED, vquad); } ratio = (f - fopt) / vquad; final double hDelta = HALF * delta; if (ratio <= ONE_OVER_TEN) { delta = Math.min(hDelta, dnorm); } else if (ratio <= .7) { delta = Math.max(hDelta, dnorm); } else { delta = Math.max(hDelta, 2 * dnorm); } if (delta <= rho * 1.5) { delta = rho; } if (f < fopt) { final int ksav = knew; final double densav = denom; final double delsq = delta * delta; scaden = ZERO; biglsq = ZERO; knew = 0; for (int k = 0; k < npt; k++) { double hdiag = ZERO; for (int m = 0; m < nptm; m++) { final double d1 = zMatrix.getEntry(k, m); hdiag += d1 * d1; } final double d1 = lagrangeValuesAtNewPoint.getEntry(k); final double den = beta * hdiag + d1 * d1; distsq = ZERO; for (int j = 0; j < n; j++) { final double d2 = interpolationPoints.getEntry(k, j) - newPoint.getEntry(j); distsq += d2 * d2; } final double d3 = distsq / delsq; final double temp = Math.max(ONE, d3 * d3); if (temp * den > scaden) { scaden = temp * den; knew = k; denom = den; } final double d4 = lagrangeValuesAtNewPoint.getEntry(k); final double d5 = temp * (d4 * d4); biglsq = Math.max(biglsq, d5); } if (scaden <= HALF * biglsq) { knew = ksav; denom = densav; } } } update(beta, denom, knew); ih = 0; final double pqold = modelSecondDerivativesParameters.getEntry(knew); modelSecondDerivativesParameters.setEntry(knew, ZERO); for (int i = 0; i < n; i++) { final double temp = pqold * interpolationPoints.getEntry(knew, i); for (int j = 0; j <= i; j++) { modelSecondDerivativesValues.setEntry(ih, modelSecondDerivativesValues.getEntry(ih) + temp * interpolationPoints.getEntry(knew, j)); ih++; } } for (int m = 0; m < nptm; m++) { final double temp = diff * zMatrix.getEntry(knew, m); for (int k = 0; k < npt; k++) { modelSecondDerivativesParameters.setEntry(k, modelSecondDerivativesParameters.getEntry(k) + temp * zMatrix.getEntry(k, m)); } } fAtInterpolationPoints.setEntry(knew, f); for (int i = 0; i < n; i++) { interpolationPoints.setEntry(knew, i, newPoint.getEntry(i)); work1.setEntry(i, bMatrix.getEntry(knew, i)); } for (int k = 0; k < npt; k++) { double suma = ZERO; for (int m = 0; m < nptm; m++) { suma += zMatrix.getEntry(knew, m) * zMatrix.getEntry(k, m); } double sumb = ZERO; for (int j = 0; j < n; j++) { sumb += interpolationPoints.getEntry(k, j) * trustRegionCenterOffset.getEntry(j); } final double temp = suma * sumb; for (int i = 0; i < n; i++) { work1.setEntry(i, work1.getEntry(i) + temp * interpolationPoints.getEntry(k, i)); } } for (int i = 0; i < n; i++) { gradientAtTrustRegionCenter.setEntry(i, gradientAtTrustRegionCenter.getEntry(i) + diff * work1.getEntry(i)); } if (f < fopt) { trustRegionCenterInterpolationPointIndex = knew; xoptsq = ZERO; ih = 0; for (int j = 0; j < n; j++) { trustRegionCenterOffset.setEntry(j, newPoint.getEntry(j)); final double d1 = trustRegionCenterOffset.getEntry(j); xoptsq += d1 * d1; for (int i = 0; i <= j; i++) { if (i < j) { gradientAtTrustRegionCenter.setEntry(j, gradientAtTrustRegionCenter.getEntry(j) + modelSecondDerivativesValues.getEntry(ih) * trialStepPoint.getEntry(i)); } gradientAtTrustRegionCenter.setEntry(i, gradientAtTrustRegionCenter.getEntry(i) + modelSecondDerivativesValues.getEntry(ih) * trialStepPoint.getEntry(j)); ih++; } } for (int k = 0; k < npt; k++) { double temp = ZERO; for (int j = 0; j < n; j++) { temp += interpolationPoints.getEntry(k, j) * trialStepPoint.getEntry(j); } temp *= modelSecondDerivativesParameters.getEntry(k); for (int i = 0; i < n; i++) { gradientAtTrustRegionCenter.setEntry(i, gradientAtTrustRegionCenter.getEntry(i) + temp * interpolationPoints.getEntry(k, i)); } } } if (ntrits > 0) { for (int k = 0; k < npt; k++) { lagrangeValuesAtNewPoint.setEntry(k, fAtInterpolationPoints.getEntry(k) - fAtInterpolationPoints.getEntry(trustRegionCenterInterpolationPointIndex)); work3.setEntry(k, ZERO); } for (int j = 0; j < nptm; j++) { double sum = ZERO; for (int k = 0; k < npt; k++) { sum += zMatrix.getEntry(k, j) * lagrangeValuesAtNewPoint.getEntry(k); } for (int k = 0; k < npt; k++) { work3.setEntry(k, work3.getEntry(k) + sum * zMatrix.getEntry(k, j)); } } for (int k = 0; k < npt; k++) { double sum = ZERO; for (int j = 0; j < n; j++) { sum += interpolationPoints.getEntry(k, j) * trustRegionCenterOffset.getEntry(j); } work2.setEntry(k, work3.getEntry(k)); work3.setEntry(k, sum * work3.getEntry(k)); } double gqsq = ZERO; double gisq = ZERO; for (int i = 0; i < n; i++) { double sum = ZERO; for (int k = 0; k < npt; k++) { sum += bMatrix.getEntry(k, i) * lagrangeValuesAtNewPoint.getEntry(k) + interpolationPoints.getEntry(k, i) * work3.getEntry(k); } if (trustRegionCenterOffset.getEntry(i) == lowerDifference.getEntry(i)) { final double d1 = Math.min(ZERO, gradientAtTrustRegionCenter.getEntry(i)); gqsq += d1 * d1; final double d2 = Math.min(ZERO, sum); gisq += d2 * d2; } else if (trustRegionCenterOffset.getEntry(i) == upperDifference.getEntry(i)) { final double d1 = Math.max(ZERO, gradientAtTrustRegionCenter.getEntry(i)); gqsq += d1 * d1; final double d2 = Math.max(ZERO, sum); gisq += d2 * d2; } else { final double d1 = gradientAtTrustRegionCenter.getEntry(i); gqsq += d1 * d1; gisq += sum * sum; } lagrangeValuesAtNewPoint.setEntry(npt + i, sum); } ++itest; if (gqsq < TEN * gisq) { itest = 0; } if (itest >= 3) { for (int i = 0, max = Math.max(npt, nh); i < max; i++) { if (i < n) { gradientAtTrustRegionCenter.setEntry(i, lagrangeValuesAtNewPoint.getEntry(npt + i)); } if (i < npt) { modelSecondDerivativesParameters.setEntry(i, work2.getEntry(i)); } if (i < nh) { modelSecondDerivativesValues.setEntry(i, ZERO); } itest = 0; } } } if (ntrits == 0) { state = 60; break; } if (f <= fopt + ONE_OVER_TEN * vquad) { state = 60; break; } final double d1 = TWO * delta; final double d2 = TEN * rho; distsq = Math.max(d1 * d1, d2 * d2); } case 650: { printState(650); knew = -1; for (int k = 0; k < npt; k++) { double sum = ZERO; for (int j = 0; j < n; j++) { final double d1 = interpolationPoints.getEntry(k, j) - trustRegionCenterOffset.getEntry(j); sum += d1 * d1; } if (sum > distsq) { knew = k; distsq = sum; } } if (knew >= 0) { final double dist = Math.sqrt(distsq); if (ntrits == -1) { delta = Math.min(ONE_OVER_TEN * delta, HALF * dist); if (delta <= rho * 1.5) { delta = rho; } } ntrits = 0; final double d1 = Math.min(ONE_OVER_TEN * dist, delta); adelt = Math.max(d1, rho); dsq = adelt * adelt; state = 90; break; } if (ntrits == -1) { state = 680; break; } if (ratio > ZERO) { state = 60; break; } if (Math.max(delta, dnorm) > rho) { state = 60; break; } } case 680: { printState(680); if (rho > stoppingTrustRegionRadius) { delta = HALF * rho; ratio = rho / stoppingTrustRegionRadius; if (ratio <= SIXTEEN) { rho = stoppingTrustRegionRadius; } else if (ratio <= TWO_HUNDRED_FIFTY) { rho = Math.sqrt(ratio) * stoppingTrustRegionRadius; } else { rho *= ONE_OVER_TEN; } delta = Math.max(delta, rho); ntrits = 0; nfsav = getEvaluations(); state = 60; break; } if (ntrits == -1) { state = 360; break; } } case 720: { printState(720); if (fAtInterpolationPoints.getEntry(trustRegionCenterInterpolationPointIndex) <= fsave) { for (int i = 0; i < n; i++) { final double d3 = lowerBound[i]; final double d4 = originShift.getEntry(i) + trustRegionCenterOffset.getEntry(i); final double d1 = Math.max(d3, d4); final double d2 = upperBound[i]; currentBest.setEntry(i, Math.min(d1, d2)); if (trustRegionCenterOffset.getEntry(i) == lowerDifference.getEntry(i)) { currentBest.setEntry(i, lowerBound[i]); } if (trustRegionCenterOffset.getEntry(i) == upperDifference.getEntry(i)) { currentBest.setEntry(i, upperBound[i]); } } f = fAtInterpolationPoints.getEntry(trustRegionCenterInterpolationPointIndex); } return f; } default: { throw new MathIllegalStateException(LocalizedFormats.SIMPLE_MESSAGE, \\\"bobyqb\\\"); } } }"
                    ],
                    [
                        0.9936806112527847,
                        "private double bobyqb(double[] lowerBound, double[] upperBound) { printMethod(); final int n = currentBest.getDimension(); final int npt = numberOfInterpolationPoints; final int np = n + 1; final int nptm = npt - np; final int nh = n * np / 2; final ArrayRealVector work1 = new ArrayRealVector(n); final ArrayRealVector work2 = new ArrayRealVector(npt); final ArrayRealVector work3 = new ArrayRealVector(npt); double cauchy = Double.NaN; double alpha = Double.NaN; double dsq = Double.NaN; double crvmin = Double.NaN; trustRegionCenterInterpolationPointIndex = 0; prelim(lowerBound, upperBound); double xoptsq = ZERO; for (int i = 0; i < n; i++) { trustRegionCenterOffset.setEntry(i, interpolationPoints.getEntry(trustRegionCenterInterpolationPointIndex, i)); final double deltaOne = trustRegionCenterOffset.getEntry(i); xoptsq += deltaOne * deltaOne; } double fsave = fAtInterpolationPoints.getEntry(0); final int kbase = 0; int ntrits = 0; int itest = 0; int knew = 0; int nfsav = getEvaluations(); double rho = initialTrustRegionRadius; double delta = rho; double diffa = ZERO; double diffb = ZERO; double diffc = ZERO; double f = ZERO; double beta = ZERO; double adelt = ZERO; double denom = ZERO; double ratio = ZERO; double dnorm = ZERO; double scaden = ZERO; double biglsq = ZERO; double distsq = ZERO; int state = 20; for (; ; ) switch(state) { case 20: { printState(20); if (trustRegionCenterInterpolationPointIndex != kbase) { int ih = 0; for (int j = 0; j < n; j++) { for (int i = 0; i <= j; i++) { if (i < j) { gradientAtTrustRegionCenter.setEntry(j, gradientAtTrustRegionCenter.getEntry(j) + modelSecondDerivativesValues.getEntry(ih) * trustRegionCenterOffset.getEntry(i)); } gradientAtTrustRegionCenter.setEntry(i, gradientAtTrustRegionCenter.getEntry(i) + modelSecondDerivativesValues.getEntry(ih) * trustRegionCenterOffset.getEntry(j)); ih++; } } if (getEvaluations() > npt) { for (int k = 0; k < npt; k++) { double temp = ZERO; for (int j = 0; j < n; j++) { temp += interpolationPoints.getEntry(k, j) * trustRegionCenterOffset.getEntry(j); } temp *= modelSecondDerivativesParameters.getEntry(k); for (int i = 0; i < n; i++) { gradientAtTrustRegionCenter.setEntry(i, gradientAtTrustRegionCenter.getEntry(i) + temp * interpolationPoints.getEntry(k, i)); } } } } } case 60: { printState(60); final ArrayRealVector gnew = new ArrayRealVector(n); final ArrayRealVector xbdi = new ArrayRealVector(n); final ArrayRealVector s = new ArrayRealVector(n); final ArrayRealVector hs = new ArrayRealVector(n); final ArrayRealVector hred = new ArrayRealVector(n); final double[] dsqCrvmin = trsbox(delta, gnew, xbdi, s, hs, hred); dsq = dsqCrvmin[0]; crvmin = dsqCrvmin[1]; double deltaOne = delta; double deltaTwo = Math.sqrt(dsq); dnorm = Math.min(deltaOne, deltaTwo); if (dnorm < HALF * rho) { ntrits = -1; deltaOne = TEN * rho; distsq = deltaOne * deltaOne; if (getEvaluations() <= nfsav + 2) { state = 650; break; } deltaOne = Math.max(diffa, diffb); final double errbig = Math.max(deltaOne, diffc); final double frhosq = rho * ONE_OVER_EIGHT * rho; if (crvmin > ZERO && errbig > frhosq * crvmin) { state = 650; break; } final double bdtol = errbig / rho; for (int j = 0; j < n; j++) { double bdtest = bdtol; if (newPoint.getEntry(j) == lowerDifference.getEntry(j)) { bdtest = work1.getEntry(j); } if (newPoint.getEntry(j) == upperDifference.getEntry(j)) { bdtest = -work1.getEntry(j); } if (bdtest < bdtol) { double curv = modelSecondDerivativesValues.getEntry((j + j * j) / 2); for (int k = 0; k < npt; k++) { final double d1 = interpolationPoints.getEntry(k, j); curv += modelSecondDerivativesParameters.getEntry(k) * (d1 * d1); } bdtest += HALF * curv * rho; if (bdtest < bdtol) { state = 650; break; } } } state = 680; break; } ++ntrits; } case 90: { printState(90); if (dsq <= xoptsq * ONE_OVER_A_THOUSAND) { final double fracsq = xoptsq * ONE_OVER_FOUR; double sumpq = ZERO; for (int k = 0; k < npt; k++) { sumpq += modelSecondDerivativesParameters.getEntry(k); double sum = -HALF * xoptsq; for (int i = 0; i < n; i++) { sum += interpolationPoints.getEntry(k, i) * trustRegionCenterOffset.getEntry(i); } work2.setEntry(k, sum); final double temp = fracsq - HALF * sum; for (int i = 0; i < n; i++) { work1.setEntry(i, bMatrix.getEntry(k, i)); lagrangeValuesAtNewPoint.setEntry(i, sum * interpolationPoints.getEntry(k, i) + temp * trustRegionCenterOffset.getEntry(i)); final int ip = npt + i; for (int j = 0; j <= i; j++) { bMatrix.setEntry(ip, j, bMatrix.getEntry(ip, j) + work1.getEntry(i) * lagrangeValuesAtNewPoint.getEntry(j) + lagrangeValuesAtNewPoint.getEntry(i) * work1.getEntry(j)); } } } for (int m = 0; m < nptm; m++) { double sumz = ZERO; double sumw = ZERO; for (int k = 0; k < npt; k++) { sumz += zMatrix.getEntry(k, m); lagrangeValuesAtNewPoint.setEntry(k, work2.getEntry(k) * zMatrix.getEntry(k, m)); sumw += lagrangeValuesAtNewPoint.getEntry(k); } for (int j = 0; j < n; j++) { double sum = (fracsq * sumz - HALF * sumw) * trustRegionCenterOffset.getEntry(j); for (int k = 0; k < npt; k++) { sum += lagrangeValuesAtNewPoint.getEntry(k) * interpolationPoints.getEntry(k, j); } work1.setEntry(j, sum); for (int k = 0; k < npt; k++) { bMatrix.setEntry(k, j, bMatrix.getEntry(k, j) + sum * zMatrix.getEntry(k, m)); } } for (int i = 0; i < n; i++) { final int ip = i + npt; final double temp = work1.getEntry(i); for (int j = 0; j <= i; j++) { bMatrix.setEntry(ip, j, bMatrix.getEntry(ip, j) + temp * work1.getEntry(j)); } } } int ih = 0; for (int j = 0; j < n; j++) { work1.setEntry(j, -HALF * sumpq * trustRegionCenterOffset.getEntry(j)); for (int k = 0; k < npt; k++) { work1.setEntry(j, work1.getEntry(j) + modelSecondDerivativesParameters.getEntry(k) * interpolationPoints.getEntry(k, j)); interpolationPoints.setEntry(k, j, interpolationPoints.getEntry(k, j) - trustRegionCenterOffset.getEntry(j)); } for (int i = 0; i <= j; i++) { modelSecondDerivativesValues.setEntry(ih, modelSecondDerivativesValues.getEntry(ih) + work1.getEntry(i) * trustRegionCenterOffset.getEntry(j) + trustRegionCenterOffset.getEntry(i) * work1.getEntry(j)); bMatrix.setEntry(npt + i, j, bMatrix.getEntry(npt + j, i)); ih++; } } for (int i = 0; i < n; i++) { originShift.setEntry(i, originShift.getEntry(i) + trustRegionCenterOffset.getEntry(i)); newPoint.setEntry(i, newPoint.getEntry(i) - trustRegionCenterOffset.getEntry(i)); lowerDifference.setEntry(i, lowerDifference.getEntry(i) - trustRegionCenterOffset.getEntry(i)); upperDifference.setEntry(i, upperDifference.getEntry(i) - trustRegionCenterOffset.getEntry(i)); trustRegionCenterOffset.setEntry(i, ZERO); } xoptsq = ZERO; } if (ntrits == 0) { state = 210; break; } state = 230; break; } case 210: { printState(210); final double[] alphaCauchy = altmov(knew, adelt); alpha = alphaCauchy[0]; cauchy = alphaCauchy[1]; for (int i = 0; i < n; i++) { trialStepPoint.setEntry(i, newPoint.getEntry(i) - trustRegionCenterOffset.getEntry(i)); } } case 230: { printState(230); for (int k = 0; k < npt; k++) { double suma = ZERO; double sumb = ZERO; double sum = ZERO; for (int j = 0; j < n; j++) { suma += interpolationPoints.getEntry(k, j) * trialStepPoint.getEntry(j); sumb += interpolationPoints.getEntry(k, j) * trustRegionCenterOffset.getEntry(j); sum += bMatrix.getEntry(k, j) * trialStepPoint.getEntry(j); } work3.setEntry(k, suma * (HALF * suma + sumb)); lagrangeValuesAtNewPoint.setEntry(k, sum); work2.setEntry(k, suma); } beta = ZERO; for (int m = 0; m < nptm; m++) { double sum = ZERO; for (int k = 0; k < npt; k++) { sum += zMatrix.getEntry(k, m) * work3.getEntry(k); } beta -= sum * sum; for (int k = 0; k < npt; k++) { lagrangeValuesAtNewPoint.setEntry(k, lagrangeValuesAtNewPoint.getEntry(k) + sum * zMatrix.getEntry(k, m)); } } dsq = ZERO; double bsum = ZERO; double dx = ZERO; for (int j = 0; j < n; j++) { final double d1 = trialStepPoint.getEntry(j); dsq += d1 * d1; double sum = ZERO; for (int k = 0; k < npt; k++) { sum += work3.getEntry(k) * bMatrix.getEntry(k, j); } bsum += sum * trialStepPoint.getEntry(j); final int jp = npt + j; for (int i = 0; i < n; i++) { sum += bMatrix.getEntry(jp, i) * trialStepPoint.getEntry(i); } lagrangeValuesAtNewPoint.setEntry(jp, sum); bsum += sum * trialStepPoint.getEntry(j); dx += trialStepPoint.getEntry(j) * trustRegionCenterOffset.getEntry(j); } beta = dx * dx + dsq * (xoptsq + dx + dx + HALF * dsq) + beta - bsum; lagrangeValuesAtNewPoint.setEntry(trustRegionCenterInterpolationPointIndex, lagrangeValuesAtNewPoint.getEntry(trustRegionCenterInterpolationPointIndex) + ONE); if (ntrits == 0) { final double d1 = lagrangeValuesAtNewPoint.getEntry(knew); denom = d1 * d1 + alpha * beta; if (denom < cauchy && cauchy > ZERO) { for (int i = 0; i < n; i++) { newPoint.setEntry(i, alternativeNewPoint.getEntry(i)); trialStepPoint.setEntry(i, newPoint.getEntry(i) - trustRegionCenterOffset.getEntry(i)); } cauchy = ZERO; state = 230; break; } } else { final double delsq = delta * delta; scaden = ZERO; biglsq = ZERO; knew = 0; for (int k = 0; k < npt; k++) { if (k == trustRegionCenterInterpolationPointIndex) { continue; } double hdiag = ZERO; for (int m = 0; m < nptm; m++) { final double d1 = zMatrix.getEntry(k, m); hdiag += d1 * d1; } final double d2 = lagrangeValuesAtNewPoint.getEntry(k); final double den = beta * hdiag + d2 * d2; distsq = ZERO; for (int j = 0; j < n; j++) { final double d3 = interpolationPoints.getEntry(k, j) - trustRegionCenterOffset.getEntry(j); distsq += d3 * d3; } final double d4 = distsq / delsq; final double temp = Math.max(ONE, d4 * d4); if (temp * den > scaden) { scaden = temp * den; knew = k; denom = den; } final double d5 = lagrangeValuesAtNewPoint.getEntry(k); biglsq = Math.max(biglsq, temp * (d5 * d5)); } } } case 360: { printState(360); for (int i = 0; i < n; i++) { final double d3 = lowerBound[i]; final double d4 = originShift.getEntry(i) + newPoint.getEntry(i); final double d1 = Math.max(d3, d4); final double d2 = upperBound[i]; currentBest.setEntry(i, Math.min(d1, d2)); if (newPoint.getEntry(i) == lowerDifference.getEntry(i)) { currentBest.setEntry(i, lowerBound[i]); } if (newPoint.getEntry(i) == upperDifference.getEntry(i)) { currentBest.setEntry(i, upperBound[i]); } } f = computeObjectiveValue(currentBest.toArray()); if (!isMinimize) f = -f; if (ntrits == -1) { fsave = f; state = 720; break; } final double fopt = fAtInterpolationPoints.getEntry(trustRegionCenterInterpolationPointIndex); double vquad = ZERO; int ih = 0; for (int j = 0; j < n; j++) { vquad += trialStepPoint.getEntry(j) * gradientAtTrustRegionCenter.getEntry(j); for (int i = 0; i <= j; i++) { double temp = trialStepPoint.getEntry(i) * trialStepPoint.getEntry(j); if (i == j) { temp *= HALF; } vquad += modelSecondDerivativesValues.getEntry(ih) * temp; ih++; } } for (int k = 0; k < npt; k++) { final double d1 = work2.getEntry(k); final double d2 = d1 * d1; vquad += HALF * modelSecondDerivativesParameters.getEntry(k) * d2; } final double diff = f - fopt - vquad; diffc = diffb; diffb = diffa; diffa = Math.abs(diff); if (dnorm > rho) { nfsav = getEvaluations(); } if (ntrits > 0) { if (vquad >= ZERO) { throw new MathIllegalStateException(LocalizedFormats.TRUST_REGION_STEP_FAILED, vquad); } ratio = (f - fopt) / vquad; final double hDelta = HALF * delta; if (ratio <= ONE_OVER_TEN) { delta = Math.min(hDelta, dnorm); } else if (ratio <= .7) { delta = Math.max(hDelta, dnorm); } else { delta = Math.max(hDelta, 2 * dnorm); } if (delta <= rho * 1.5) { delta = rho; } if (f < fopt) { final int ksav = knew; final double densav = denom; final double delsq = delta * delta; scaden = ZERO; biglsq = ZERO; knew = 0; for (int k = 0; k < npt; k++) { double hdiag = ZERO; for (int m = 0; m < nptm; m++) { final double d1 = zMatrix.getEntry(k, m); hdiag += d1 * d1; } final double d1 = lagrangeValuesAtNewPoint.getEntry(k); final double den = beta * hdiag + d1 * d1; distsq = ZERO; for (int j = 0; j < n; j++) { final double d2 = interpolationPoints.getEntry(k, j) - newPoint.getEntry(j); distsq += d2 * d2; } final double d3 = distsq / delsq; final double temp = Math.max(ONE, d3 * d3); if (temp * den > scaden) { scaden = temp * den; knew = k; denom = den; } final double d4 = lagrangeValuesAtNewPoint.getEntry(k); final double d5 = temp * (d4 * d4); biglsq = Math.max(biglsq, d5); } if (scaden <= HALF * biglsq) { knew = ksav; denom = densav; } } } update(beta, denom, knew); ih = 0; final double pqold = modelSecondDerivativesParameters.getEntry(knew); modelSecondDerivativesParameters.setEntry(knew, ZERO); for (int i = 0; i < n; i++) { final double temp = pqold * interpolationPoints.getEntry(knew, i); for (int j = 0; j <= i; j++) { modelSecondDerivativesValues.setEntry(ih, modelSecondDerivativesValues.getEntry(ih) + temp * interpolationPoints.getEntry(knew, j)); ih++; } } for (int m = 0; m < nptm; m++) { final double temp = diff * zMatrix.getEntry(knew, m); for (int k = 0; k < npt; k++) { modelSecondDerivativesParameters.setEntry(k, modelSecondDerivativesParameters.getEntry(k) + temp * zMatrix.getEntry(k, m)); } } fAtInterpolationPoints.setEntry(knew, f); for (int i = 0; i < n; i++) { interpolationPoints.setEntry(knew, i, newPoint.getEntry(i)); work1.setEntry(i, bMatrix.getEntry(knew, i)); } for (int k = 0; k < npt; k++) { double suma = ZERO; for (int m = 0; m < nptm; m++) { suma += zMatrix.getEntry(knew, m) * zMatrix.getEntry(k, m); } double sumb = ZERO; for (int j = 0; j < n; j++) { sumb += interpolationPoints.getEntry(k, j) * trustRegionCenterOffset.getEntry(j); } final double temp = suma * sumb; for (int i = 0; i < n; i++) { work1.setEntry(i, work1.getEntry(i) + temp * interpolationPoints.getEntry(k, i)); } } for (int i = 0; i < n; i++) { gradientAtTrustRegionCenter.setEntry(i, gradientAtTrustRegionCenter.getEntry(i) + diff * work1.getEntry(i)); } if (f < fopt) { trustRegionCenterInterpolationPointIndex = knew; xoptsq = ZERO; ih = 0; for (int j = 0; j < n; j++) { trustRegionCenterOffset.setEntry(j, newPoint.getEntry(j)); final double d1 = trustRegionCenterOffset.getEntry(j); xoptsq += d1 * d1; for (int i = 0; i <= j; i++) { if (i < j) { gradientAtTrustRegionCenter.setEntry(j, gradientAtTrustRegionCenter.getEntry(j) + modelSecondDerivativesValues.getEntry(ih) * trialStepPoint.getEntry(i)); } gradientAtTrustRegionCenter.setEntry(i, gradientAtTrustRegionCenter.getEntry(i) + modelSecondDerivativesValues.getEntry(ih) * trialStepPoint.getEntry(j)); ih++; } } for (int k = 0; k < npt; k++) { double temp = ZERO; for (int j = 0; j < n; j++) { temp += interpolationPoints.getEntry(k, j) * trialStepPoint.getEntry(j); } temp *= modelSecondDerivativesParameters.getEntry(k); for (int i = 0; i < n; i++) { gradientAtTrustRegionCenter.setEntry(i, gradientAtTrustRegionCenter.getEntry(i) + temp * interpolationPoints.getEntry(k, i)); } } } if (ntrits > 0) { for (int k = 0; k < npt; k++) { lagrangeValuesAtNewPoint.setEntry(k, fAtInterpolationPoints.getEntry(k) - fAtInterpolationPoints.getEntry(trustRegionCenterInterpolationPointIndex)); work3.setEntry(k, ZERO); } for (int j = 0; j < nptm; j++) { double sum = ZERO; for (int k = 0; k < npt; k++) { sum += zMatrix.getEntry(k, j) * lagrangeValuesAtNewPoint.getEntry(k); } for (int k = 0; k < npt; k++) { work3.setEntry(k, work3.getEntry(k) + sum * zMatrix.getEntry(k, j)); } } for (int k = 0; k < npt; k++) { double sum = ZERO; for (int j = 0; j < n; j++) { sum += interpolationPoints.getEntry(k, j) * trustRegionCenterOffset.getEntry(j); } work2.setEntry(k, work3.getEntry(k)); work3.setEntry(k, sum * work3.getEntry(k)); } double gqsq = ZERO; double gisq = ZERO; for (int i = 0; i < n; i++) { double sum = ZERO; for (int k = 0; k < npt; k++) { sum += bMatrix.getEntry(k, i) * lagrangeValuesAtNewPoint.getEntry(k) + interpolationPoints.getEntry(k, i) * work3.getEntry(k); } if (trustRegionCenterOffset.getEntry(i) == lowerDifference.getEntry(i)) { final double d1 = Math.min(ZERO, gradientAtTrustRegionCenter.getEntry(i)); gqsq += d1 * d1; final double d2 = Math.min(ZERO, sum); gisq += d2 * d2; } else if (trustRegionCenterOffset.getEntry(i) == upperDifference.getEntry(i)) { final double d1 = Math.max(ZERO, gradientAtTrustRegionCenter.getEntry(i)); gqsq += d1 * d1; final double d2 = Math.max(ZERO, sum); gisq += d2 * d2; } else { final double d1 = gradientAtTrustRegionCenter.getEntry(i); gqsq += d1 * d1; gisq += sum * sum; } lagrangeValuesAtNewPoint.setEntry(npt + i, sum); } ++itest; if (gqsq < TEN * gisq) { itest = 0; } if (itest >= 3) { for (int i = 0, max = Math.max(npt, nh); i < max; i++) { if (i < n) { gradientAtTrustRegionCenter.setEntry(i, lagrangeValuesAtNewPoint.getEntry(npt + i)); } if (i < npt) { modelSecondDerivativesParameters.setEntry(i, work2.getEntry(i)); } if (i < nh) { modelSecondDerivativesValues.setEntry(i, ZERO); } itest = 0; } } } if (ntrits == 0) { state = 60; break; } if (f <= fopt + ONE_OVER_TEN * vquad) { state = 60; break; } final double d1 = TWO * delta; final double d2 = TEN * rho; distsq = Math.max(d1 * d1, d2 * d2); } case 650: { printState(650); knew = -1; for (int k = 0; k < npt; k++) { double sum = ZERO; for (int j = 0; j < n; j++) { final double d1 = interpolationPoints.getEntry(k, j) - trustRegionCenterOffset.getEntry(j); sum += d1 * d1; } if (sum > distsq) { knew = k; distsq = sum; } } if (knew >= 0) { final double dist = Math.sqrt(distsq); if (ntrits == -1) { delta = Math.min(ONE_OVER_TEN * delta, HALF * dist); if (delta <= rho * 1.5) { delta = rho; } } ntrits = 0; final double d1 = Math.min(ONE_OVER_TEN * dist, delta); adelt = Math.max(d1, rho); dsq = adelt * adelt; state = 90; break; } if (ntrits == -1) { state = 680; break; } if (ratio > ZERO) { state = 60; break; } if (Math.max(delta, dnorm) > rho) { state = 60; break; } } case 680: { printState(680); if (rho > stoppingTrustRegionRadius) { delta = HALF * rho; ratio = rho / stoppingTrustRegionRadius; if (ratio <= SIXTEEN) { rho = stoppingTrustRegionRadius; } else if (ratio <= TWO_HUNDRED_FIFTY) { rho = Math.sqrt(ratio) * stoppingTrustRegionRadius; } else { rho *= ONE_OVER_TEN; } delta = Math.max(delta, rho); ntrits = 0; nfsav = getEvaluations(); state = 60; break; } if (ntrits == -1) { state = 360; break; } } case 720: { printState(720); if (fAtInterpolationPoints.getEntry(trustRegionCenterInterpolationPointIndex) <= fsave) { for (int i = 0; i < n; i++) { final double d3 = lowerBound[i]; final double d4 = originShift.getEntry(i) + trustRegionCenterOffset.getEntry(i); final double d1 = Math.max(d3, d4); final double d2 = upperBound[i]; currentBest.setEntry(i, Math.min(d1, d2)); if (trustRegionCenterOffset.getEntry(i) == lowerDifference.getEntry(i)) { currentBest.setEntry(i, lowerBound[i]); } if (trustRegionCenterOffset.getEntry(i) == upperDifference.getEntry(i)) { currentBest.setEntry(i, upperBound[i]); } } f = fAtInterpolationPoints.getEntry(trustRegionCenterInterpolationPointIndex); } return f; } default: { throw new MathIllegalStateException(LocalizedFormats.SIMPLE_MESSAGE, \\\"bobyqb\\\"); } } }"
                    ],
                    [
                        0.9919705837965012,
                        "public final double[] smooth(final double[] xval, final double[] yval, final double[] weights) throws NonMonotonicSequenceException, DimensionMismatchException, NoDataException, NotFiniteNumberException, NumberIsTooSmallException { if (xval.length != yval.length) { throw new DimensionMismatchException(xval.length, yval.length); } final int n = xval.length; if (n == 0) { throw new NoDataException(); } checkAllFiniteReal(xval); checkAllFiniteReal(yval); checkAllFiniteReal(weights); MathArrays.checkOrder(xval); if (n == 1) { return new double[] { yval[0] }; } if (n == 2) { return new double[] { yval[0], yval[1] }; } int bandwidthInPoints = (int) (bandwidth * n); if (bandwidthInPoints < 2) { throw new NumberIsTooSmallException(LocalizedFormats.BANDWIDTH, bandwidthInPoints, 2, true); } final double[] res = new double[n]; final double[] residuals = new double[n]; final double[] sortedResiduals = new double[n]; final double[] robustnessWeights = new double[n]; Arrays.fill(robustnessWeights, 1); for (int iter = 0; iter <= robustnessIters; ++iter) { final int[] bandwidthInterval = { 0, bandwidthInPoints - 1 }; for (int i = 0; i < n; ++i) { final double x = xval[i]; if (i > 0) { updateBandwidthInterval(xval, weights, i, bandwidthInterval); } final int ileft = bandwidthInterval[0]; final int iright = bandwidthInterval[1]; final int edge; if (xval[i] - xval[ileft] > xval[iright] - xval[i]) { edge = ileft; } else { edge = iright; } double sumWeights = 0; double sumX = 0; double sumXSquared = 0; double sumY = 0; double sumXY = 0; double denom = FastMath.abs(1.0 / (xval[edge] - x)); for (int k = ileft; k <= iright; ++k) { final double xk = xval[k]; final double yk = yval[k]; final double dist = (k < i) ? x - xk : xk - x; final double w = tricube(dist * denom) * robustnessWeights[k] * weights[k]; final double xkw = xk * w; sumWeights += w; sumX += xkw; sumXSquared += xk * xkw; sumY += yk * w; sumXY += yk * xkw; } final double meanX = sumX / sumWeights; final double meanY = sumY / sumWeights; final double meanXY = sumXY / sumWeights; final double meanXSquared = sumXSquared / sumWeights; final double beta; if (FastMath.sqrt(FastMath.abs(meanXSquared - meanX * meanX)) < accuracy) { beta = 0; } else { beta = (meanXY - meanX * meanY) / (meanXSquared - meanX * meanX); } final double alpha = meanY - beta * meanX; res[i] = beta * x + alpha; residuals[i] = FastMath.abs(yval[i] - res[i]); } if (iter == robustnessIters) { break; } System.arraycopy(residuals, 0, sortedResiduals, 0, n); Arrays.sort(sortedResiduals); final double medianResidual = sortedResiduals[n / 2]; if (FastMath.abs(medianResidual) < accuracy) { break; } for (int i = 0; i < n; ++i) { final double arg = residuals[i] / (6 * medianResidual); if (arg >= 1) { robustnessWeights[i] = 0; } else { final double w = 1 - arg * arg; robustnessWeights[i] = w * w; } } } return res; }"
                    ],
                    [
                        0.9919504672288895,
                        "@Override public void integrate(final ExpandableStatefulODE equations, final double t) throws NumberIsTooSmallException, DimensionMismatchException, MaxCountExceededException, NoBracketingException { sanityChecks(equations, t); setEquations(equations); final boolean forward = t > equations.getTime(); final double[] y0 = equations.getCompleteState(); final double[] y = y0.clone(); final double[] yDot = new double[y.length]; final double[] yTmp = new double[y.length]; final double[] predictedScaled = new double[y.length]; Array2DRowRealMatrix nordsieckTmp = null; final NordsieckStepInterpolator interpolator = new NordsieckStepInterpolator(); interpolator.reinitialize(y, forward, equations.getPrimaryMapper(), equations.getSecondaryMappers()); initIntegration(equations.getTime(), y0, t); start(equations.getTime(), y, t); interpolator.reinitialize(stepStart, stepSize, scaled, nordsieck); interpolator.storeTime(stepStart); double hNew = stepSize; interpolator.rescale(hNew); isLastStep = false; do { double error = 10; while (error >= 1.0) { stepSize = hNew; final double stepEnd = stepStart + stepSize; interpolator.setInterpolatedTime(stepEnd); System.arraycopy(interpolator.getInterpolatedState(), 0, yTmp, 0, y0.length); computeDerivatives(stepEnd, yTmp, yDot); for (int j = 0; j < y0.length; ++j) { predictedScaled[j] = stepSize * yDot[j]; } nordsieckTmp = updateHighOrderDerivativesPhase1(nordsieck); updateHighOrderDerivativesPhase2(scaled, predictedScaled, nordsieckTmp); error = nordsieckTmp.walkInOptimizedOrder(new Corrector(y, predictedScaled, yTmp)); if (error >= 1.0) { final double factor = computeStepGrowShrinkFactor(error); hNew = filterStep(stepSize * factor, forward, false); interpolator.rescale(hNew); } } final double stepEnd = stepStart + stepSize; computeDerivatives(stepEnd, yTmp, yDot); final double[] correctedScaled = new double[y0.length]; for (int j = 0; j < y0.length; ++j) { correctedScaled[j] = stepSize * yDot[j]; } updateHighOrderDerivativesPhase2(predictedScaled, correctedScaled, nordsieckTmp); System.arraycopy(yTmp, 0, y, 0, y.length); interpolator.reinitialize(stepEnd, stepSize, correctedScaled, nordsieckTmp); interpolator.storeTime(stepStart); interpolator.shift(); interpolator.storeTime(stepEnd); stepStart = acceptStep(interpolator, y, yDot, t); scaled = correctedScaled; nordsieck = nordsieckTmp; if (!isLastStep) { interpolator.storeTime(stepStart); if (resetOccurred) { start(stepStart, y, t); interpolator.reinitialize(stepStart, stepSize, scaled, nordsieck); } final double factor = computeStepGrowShrinkFactor(error); final double scaledH = stepSize * factor; final double nextT = stepStart + scaledH; final boolean nextIsLast = forward ? (nextT >= t) : (nextT <= t); hNew = filterStep(scaledH, forward, nextIsLast); final double filteredNextT = stepStart + hNew; final boolean filteredNextIsLast = forward ? (filteredNextT >= t) : (filteredNextT <= t); if (filteredNextIsLast) { hNew = t - stepStart; } interpolator.rescale(hNew); } } while (!isLastStep); equations.setTime(stepStart); equations.setCompleteState(y); resetInternalState(); }"
                    ],
                    [
                        0.9907494336366653,
                        "private void findEigenVectorsFromSchur(final SchurTransformer schur) throws MathArithmeticException { final double[][] matrixT = schur.getT().getData(); final double[][] matrixP = schur.getP().getData(); final int n = matrixT.length; double norm = 0.0; for (int i = 0; i < n; i++) { for (int j = FastMath.max(i - 1, 0); j < n; j++) { norm = norm + FastMath.abs(matrixT[i][j]); } } if (Precision.equals(norm, 0.0, EPSILON)) { throw new MathArithmeticException(LocalizedFormats.ZERO_NORM); } double r = 0.0; double s = 0.0; double z = 0.0; for (int idx = n - 1; idx >= 0; idx--) { double p = realEigenvalues[idx]; double q = imagEigenvalues[idx]; if (Precision.equals(q, 0.0)) { int l = idx; matrixT[idx][idx] = 1.0; for (int i = idx - 1; i >= 0; i--) { double w = matrixT[i][i] - p; r = 0.0; for (int j = l; j <= idx; j++) { r = r + matrixT[i][j] * matrixT[j][idx]; } if (Precision.compareTo(imagEigenvalues[i], 0.0, EPSILON) < 0.0) { z = w; s = r; } else { l = i; if (Precision.equals(imagEigenvalues[i], 0.0)) { if (w != 0.0) { matrixT[i][idx] = -r / w; } else { matrixT[i][idx] = -r / (Precision.EPSILON * norm); } } else { double x = matrixT[i][i + 1]; double y = matrixT[i + 1][i]; q = (realEigenvalues[i] - p) * (realEigenvalues[i] - p) + imagEigenvalues[i] * imagEigenvalues[i]; double t = (x * s - z * r) / q; matrixT[i][idx] = t; if (FastMath.abs(x) > FastMath.abs(z)) { matrixT[i + 1][idx] = (-r - w * t) / x; } else { matrixT[i + 1][idx] = (-s - y * t) / z; } } double t = FastMath.abs(matrixT[i][idx]); if ((Precision.EPSILON * t) * t > 1) { for (int j = i; j <= idx; j++) { matrixT[j][idx] = matrixT[j][idx] / t; } } } } } else if (q < 0.0) { int l = idx - 1; if (FastMath.abs(matrixT[idx][idx - 1]) > FastMath.abs(matrixT[idx - 1][idx])) { matrixT[idx - 1][idx - 1] = q / matrixT[idx][idx - 1]; matrixT[idx - 1][idx] = -(matrixT[idx][idx] - p) / matrixT[idx][idx - 1]; } else { final Complex result = cdiv(0.0, -matrixT[idx - 1][idx], matrixT[idx - 1][idx - 1] - p, q); matrixT[idx - 1][idx - 1] = result.getReal(); matrixT[idx - 1][idx] = result.getImaginary(); } matrixT[idx][idx - 1] = 0.0; matrixT[idx][idx] = 1.0; for (int i = idx - 2; i >= 0; i--) { double ra = 0.0; double sa = 0.0; for (int j = l; j <= idx; j++) { ra = ra + matrixT[i][j] * matrixT[j][idx - 1]; sa = sa + matrixT[i][j] * matrixT[j][idx]; } double w = matrixT[i][i] - p; if (Precision.compareTo(imagEigenvalues[i], 0.0, EPSILON) < 0.0) { z = w; r = ra; s = sa; } else { l = i; if (Precision.equals(imagEigenvalues[i], 0.0)) { final Complex c = cdiv(-ra, -sa, w, q); matrixT[i][idx - 1] = c.getReal(); matrixT[i][idx] = c.getImaginary(); } else { double x = matrixT[i][i + 1]; double y = matrixT[i + 1][i]; double vr = (realEigenvalues[i] - p) * (realEigenvalues[i] - p) + imagEigenvalues[i] * imagEigenvalues[i] - q * q; final double vi = (realEigenvalues[i] - p) * 2.0 * q; if (Precision.equals(vr, 0.0) && Precision.equals(vi, 0.0)) { vr = Precision.EPSILON * norm * (FastMath.abs(w) + FastMath.abs(q) + FastMath.abs(x) + FastMath.abs(y) + FastMath.abs(z)); } final Complex c = cdiv(x * r - z * ra + q * sa, x * s - z * sa - q * ra, vr, vi); matrixT[i][idx - 1] = c.getReal(); matrixT[i][idx] = c.getImaginary(); if (FastMath.abs(x) > (FastMath.abs(z) + FastMath.abs(q))) { matrixT[i + 1][idx - 1] = (-ra - w * matrixT[i][idx - 1] + q * matrixT[i][idx]) / x; matrixT[i + 1][idx] = (-sa - w * matrixT[i][idx] - q * matrixT[i][idx - 1]) / x; } else { final Complex c2 = cdiv(-r - y * matrixT[i][idx - 1], -s - y * matrixT[i][idx], z, q); matrixT[i + 1][idx - 1] = c2.getReal(); matrixT[i + 1][idx] = c2.getImaginary(); } } double t = FastMath.max(FastMath.abs(matrixT[i][idx - 1]), FastMath.abs(matrixT[i][idx])); if ((Precision.EPSILON * t) * t > 1) { for (int j = i; j <= idx; j++) { matrixT[j][idx - 1] = matrixT[j][idx - 1] / t; matrixT[j][idx] = matrixT[j][idx] / t; } } } } } } for (int i = 0; i < n; i++) { if (i < 0 | i > n - 1) { for (int j = i; j < n; j++) { matrixP[i][j] = matrixT[i][j]; } } } for (int j = n - 1; j >= 0; j--) { for (int i = 0; i <= n - 1; i++) { z = 0.0; for (int k = 0; k <= FastMath.min(j, n - 1); k++) { z = z + matrixP[i][k] * matrixT[k][j]; } matrixP[i][j] = z; } } eigenvectors = new ArrayRealVector[n]; final double[] tmp = new double[n]; for (int i = 0; i < n; i++) { for (int j = 0; j < n; j++) { tmp[j] = matrixP[j][i]; } eigenvectors[i] = new ArrayRealVector(tmp); } }"
                    ],
                    [
                        0.990585133433342,
                        "private void prelim(double[] lowerBound, double[] upperBound) { printMethod(); final int n = currentBest.getDimension(); final int npt = numberOfInterpolationPoints; final int ndim = bMatrix.getRowDimension(); final double rhosq = initialTrustRegionRadius * initialTrustRegionRadius; final double recip = 1d / rhosq; final int np = n + 1; for (int j = 0; j < n; j++) { originShift.setEntry(j, currentBest.getEntry(j)); for (int k = 0; k < npt; k++) { interpolationPoints.setEntry(k, j, ZERO); } for (int i = 0; i < ndim; i++) { bMatrix.setEntry(i, j, ZERO); } } for (int i = 0, max = n * np / 2; i < max; i++) { modelSecondDerivativesValues.setEntry(i, ZERO); } for (int k = 0; k < npt; k++) { modelSecondDerivativesParameters.setEntry(k, ZERO); for (int j = 0, max = npt - np; j < max; j++) { zMatrix.setEntry(k, j, ZERO); } } int ipt = 0; int jpt = 0; double fbeg = Double.NaN; do { final int nfm = getEvaluations(); final int nfx = nfm - n; final int nfmm = nfm - 1; final int nfxm = nfx - 1; double stepa = 0; double stepb = 0; if (nfm <= 2 * n) { if (nfm >= 1 && nfm <= n) { stepa = initialTrustRegionRadius; if (upperDifference.getEntry(nfmm) == ZERO) { stepa = -stepa; } interpolationPoints.setEntry(nfm, nfmm, stepa); } else if (nfm > n) { stepa = interpolationPoints.getEntry(nfx, nfxm); stepb = -initialTrustRegionRadius; if (lowerDifference.getEntry(nfxm) == ZERO) { stepb = Math.min(TWO * initialTrustRegionRadius, upperDifference.getEntry(nfxm)); } if (upperDifference.getEntry(nfxm) == ZERO) { stepb = Math.max(-TWO * initialTrustRegionRadius, lowerDifference.getEntry(nfxm)); } interpolationPoints.setEntry(nfm, nfxm, stepb); } } else { final int tmp1 = (nfm - np) / n; jpt = nfm - tmp1 * n - n; ipt = jpt + tmp1; if (ipt > n) { final int tmp2 = jpt; jpt = ipt - n; ipt = tmp2; } final int iptMinus1 = ipt - 1; final int jptMinus1 = jpt - 1; interpolationPoints.setEntry(nfm, iptMinus1, interpolationPoints.getEntry(ipt, iptMinus1)); interpolationPoints.setEntry(nfm, jptMinus1, interpolationPoints.getEntry(jpt, jptMinus1)); } for (int j = 0; j < n; j++) { currentBest.setEntry(j, Math.min(Math.max(lowerBound[j], originShift.getEntry(j) + interpolationPoints.getEntry(nfm, j)), upperBound[j])); if (interpolationPoints.getEntry(nfm, j) == lowerDifference.getEntry(j)) { currentBest.setEntry(j, lowerBound[j]); } if (interpolationPoints.getEntry(nfm, j) == upperDifference.getEntry(j)) { currentBest.setEntry(j, upperBound[j]); } } final double objectiveValue = computeObjectiveValue(currentBest.toArray()); final double f = isMinimize ? objectiveValue : -objectiveValue; final int numEval = getEvaluations(); fAtInterpolationPoints.setEntry(nfm, f); if (numEval == 1) { fbeg = f; trustRegionCenterInterpolationPointIndex = 0; } else if (f < fAtInterpolationPoints.getEntry(trustRegionCenterInterpolationPointIndex)) { trustRegionCenterInterpolationPointIndex = nfm; } if (numEval <= 2 * n + 1) { if (numEval >= 2 && numEval <= n + 1) { gradientAtTrustRegionCenter.setEntry(nfmm, (f - fbeg) / stepa); if (npt < numEval + n) { final double oneOverStepA = ONE / stepa; bMatrix.setEntry(0, nfmm, -oneOverStepA); bMatrix.setEntry(nfm, nfmm, oneOverStepA); bMatrix.setEntry(npt + nfmm, nfmm, -HALF * rhosq); } } else if (numEval >= n + 2) { final int ih = nfx * (nfx + 1) / 2 - 1; final double tmp = (f - fbeg) / stepb; final double diff = stepb - stepa; modelSecondDerivativesValues.setEntry(ih, TWO * (tmp - gradientAtTrustRegionCenter.getEntry(nfxm)) / diff); gradientAtTrustRegionCenter.setEntry(nfxm, (gradientAtTrustRegionCenter.getEntry(nfxm) * stepb - tmp * stepa) / diff); if (stepa * stepb < ZERO) { if (f < fAtInterpolationPoints.getEntry(nfm - n)) { fAtInterpolationPoints.setEntry(nfm, fAtInterpolationPoints.getEntry(nfm - n)); fAtInterpolationPoints.setEntry(nfm - n, f); if (trustRegionCenterInterpolationPointIndex == nfm) { trustRegionCenterInterpolationPointIndex = nfm - n; } interpolationPoints.setEntry(nfm - n, nfxm, stepb); interpolationPoints.setEntry(nfm, nfxm, stepa); } } bMatrix.setEntry(0, nfxm, -(stepa + stepb) / (stepa * stepb)); bMatrix.setEntry(nfm, nfxm, -HALF / interpolationPoints.getEntry(nfm - n, nfxm)); bMatrix.setEntry(nfm - n, nfxm, -bMatrix.getEntry(0, nfxm) - bMatrix.getEntry(nfm, nfxm)); zMatrix.setEntry(0, nfxm, Math.sqrt(TWO) / (stepa * stepb)); zMatrix.setEntry(nfm, nfxm, Math.sqrt(HALF) / rhosq); zMatrix.setEntry(nfm - n, nfxm, -zMatrix.getEntry(0, nfxm) - zMatrix.getEntry(nfm, nfxm)); } } else { zMatrix.setEntry(0, nfxm, recip); zMatrix.setEntry(nfm, nfxm, recip); zMatrix.setEntry(ipt, nfxm, -recip); zMatrix.setEntry(jpt, nfxm, -recip); final int ih = ipt * (ipt - 1) / 2 + jpt - 1; final double tmp = interpolationPoints.getEntry(nfm, ipt - 1) * interpolationPoints.getEntry(nfm, jpt - 1); modelSecondDerivativesValues.setEntry(ih, (fbeg - fAtInterpolationPoints.getEntry(ipt) - fAtInterpolationPoints.getEntry(jpt) + f) / tmp); } } while (getEvaluations() < npt); }"
                    ],
                    [
                        0.990585133433342,
                        "private void prelim(double[] lowerBound, double[] upperBound) { printMethod(); final int n = currentBest.getDimension(); final int npt = numberOfInterpolationPoints; final int ndim = bMatrix.getRowDimension(); final double rhosq = initialTrustRegionRadius * initialTrustRegionRadius; final double recip = 1d / rhosq; final int np = n + 1; for (int j = 0; j < n; j++) { originShift.setEntry(j, currentBest.getEntry(j)); for (int k = 0; k < npt; k++) { interpolationPoints.setEntry(k, j, ZERO); } for (int i = 0; i < ndim; i++) { bMatrix.setEntry(i, j, ZERO); } } for (int i = 0, max = n * np / 2; i < max; i++) { modelSecondDerivativesValues.setEntry(i, ZERO); } for (int k = 0; k < npt; k++) { modelSecondDerivativesParameters.setEntry(k, ZERO); for (int j = 0, max = npt - np; j < max; j++) { zMatrix.setEntry(k, j, ZERO); } } int ipt = 0; int jpt = 0; double fbeg = Double.NaN; do { final int nfm = getEvaluations(); final int nfx = nfm - n; final int nfmm = nfm - 1; final int nfxm = nfx - 1; double stepa = 0; double stepb = 0; if (nfm <= 2 * n) { if (nfm >= 1 && nfm <= n) { stepa = initialTrustRegionRadius; if (upperDifference.getEntry(nfmm) == ZERO) { stepa = -stepa; } interpolationPoints.setEntry(nfm, nfmm, stepa); } else if (nfm > n) { stepa = interpolationPoints.getEntry(nfx, nfxm); stepb = -initialTrustRegionRadius; if (lowerDifference.getEntry(nfxm) == ZERO) { stepb = Math.min(TWO * initialTrustRegionRadius, upperDifference.getEntry(nfxm)); } if (upperDifference.getEntry(nfxm) == ZERO) { stepb = Math.max(-TWO * initialTrustRegionRadius, lowerDifference.getEntry(nfxm)); } interpolationPoints.setEntry(nfm, nfxm, stepb); } } else { final int tmp1 = (nfm - np) / n; jpt = nfm - tmp1 * n - n; ipt = jpt + tmp1; if (ipt > n) { final int tmp2 = jpt; jpt = ipt - n; ipt = tmp2; } final int iptMinus1 = ipt - 1; final int jptMinus1 = jpt - 1; interpolationPoints.setEntry(nfm, iptMinus1, interpolationPoints.getEntry(ipt, iptMinus1)); interpolationPoints.setEntry(nfm, jptMinus1, interpolationPoints.getEntry(jpt, jptMinus1)); } for (int j = 0; j < n; j++) { currentBest.setEntry(j, Math.min(Math.max(lowerBound[j], originShift.getEntry(j) + interpolationPoints.getEntry(nfm, j)), upperBound[j])); if (interpolationPoints.getEntry(nfm, j) == lowerDifference.getEntry(j)) { currentBest.setEntry(j, lowerBound[j]); } if (interpolationPoints.getEntry(nfm, j) == upperDifference.getEntry(j)) { currentBest.setEntry(j, upperBound[j]); } } final double objectiveValue = computeObjectiveValue(currentBest.toArray()); final double f = isMinimize ? objectiveValue : -objectiveValue; final int numEval = getEvaluations(); fAtInterpolationPoints.setEntry(nfm, f); if (numEval == 1) { fbeg = f; trustRegionCenterInterpolationPointIndex = 0; } else if (f < fAtInterpolationPoints.getEntry(trustRegionCenterInterpolationPointIndex)) { trustRegionCenterInterpolationPointIndex = nfm; } if (numEval <= 2 * n + 1) { if (numEval >= 2 && numEval <= n + 1) { gradientAtTrustRegionCenter.setEntry(nfmm, (f - fbeg) / stepa); if (npt < numEval + n) { final double oneOverStepA = ONE / stepa; bMatrix.setEntry(0, nfmm, -oneOverStepA); bMatrix.setEntry(nfm, nfmm, oneOverStepA); bMatrix.setEntry(npt + nfmm, nfmm, -HALF * rhosq); } } else if (numEval >= n + 2) { final int ih = nfx * (nfx + 1) / 2 - 1; final double tmp = (f - fbeg) / stepb; final double diff = stepb - stepa; modelSecondDerivativesValues.setEntry(ih, TWO * (tmp - gradientAtTrustRegionCenter.getEntry(nfxm)) / diff); gradientAtTrustRegionCenter.setEntry(nfxm, (gradientAtTrustRegionCenter.getEntry(nfxm) * stepb - tmp * stepa) / diff); if (stepa * stepb < ZERO) { if (f < fAtInterpolationPoints.getEntry(nfm - n)) { fAtInterpolationPoints.setEntry(nfm, fAtInterpolationPoints.getEntry(nfm - n)); fAtInterpolationPoints.setEntry(nfm - n, f); if (trustRegionCenterInterpolationPointIndex == nfm) { trustRegionCenterInterpolationPointIndex = nfm - n; } interpolationPoints.setEntry(nfm - n, nfxm, stepb); interpolationPoints.setEntry(nfm, nfxm, stepa); } } bMatrix.setEntry(0, nfxm, -(stepa + stepb) / (stepa * stepb)); bMatrix.setEntry(nfm, nfxm, -HALF / interpolationPoints.getEntry(nfm - n, nfxm)); bMatrix.setEntry(nfm - n, nfxm, -bMatrix.getEntry(0, nfxm) - bMatrix.getEntry(nfm, nfxm)); zMatrix.setEntry(0, nfxm, Math.sqrt(TWO) / (stepa * stepb)); zMatrix.setEntry(nfm, nfxm, Math.sqrt(HALF) / rhosq); zMatrix.setEntry(nfm - n, nfxm, -zMatrix.getEntry(0, nfxm) - zMatrix.getEntry(nfm, nfxm)); } } else { zMatrix.setEntry(0, nfxm, recip); zMatrix.setEntry(nfm, nfxm, recip); zMatrix.setEntry(ipt, nfxm, -recip); zMatrix.setEntry(jpt, nfxm, -recip); final int ih = ipt * (ipt - 1) / 2 + jpt - 1; final double tmp = interpolationPoints.getEntry(nfm, ipt - 1) * interpolationPoints.getEntry(nfm, jpt - 1); modelSecondDerivativesValues.setEntry(ih, (fbeg - fAtInterpolationPoints.getEntry(ipt) - fAtInterpolationPoints.getEntry(jpt) + f) / tmp); } } while (getEvaluations() < npt); }"
                    ],
                    [
                        0.9902382344007492,
                        "@Override public void integrate(final ExpandableStatefulODE equations, final double t) throws NumberIsTooSmallException, DimensionMismatchException, MaxCountExceededException, NoBracketingException { sanityChecks(equations, t); setEquations(equations); final boolean forward = t > equations.getTime(); final double[] y0 = equations.getCompleteState(); final double[] y = y0.clone(); final double[] yDot = new double[y.length]; final NordsieckStepInterpolator interpolator = new NordsieckStepInterpolator(); interpolator.reinitialize(y, forward, equations.getPrimaryMapper(), equations.getSecondaryMappers()); initIntegration(equations.getTime(), y0, t); start(equations.getTime(), y, t); interpolator.reinitialize(stepStart, stepSize, scaled, nordsieck); interpolator.storeTime(stepStart); final int lastRow = nordsieck.getRowDimension() - 1; double hNew = stepSize; interpolator.rescale(hNew); isLastStep = false; do { double error = 10; while (error >= 1.0) { stepSize = hNew; error = 0; for (int i = 0; i < mainSetDimension; ++i) { final double yScale = FastMath.abs(y[i]); final double tol = (vecAbsoluteTolerance == null) ? (scalAbsoluteTolerance + scalRelativeTolerance * yScale) : (vecAbsoluteTolerance[i] + vecRelativeTolerance[i] * yScale); final double ratio = nordsieck.getEntry(lastRow, i) / tol; error += ratio * ratio; } error = FastMath.sqrt(error / mainSetDimension); if (error >= 1.0) { final double factor = computeStepGrowShrinkFactor(error); hNew = filterStep(stepSize * factor, forward, false); interpolator.rescale(hNew); } } final double stepEnd = stepStart + stepSize; interpolator.shift(); interpolator.setInterpolatedTime(stepEnd); System.arraycopy(interpolator.getInterpolatedState(), 0, y, 0, y0.length); computeDerivatives(stepEnd, y, yDot); final double[] predictedScaled = new double[y0.length]; for (int j = 0; j < y0.length; ++j) { predictedScaled[j] = stepSize * yDot[j]; } final Array2DRowRealMatrix nordsieckTmp = updateHighOrderDerivativesPhase1(nordsieck); updateHighOrderDerivativesPhase2(scaled, predictedScaled, nordsieckTmp); interpolator.reinitialize(stepEnd, stepSize, predictedScaled, nordsieckTmp); interpolator.storeTime(stepEnd); stepStart = acceptStep(interpolator, y, yDot, t); scaled = predictedScaled; nordsieck = nordsieckTmp; interpolator.reinitialize(stepEnd, stepSize, scaled, nordsieck); if (!isLastStep) { interpolator.storeTime(stepStart); if (resetOccurred) { start(stepStart, y, t); interpolator.reinitialize(stepStart, stepSize, scaled, nordsieck); } final double factor = computeStepGrowShrinkFactor(error); final double scaledH = stepSize * factor; final double nextT = stepStart + scaledH; final boolean nextIsLast = forward ? (nextT >= t) : (nextT <= t); hNew = filterStep(scaledH, forward, nextIsLast); final double filteredNextT = stepStart + hNew; final boolean filteredNextIsLast = forward ? (filteredNextT >= t) : (filteredNextT <= t); if (filteredNextIsLast) { hNew = t - stepStart; } interpolator.rescale(hNew); } } while (!isLastStep); equations.setTime(stepStart); equations.setCompleteState(y); resetInternalState(); }"
                    ],
                    [
                        0.9897048473358154,
                        "public static long stirlingS2(final int n, final int k) throws NotPositiveException, NumberIsTooLargeException, MathArithmeticException { if (k < 0) { throw new NotPositiveException(k); } if (k > n) { throw new NumberIsTooLargeException(k, n, true); } long[][] stirlingS2 = STIRLING_S2.get(); if (stirlingS2 == null) { final int maxIndex = 26; stirlingS2 = new long[maxIndex][]; stirlingS2[0] = new long[] { 1l }; for (int i = 1; i < stirlingS2.length; ++i) { stirlingS2[i] = new long[i + 1]; stirlingS2[i][0] = 0; stirlingS2[i][1] = 1; stirlingS2[i][i] = 1; for (int j = 2; j < i; ++j) { stirlingS2[i][j] = j * stirlingS2[i - 1][j] + stirlingS2[i - 1][j - 1]; } } STIRLING_S2.compareAndSet(null, stirlingS2); } if (n < stirlingS2.length) { return stirlingS2[n][k]; } else { if (k == 0) { return 0; } else if (k == 1 || k == n) { return 1; } else if (k == 2) { return (1l << (n - 1)) - 1l; } else if (k == n - 1) { return binomialCoefficient(n, 2); } else { long sum = 0; long sign = ((k & 0x1) == 0) ? 1 : -1; for (int j = 1; j <= k; ++j) { sign = -sign; sum += sign * binomialCoefficient(k, j) * pow(j, n); if (sum < 0) { throw new MathArithmeticException(LocalizedFormats.ARGUMENT_OUTSIDE_DOMAIN, n, 0, stirlingS2.length - 1); } } return sum / factorial(k); } } }"
                    ],
                    [
                        0.9896530508995056,
                        "@Test public void testPreconditionedSolution2() { final int n = 100; final int maxIterations = 100000; final Array2DRowRealMatrix a = new Array2DRowRealMatrix(n, n); double daux = 1.; for (int i = 0; i < n; i++) { a.setEntry(i, i, daux); daux *= 1.2; for (int j = i + 1; j < n; j++) { if (i == j) { } else { final double value = 1.0; a.setEntry(i, j, value); a.setEntry(j, i, value); } } } final RealLinearOperator m = JacobiPreconditioner.create(a); final PreconditionedIterativeLinearSolver pcg; final IterativeLinearSolver cg; pcg = new ConjugateGradient(maxIterations, 1E-6, true); cg = new ConjugateGradient(maxIterations, 1E-6, true); final RealVector b = new ArrayRealVector(n); final String pattern = \\\"preconditioned gradient (%d iterations) should\\\" + \\\" have been faster than unpreconditioned (%d iterations)\\\"; String msg; for (int j = 0; j < 1; j++) { b.set(0.); b.setEntry(j, 1.); final RealVector px = pcg.solve(a, m, b); final RealVector x = cg.solve(a, b); final int npcg = pcg.getIterationManager().getIterations(); final int ncg = cg.getIterationManager().getIterations(); msg = String.format(pattern, npcg, ncg); Assert.assertTrue(msg, npcg < ncg); for (int i = 0; i < n; i++) { msg = String.format(\\\"row %d, column %d\\\", i, j); final double expected = x.getEntry(i); final double actual = px.getEntry(i); final double delta = 1E-6 * Math.abs(expected); Assert.assertEquals(msg, expected, actual, delta); } } }"
                    ]
                ],
                "directly_related_tests": [
                    "org.apache.commons.math3.genetics.FixedElapsedTimeTest::testIsSatisfied"
                ],
                "generation": {
                    "1": {
                        "prompt": "You are an expert Java developer. Below is a buggy method from a large Java project. This method causes one or more test failures.\nYour task is to fix the bug in this method. Use the provided test failure(s) to guide your reasoning.\n\n---\n\n## Buggy Method\n\n```java\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointVectorValuePair doOptimize() {\n        checkParameters();\n\n        final int nR = getTarget().length; // Number of observed data.\n        final double[] currentPoint = getStartPoint();\n        final int nC = currentPoint.length; // Number of parameters.\n\n        // arrays shared with the other private methods\n        solvedCols  = FastMath.min(nR, nC);\n        diagR       = new double[nC];\n        jacNorm     = new double[nC];\n        beta        = new double[nC];\n        permutation = new int[nC];\n        lmDir       = new double[nC];\n\n        // local point\n        double   delta   = 0;\n        double   xNorm   = 0;\n        double[] diag    = new double[nC];\n        double[] oldX    = new double[nC];\n        double[] oldRes  = new double[nR];\n        double[] oldObj  = new double[nR];\n        double[] qtf     = new double[nR];\n        double[] work1   = new double[nC];\n        double[] work2   = new double[nC];\n        double[] work3   = new double[nC];\n\n        final RealMatrix weightMatrixSqrt = getWeightSquareRoot();\n\n        // Evaluate the function at the starting point and calculate its norm.\n        double[] currentObjective = computeObjectiveValue(currentPoint);\n        double[] currentResiduals = computeResiduals(currentObjective);\n        PointVectorValuePair current = new PointVectorValuePair(currentPoint, currentObjective);\n        double currentCost = computeCost(currentResiduals);\n\n        // Outer loop.\n        lmPar = 0;\n        boolean firstIteration = true;\n        int iter = 0;\n        final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n        while (true) {\n            ++iter;\n            final PointVectorValuePair previous = current;\n\n            // QR decomposition of the jacobian matrix\n            qrDecomposition(computeWeightedJacobian(currentPoint));\n\n            weightedResidual = weightMatrixSqrt.operate(currentResiduals);\n            for (int i = 0; i < nR; i++) {\n                qtf[i] = weightedResidual[i];\n            }\n\n            // compute Qt.res\n            qTy(qtf);\n\n            // now we don't need Q anymore,\n            // so let jacobian contain the R matrix with its diagonal elements\n            for (int k = 0; k < solvedCols; ++k) {\n                int pk = permutation[k];\n                weightedJacobian[k][pk] = diagR[pk];\n            }\n\n            if (firstIteration) {\n                // scale the point according to the norms of the columns\n                // of the initial jacobian\n                xNorm = 0;\n                for (int k = 0; k < nC; ++k) {\n                    double dk = jacNorm[k];\n                    if (dk == 0) {\n                        dk = 1.0;\n                    }\n                    double xk = dk * currentPoint[k];\n                    xNorm  += xk * xk;\n                    diag[k] = dk;\n                }\n                xNorm = FastMath.sqrt(xNorm);\n\n                // initialize the step bound delta\n                delta = (xNorm == 0) ? initialStepBoundFactor : (initialStepBoundFactor * xNorm);\n            }\n\n            // check orthogonality between function vector and jacobian columns\n            double maxCosine = 0;\n            if (currentCost != 0) {\n                for (int j = 0; j < solvedCols; ++j) {\n                    int    pj = permutation[j];\n                    double s  = jacNorm[pj];\n                    if (s != 0) {\n                        double sum = 0;\n                        for (int i = 0; i <= j; ++i) {\n                            sum += weightedJacobian[i][pj] * qtf[i];\n                        }\n                        maxCosine = FastMath.max(maxCosine, FastMath.abs(sum) / (s * currentCost));\n                    }\n                }\n            }\n            if (maxCosine <= orthoTolerance) {\n                // Convergence has been reached.\n                setCost(currentCost);\n                return current;\n            }\n\n            // rescale if necessary\n            for (int j = 0; j < nC; ++j) {\n                diag[j] = FastMath.max(diag[j], jacNorm[j]);\n            }\n\n            // Inner loop.\n            for (double ratio = 0; ratio < 1.0e-4;) {\n\n                // save the state\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    oldX[pj] = currentPoint[pj];\n                }\n                final double previousCost = currentCost;\n                double[] tmpVec = weightedResidual;\n                weightedResidual = oldRes;\n                oldRes    = tmpVec;\n                tmpVec    = currentObjective;\n                currentObjective = oldObj;\n                oldObj    = tmpVec;\n\n                // determine the Levenberg-Marquardt parameter\n                determineLMParameter(qtf, delta, diag, work1, work2, work3);\n\n                // compute the new point and the norm of the evolution direction\n                double lmNorm = 0;\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    lmDir[pj] = -lmDir[pj];\n                    currentPoint[pj] = oldX[pj] + lmDir[pj];\n                    double s = diag[pj] * lmDir[pj];\n                    lmNorm  += s * s;\n                }\n                lmNorm = FastMath.sqrt(lmNorm);\n                // on the first iteration, adjust the initial step bound.\n                if (firstIteration) {\n                    delta = FastMath.min(delta, lmNorm);\n                }\n\n                // Evaluate the function at x + p and calculate its norm.\n                currentObjective = computeObjectiveValue(currentPoint);\n                currentResiduals = computeResiduals(currentObjective);\n                current = new PointVectorValuePair(currentPoint, currentObjective);\n                currentCost = computeCost(currentResiduals);\n\n                // compute the scaled actual reduction\n                double actRed = -1.0;\n                if (0.1 * currentCost < previousCost) {\n                    double r = currentCost / previousCost;\n                    actRed = 1.0 - r * r;\n                }\n\n                // compute the scaled predicted reduction\n                // and the scaled directional derivative\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    double dirJ = lmDir[pj];\n                    work1[j] = 0;\n                    for (int i = 0; i <= j; ++i) {\n                        work1[i] += weightedJacobian[i][pj] * dirJ;\n                    }\n                }\n                double coeff1 = 0;\n                for (int j = 0; j < solvedCols; ++j) {\n                    coeff1 += work1[j] * work1[j];\n                }\n                double pc2 = previousCost * previousCost;\n                coeff1 = coeff1 / pc2;\n                double coeff2 = lmPar * lmNorm * lmNorm / pc2;\n                double preRed = coeff1 + 2 * coeff2;\n                double dirDer = -(coeff1 + coeff2);\n\n                // ratio of the actual to the predicted reduction\n                ratio = (preRed == 0) ? 0 : (actRed / preRed);\n\n                // update the step bound\n                if (ratio <= 0.25) {\n                    double tmp =\n                        (actRed < 0) ? (0.5 * dirDer / (dirDer + 0.5 * actRed)) : 0.5;\n                        if ((0.1 * currentCost >= previousCost) || (tmp < 0.1)) {\n                            tmp = 0.1;\n                        }\n                        delta = tmp * FastMath.min(delta, 10.0 * lmNorm);\n                        lmPar /= tmp;\n                } else if ((lmPar == 0) || (ratio >= 0.75)) {\n                    delta = 2 * lmNorm;\n                    lmPar *= 0.5;\n                }\n\n                // test for successful iteration.\n                if (ratio >= 1.0e-4) {\n                    // successful iteration, update the norm\n                    firstIteration = false;\n                    xNorm = 0;\n                    for (int k = 0; k < nC; ++k) {\n                        double xK = diag[k] * currentPoint[k];\n                        xNorm += xK * xK;\n                    }\n                    xNorm = FastMath.sqrt(xNorm);\n\n                    // tests for convergence.\n                    if (checker != null) {\n                        // we use the vectorial convergence checker\n                        if (checker.converged(iter, previous, current)) {\n                            setCost(currentCost);\n                            return current;\n                        }\n                    }\n                } else {\n                    // failed iteration, reset the previous values\n                    currentCost = previousCost;\n                    for (int j = 0; j < solvedCols; ++j) {\n                        int pj = permutation[j];\n                        currentPoint[pj] = oldX[pj];\n                    }\n                    tmpVec    = weightedResidual;\n                    weightedResidual = oldRes;\n                    oldRes    = tmpVec;\n                    tmpVec    = currentObjective;\n                    currentObjective = oldObj;\n                    oldObj    = tmpVec;\n                    // Reset \"current\" to previous values.\n                    current = new PointVectorValuePair(currentPoint, currentObjective);\n                }\n\n                // Default convergence criteria.\n                if ((FastMath.abs(actRed) <= costRelativeTolerance &&\n                     preRed <= costRelativeTolerance &&\n                     ratio <= 2.0) ||\n                    delta <= parRelativeTolerance * xNorm) {\n                    setCost(currentCost);\n                    return current;\n                }\n\n                // tests for termination and stringent tolerances\n                // (2.2204e-16 is the machine epsilon for IEEE754)\n                if ((FastMath.abs(actRed) <= 2.2204e-16) && (preRed <= 2.2204e-16) && (ratio <= 2.0)) {\n                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_COST_RELATIVE_TOLERANCE,\n                                                   costRelativeTolerance);\n                } else if (delta <= 2.2204e-16 * xNorm) {\n                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE,\n                                                   parRelativeTolerance);\n                } else if (maxCosine <= 2.2204e-16)  {\n                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_ORTHOGONALITY_TOLERANCE,\n                                                   orthoTolerance);\n                }\n            }\n        }\n    }\n\n```\n---\n\n## Failing Test Case(s)\n\n\n---\n\n## Your Goal\n\nFix the buggy method. Return only the fixed Java method. Do not include explanations or other text.\n",
                        "patches": [
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointVectorValuePair doOptimize() {\n        checkParameters();\n\n        final int nR = getTarget().length; // Number of observed data.\n        final double[] currentPoint = getStartPoint();\n        final int nC = currentPoint.length; // Number of parameters.\n\n        // arrays shared with the other private methods\n        solvedCols  = FastMath.min(nR, nC);\n        diagR       = new double[nC];\n        jacNorm     = new double[nC];\n        beta        = new double[nC];\n        permutation = new int[nC];\n        lmDir       = new double[nC];\n\n        // local point\n        double   delta   = 0;\n        double   xNorm   = 0;\n        double[] diag    = new double[nC];\n        double[] oldX    = new double[nC];\n        double[] oldRes  = new double[nR];\n        double[] oldObj  = new double[nR];\n        double[] qtf     = new double[nR];\n        double[] work1   = new double[nC];\n        double[] work2   = new double[nC];\n        double[] work3   = new double[nC];\n\n        final RealMatrix weightMatrixSqrt = getWeightSquareRoot();\n\n        // Evaluate the function at the starting point and calculate its norm.\n        double[] currentObjective = computeObjectiveValue(currentPoint);\n        double[] currentResiduals = computeResiduals(currentObjective);\n        PointVectorValuePair current = new PointVectorValuePair(currentPoint, currentObjective);\n        double currentCost = computeCost(currentResiduals);\n\n        // Outer loop.\n        lmPar = 0;\n        boolean firstIteration = true;\n        int iter = 0;\n        final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n        while (true) {\n            ++iter;\n            final PointVectorValuePair previous = current;\n\n            // QR decomposition of the jacobian matrix\n            qrDecomposition(computeWeightedJacobian(currentPoint));\n\n            weightedResidual = weightMatrixSqrt.operate(currentResiduals);\n            for (int i = 0; i < nR; i++) {\n                qtf[i] = weightedResidual[i];\n            }\n\n            // compute Qt.res\n            qTy(qtf);\n\n            // now we don't need Q anymore,\n            // so let jacobian contain the R matrix with its diagonal elements\n            for (int k = 0; k < solvedCols; ++k) {\n                int pk = permutation[k];\n                weightedJacobian[k][pk] = diagR[pk];\n            }\n\n            if (firstIteration) {\n                // scale the point according to the norms of the columns\n                // of the initial jacobian\n                xNorm = 0;\n                for (int k = 0; k < nC; ++k) {\n                    double dk = jacNorm[k];\n                    if (dk == 0) {\n                        dk = 1.0;\n                    }\n                    double xk = dk * currentPoint[k];\n                    xNorm  += xk * xk;\n                    diag[k] = dk;\n                }\n                xNorm = FastMath.sqrt(xNorm);\n\n                // initialize the step bound delta\n                delta = (xNorm == 0) ? initialStepBoundFactor : (initialStepBoundFactor * xNorm);\n            }\n\n            // check orthogonality between function vector and jacobian columns\n            double maxCosine = 0;\n            if (currentCost != 0) {\n                for (int j = 0; j < solvedCols; ++j) {\n                    int    pj = permutation[j];\n                    double s  = jacNorm[pj];\n                    if (s != 0) {\n                        double sum = 0;\n                        for (int i = 0; i <= j; ++i) {\n                            sum += weightedJacobian[i][pj] * qtf[i];\n                        }\n                        maxCosine = FastMath.max(maxCosine, FastMath.abs(sum) / (s * currentCost));\n                    }\n                }\n            }\n            if (maxCosine <= orthoTolerance) {\n                // Convergence has been reached.\n                setCost(currentCost);\n                return current;\n            }\n\n            // rescale if necessary\n            for (int j = 0; j < nC; ++j) {\n                diag[j] = FastMath.max(diag[j], jacNorm[j]);\n            }\n\n            // Inner loop.\n            for (double ratio = 0; ratio < 1.0e-4;) {\n\n                // save the state\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    oldX[pj] = currentPoint[pj];\n                }\n                final double previousCost = currentCost;\n                double[] tmpVec = weightedResidual;\n                weightedResidual = oldRes;\n                oldRes    = tmpVec;\n                tmpVec    = currentObjective;\n                currentObjective = oldObj;\n                oldObj    = tmpVec;\n\n                // determine the Levenberg-Marquardt parameter\n                determineLMParameter(qtf, delta, diag, work1, work2, work3);\n\n                // compute the new point and the norm of the evolution direction\n                double lmNorm = 0;\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    lmDir[pj] = -lmDir[pj];\n                    currentPoint[pj] = oldX[pj] + lmDir[pj];\n                    double s = diag[pj] * lmDir[pj];\n                    lmNorm  += s * s;\n                }\n                lmNorm = FastMath.sqrt(lmNorm);\n                // on the first iteration, adjust the initial step bound.\n                if (firstIteration) {\n                    delta = FastMath.min(delta, lmNorm);\n                }\n\n                // Evaluate the function at x + p and calculate its norm.\n                currentObjective = computeObjectiveValue(currentPoint);\n                currentResiduals = computeResiduals(currentObjective);\n                current = new PointVectorValuePair(currentPoint, currentObjective);\n                currentCost = computeCost(currentResiduals);\n\n                // compute the scaled actual reduction\n                double actRed = -1.0;\n                if (0.1 * currentCost < previousCost) {\n                    double r = currentCost / previousCost;\n                    actRed = 1.0 - r * r;\n                }\n\n                // compute the scaled predicted reduction\n                // and the scaled directional derivative\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    double dirJ = lmDir[pj];\n                    work1[j] = 0;\n                    for (int i = 0; i <= j; ++i) {\n                        work1[i] += weightedJacobian[i][pj] * dirJ;\n                    }\n                }\n                double coeff1 = 0;\n                for (int j = 0; j < solvedCols; ++j) {\n                    coeff1 += work1[j] * work1[j];\n                }\n                double pc2 = previousCost * previousCost;\n                coeff1 = coeff1 / pc2;\n                double coeff2 = lmPar * lmNorm * lmNorm / pc2;\n                double preRed = coeff1 + 2 * coeff2;\n                double dirDer = -(coeff1 + coeff2);\n\n                // ratio of the actual to the predicted reduction\n                ratio = (preRed == 0) ? 0 : (actRed / preRed);\n\n                // update the step bound\n                if (ratio <= 0.25) {\n                    double tmp =\n                        (actRed < 0) ? (0.5 * dirDer / (dirDer + 0.5 * actRed)) : 0.5;\n                    if ((0.1 * currentCost >= previousCost) || (tmp < 0.1)) {\n                        tmp = 0.1;\n                    }\n                    delta = tmp * FastMath.min(delta, 10.0 * lmNorm);\n                    lmPar /= tmp;\n                } else if ((lmPar == 0) || (ratio >= 0.75)) {\n                    delta = 2 * lmNorm;\n                    lmPar *= 0.5;\n                }\n\n                // test for successful iteration.\n                if (ratio >= 1.0e-4) {\n                    // successful iteration, update the norm\n                    firstIteration = false;\n                    xNorm = 0;\n                    for (int k = 0; k < nC; ++k) {\n                        double xK = diag[k] * currentPoint[k];\n                        xNorm += xK * xK;\n                    }\n                    xNorm = FastMath.sqrt(xNorm);\n\n                    // tests for convergence.\n                    if (checker != null) {\n                        // we use the vectorial convergence checker\n                        if (checker.converged(iter, previous, current)) {\n                            setCost(currentCost);\n                            return current;\n                        }\n                    }\n                } else {\n                    // failed iteration, reset the previous values\n                    currentCost = previousCost;\n                    for (int j = 0; j < solvedCols; ++j) {\n                        int pj = permutation[j];\n                        currentPoint[pj] = oldX[pj];\n                    }\n                    tmpVec    = weightedResidual;\n                    weightedResidual = oldRes;\n                    oldRes    = tmpVec;\n                    tmpVec    = currentObjective;\n                    currentObjective = oldObj;\n                    oldObj    = tmpVec;\n                    // Reset \"current\" to previous values.\n                    current = new PointVectorValuePair(currentPoint, currentObjective);\n                }\n\n                // Default convergence criteria.\n                if ((FastMath.abs(actRed) <= costRelativeTolerance &&\n                     preRed <= costRelativeTolerance &&\n                     ratio <= 2.0) ||\n                    delta <= parRelativeTolerance * xNorm) {\n                    setCost(currentCost);\n                    return current;\n                }\n\n                // tests for termination and stringent tolerances\n                // (2.2204e-16 is the machine epsilon for IEEE754)\n                if ((FastMath.abs(actRed) <= 2.2204e-16) && (preRed <= 2.2204e-16) && (ratio <= 2.0)) {\n                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_COST_RELATIVE_TOLERANCE,\n                                                   costRelativeTolerance);\n                } else if (delta <= 2.2204e-16 * xNorm) {\n                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE,\n                                                   parRelativeTolerance);\n                } else if (maxCosine <= 2.2204e-16)  {\n                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_ORTHOGONALITY_TOLERANCE,\n                                                   orthoTolerance);\n                }\n            }\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointVectorValuePair doOptimize() {\n        checkParameters();\n\n        final int nR = getTarget().length; // Number of observed data.\n        final double[] currentPoint = getStartPoint();\n        final int nC = currentPoint.length; // Number of parameters.\n\n        // arrays shared with the other private methods\n        solvedCols  = FastMath.min(nR, nC);\n        diagR       = new double[nC];\n        jacNorm     = new double[nC];\n        beta        = new double[nC];\n        permutation = new int[nC];\n        lmDir       = new double[nC];\n\n        // local point\n        double   delta   = 0;\n        double   xNorm   = 0;\n        double[] diag    = new double[nC];\n        double[] oldX    = new double[nC];\n        double[] oldRes  = new double[nR];\n        double[] oldObj  = new double[nR];\n        double[] qtf     = new double[nR];\n        double[] work1   = new double[nC];\n        double[] work2   = new double[nC];\n        double[] work3   = new double[nC];\n\n        final RealMatrix weightMatrixSqrt = getWeightSquareRoot();\n\n        // Evaluate the function at the starting point and calculate its norm.\n        double[] currentObjective = computeObjectiveValue(currentPoint);\n        double[] currentResiduals = computeResiduals(currentObjective);\n        PointVectorValuePair current = new PointVectorValuePair(currentPoint, currentObjective);\n        double currentCost = computeCost(currentResiduals);\n\n        // Outer loop.\n        lmPar = 0;\n        boolean firstIteration = true;\n        int iter = 0;\n        final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n        while (true) {\n            ++iter;\n            final PointVectorValuePair previous = current;\n\n            // QR decomposition of the jacobian matrix\n            qrDecomposition(computeWeightedJacobian(currentPoint));\n\n            weightedResidual = weightMatrixSqrt.operate(currentResiduals);\n            for (int i = 0; i < nR; i++) {\n                qtf[i] = weightedResidual[i];\n            }\n\n            // compute Qt.res\n            qTy(qtf);\n\n            // now we don't need Q anymore,\n            // so let jacobian contain the R matrix with its diagonal elements\n            for (int k = 0; k < solvedCols; ++k) {\n                int pk = permutation[k];\n                weightedJacobian[k][pk] = diagR[pk];\n            }\n\n            if (firstIteration) {\n                // scale the point according to the norms of the columns\n                // of the initial jacobian\n                xNorm = 0;\n                for (int k = 0; k < nC; ++k) {\n                    double dk = jacNorm[k];\n                    if (dk == 0) {\n                        dk = 1.0;\n                    }\n                    double xk = dk * currentPoint[k];\n                    xNorm  += xk * xk;\n                    diag[k] = dk;\n                }\n                xNorm = FastMath.sqrt(xNorm);\n\n                // initialize the step bound delta\n                delta = (xNorm == 0) ? initialStepBoundFactor : (initialStepBoundFactor * xNorm);\n            }\n\n            // check orthogonality between function vector and jacobian columns\n            double maxCosine = 0;\n            if (currentCost != 0) {\n                for (int j = 0; j < solvedCols; ++j) {\n                    int    pj = permutation[j];\n                    double s  = jacNorm[pj];\n                    if (s != 0) {\n                        double sum = 0;\n                        for (int i = 0; i <= j; ++i) {\n                            sum += weightedJacobian[i][pj] * qtf[i];\n                        }\n                        maxCosine = FastMath.max(maxCosine, FastMath.abs(sum) / (s * currentCost));\n                    }\n                }\n            }\n            if (maxCosine <= orthoTolerance) {\n                // Convergence has been reached.\n                setCost(currentCost);\n                return current;\n            }\n\n            // rescale if necessary\n            for (int j = 0; j < nC; ++j) {\n                diag[j] = FastMath.max(diag[j], jacNorm[j]);\n            }\n\n            // Inner loop.\n            for (double ratio = 0; ratio < 1.0e-4;) {\n\n                // save the state\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    oldX[pj] = currentPoint[pj];\n                }\n                final double previousCost = currentCost;\n                double[] tmpVec = weightedResidual;\n                weightedResidual = oldRes;\n                oldRes    = tmpVec;\n                tmpVec    = currentObjective;\n                currentObjective = oldObj;\n                oldObj    = tmpVec;\n\n                // determine the Levenberg-Marquardt parameter\n                determineLMParameter(qtf, delta, diag, work1, work2, work3);\n\n                // compute the new point and the norm of the evolution direction\n                double lmNorm = 0;\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    lmDir[pj] = -lmDir[pj];\n                    currentPoint[pj] = oldX[pj] + lmDir[pj];\n                    double s = diag[pj] * lmDir[pj];\n                    lmNorm  += s * s;\n                }\n                lmNorm = FastMath.sqrt(lmNorm);\n                // on the first iteration, adjust the initial step bound.\n                if (firstIteration) {\n                    delta = FastMath.min(delta, lmNorm);\n                }\n\n                // Evaluate the function at x + p and calculate its norm.\n                currentObjective = computeObjectiveValue(currentPoint);\n                currentResiduals = computeResiduals(currentObjective);\n                current = new PointVectorValuePair(currentPoint, currentObjective);\n                currentCost = computeCost(currentResiduals);\n\n                // compute the scaled actual reduction\n                double actRed = -1.0;\n                if (0.1 * currentCost < previousCost) {\n                    double r = currentCost / previousCost;\n                    actRed = 1.0 - r * r;\n                }\n\n                // compute the scaled predicted reduction\n                // and the scaled directional derivative\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    double dirJ = lmDir[pj];\n                    work1[j] = 0;\n                    for (int i = 0; i <= j; ++i) {\n                        work1[i] += weightedJacobian[i][pj] * dirJ;\n                    }\n                }\n                double coeff1 = 0;\n                for (int j = 0; j < solvedCols; ++j) {\n                    coeff1 += work1[j] * work1[j];\n                }\n                double pc2 = previousCost * previousCost;\n                coeff1 = coeff1 / pc2;\n                double coeff2 = lmPar * lmNorm * lmNorm / pc2;\n                double preRed = coeff1 + 2 * coeff2;\n                double dirDer = -(coeff1 + coeff2);\n\n                // ratio of the actual to the predicted reduction\n                ratio = (preRed == 0) ? 0 : (actRed / preRed);\n\n                // update the step bound\n                if (ratio <= 0.25) {\n                    double tmp =\n                        (actRed < 0) ? (0.5 * dirDer / (dirDer + 0.5 * actRed)) : 0.5;\n                        if ((0.1 * currentCost >= previousCost) || (tmp < 0.1)) {\n                            tmp = 0.1;\n                        }\n                        delta = tmp * FastMath.min(delta, 10.0 * lmNorm);\n                        lmPar /= tmp;\n                } else if ((lmPar == 0) || (ratio >= 0.75)) {\n                    delta = 2 * lmNorm;\n                    lmPar *= 0.5;\n                }\n\n                // test for successful iteration.\n                if (ratio >= 1.0e-4) {\n                    // successful iteration, update the norm\n                    firstIteration = false;\n                    xNorm = 0;\n                    for (int k = 0; k < nC; ++k) {\n                        double xK = diag[k] * currentPoint[k];\n                        xNorm += xK * xK;\n                    }\n                    xNorm = FastMath.sqrt(xNorm);\n\n                    // tests for convergence.\n                    if (checker != null) {\n                        // we use the vectorial convergence checker\n                        if (checker.converged(iter, previous, current)) {\n                            setCost(currentCost);\n                            return current;\n                        }\n                    }\n                } else {\n                    // failed iteration, reset the previous values\n                    currentCost = previousCost;\n                    for (int j = 0; j < solvedCols; ++j) {\n                        int pj = permutation[j];\n                        currentPoint[pj] = oldX[pj];\n                    }\n                    tmpVec    = weightedResidual;\n                    weightedResidual = oldRes;\n                    oldRes    = tmpVec;\n                    tmpVec    = currentObjective;\n                    currentObjective = oldObj;\n                    oldObj    = tmpVec;\n                    // Reset \"current\" to previous values.\n                    current = new PointVectorValuePair(currentPoint, currentObjective);\n                }\n\n                // Default convergence criteria.\n                if ((FastMath.abs(actRed) <= costRelativeTolerance &&\n                     preRed <= costRelativeTolerance &&\n                     ratio <= 2.0) ||\n                    delta <= parRelativeTolerance * xNorm) {\n                    setCost(currentCost);\n                    return current;\n                }\n\n                // tests for termination and stringent tolerances\n                // (2.2204e-16 is the machine epsilon for IEEE754)\n                if ((FastMath.abs(actRed) <= 2.2204e-16) && (preRed <= 2.2204e-16) && (ratio <= 2.0)) {\n                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_COST_RELATIVE_TOLERANCE,\n                                                   costRelativeTolerance);\n                } else if (delta <= 2.2204e-16 * xNorm) {\n                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE,\n                                                   parRelativeTolerance);\n                } else if (maxCosine <= 2.2204e-16)  {\n                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_ORTHOGONALITY_TOLERANCE,\n                                                   orthoTolerance);\n                }\n            }\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointVectorValuePair doOptimize() {\n        checkParameters();\n\n        final int nR = getTarget().length; // Number of observed data.\n        final double[] currentPoint = getStartPoint();\n        final int nC = currentPoint.length; // Number of parameters.\n\n        // arrays shared with the other private methods\n        solvedCols  = FastMath.min(nR, nC);\n        diagR       = new double[nC];\n        jacNorm     = new double[nC];\n        beta        = new double[nC];\n        permutation = new int[nC];\n        lmDir       = new double[nC];\n\n        // local point\n        double   delta   = 0;\n        double   xNorm   = 0;\n        double[] diag    = new double[nC];\n        double[] oldX    = new double[nC];\n        double[] oldRes  = new double[nR];\n        double[] oldObj  = new double[nR];\n        double[] qtf     = new double[nR];\n        double[] work1   = new double[nC];\n        double[] work2   = new double[nC];\n        double[] work3   = new double[nC];\n\n        final RealMatrix weightMatrixSqrt = getWeightSquareRoot();\n\n        // Evaluate the function at the starting point and calculate its norm.\n        double[] currentObjective = computeObjectiveValue(currentPoint);\n        double[] currentResiduals = computeResiduals(currentObjective);\n        PointVectorValuePair current = new PointVectorValuePair(currentPoint.clone(), currentObjective.clone());\n        double currentCost = computeCost(currentResiduals);\n\n        // Outer loop.\n        lmPar = 0;\n        boolean firstIteration = true;\n        int iter = 0;\n        final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n        while (true) {\n            ++iter;\n            final PointVectorValuePair previous = current;\n\n            // QR decomposition of the jacobian matrix\n            qrDecomposition(computeWeightedJacobian(currentPoint));\n\n            weightedResidual = weightMatrixSqrt.operate(currentResiduals);\n            for (int i = 0; i < nR; i++) {\n                qtf[i] = weightedResidual[i];\n            }\n\n            // compute Qt.res\n            qTy(qtf);\n\n            // now we don't need Q anymore,\n            // so let jacobian contain the R matrix with its diagonal elements\n            for (int k = 0; k < solvedCols; ++k) {\n                int pk = permutation[k];\n                weightedJacobian[k][pk] = diagR[pk];\n            }\n\n            if (firstIteration) {\n                // scale the point according to the norms of the columns\n                // of the initial jacobian\n                xNorm = 0;\n                for (int k = 0; k < nC; ++k) {\n                    double dk = jacNorm[k];\n                    if (dk == 0) {\n                        dk = 1.0;\n                    }\n                    double xk = dk * currentPoint[k];\n                    xNorm  += xk * xk;\n                    diag[k] = dk;\n                }\n                xNorm = FastMath.sqrt(xNorm);\n\n                // initialize the step bound delta\n                delta = (xNorm == 0) ? initialStepBoundFactor : (initialStepBoundFactor * xNorm);\n            }\n\n            // check orthogonality between function vector and jacobian columns\n            double maxCosine = 0;\n            if (currentCost != 0) {\n                for (int j = 0; j < solvedCols; ++j) {\n                    int    pj = permutation[j];\n                    double s  = jacNorm[pj];\n                    if (s != 0) {\n                        double sum = 0;\n                        for (int i = 0; i <= j; ++i) {\n                            sum += weightedJacobian[i][pj] * qtf[i];\n                        }\n                        maxCosine = FastMath.max(maxCosine, FastMath.abs(sum) / (s * currentCost));\n                    }\n                }\n            }\n            if (maxCosine <= orthoTolerance) {\n                // Convergence has been reached.\n                setCost(currentCost);\n                return current;\n            }\n\n            // rescale if necessary\n            for (int j = 0; j < nC; ++j) {\n                diag[j] = FastMath.max(diag[j], jacNorm[j]);\n            }\n\n            // Inner loop.\n            for (double ratio = 0; ratio < 1.0e-4;) {\n\n                // save the state\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    oldX[pj] = currentPoint[pj];\n                }\n                final double previousCost = currentCost;\n                double[] tmpVec = weightedResidual;\n                weightedResidual = oldRes;\n                oldRes    = tmpVec;\n                tmpVec    = currentObjective;\n                currentObjective = oldObj;\n                oldObj    = tmpVec;\n\n                // determine the Levenberg-Marquardt parameter\n                determineLMParameter(qtf, delta, diag, work1, work2, work3);\n\n                // compute the new point and the norm of the evolution direction\n                double lmNorm = 0;\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    lmDir[pj] = -lmDir[pj];\n                    currentPoint[pj] = oldX[pj] + lmDir[pj];\n                    double s = diag[pj] * lmDir[pj];\n                    lmNorm  += s * s;\n                }\n                lmNorm = FastMath.sqrt(lmNorm);\n                // on the first iteration, adjust the initial step bound.\n                if (firstIteration) {\n                    delta = FastMath.min(delta, lmNorm);\n                }\n\n                // Evaluate the function at x + p and calculate its norm.\n                currentObjective = computeObjectiveValue(currentPoint);\n                currentResiduals = computeResiduals(currentObjective);\n                current = new PointVectorValuePair(currentPoint.clone(), currentObjective.clone());\n                currentCost = computeCost(currentResiduals);\n\n                // compute the scaled actual reduction\n                double actRed = -1.0;\n                if (0.1 * currentCost < previousCost) {\n                    double r = currentCost / previousCost;\n                    actRed = 1.0 - r * r;\n                }\n\n                // compute the scaled predicted reduction\n                // and the scaled directional derivative\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    double dirJ = lmDir[pj];\n                    work1[j] = 0;\n                    for (int i = 0; i <= j; ++i) {\n                        work1[i] += weightedJacobian[i][pj] * dirJ;\n                    }\n                }\n                double coeff1 = 0;\n                for (int j = 0; j < solvedCols; ++j) {\n                    coeff1 += work1[j] * work1[j];\n                }\n                double pc2 = previousCost * previousCost;\n                coeff1 = coeff1 / pc2;\n                double coeff2 = lmPar * lmNorm * lmNorm / pc2;\n                double preRed = coeff1 + 2 * coeff2;\n                double dirDer = -(coeff1 + coeff2);\n\n                // ratio of the actual to the predicted reduction\n                ratio = (preRed == 0) ? 0 : (actRed / preRed);\n\n                // update the step bound\n                if (ratio <= 0.25) {\n                    double tmp =\n                        (actRed < 0) ? (0.5 * dirDer / (dirDer + 0.5 * actRed)) : 0.5;\n                        if ((0.1 * currentCost >= previousCost) || (tmp < 0.1)) {\n                            tmp = 0.1;\n                        }\n                        delta = tmp * FastMath.min(delta, 10.0 * lmNorm);\n                        lmPar /= tmp;\n                } else if ((lmPar == 0) || (ratio >= 0.75)) {\n                    delta = 2 * lmNorm;\n                    lmPar *= 0.5;\n                }\n\n                // test for successful iteration.\n                if (ratio >= 1.0e-4) {\n                    // successful iteration, update the norm\n                    firstIteration = false;\n                    xNorm = 0;\n                    for (int k = 0; k < nC; ++k) {\n                        double xK = diag[k] * currentPoint[k];\n                        xNorm += xK * xK;\n                    }\n                    xNorm = FastMath.sqrt(xNorm);\n\n                    // tests for convergence.\n                    if (checker != null) {\n                        // we use the vectorial convergence checker\n                        if (checker.converged(iter, previous, current)) {\n                            setCost(currentCost);\n                            return current;\n                        }\n                    }\n                } else {\n                    // failed iteration, reset the previous values\n                    currentCost = previousCost;\n                    for (int j = 0; j < solvedCols; ++j) {\n                        int pj = permutation[j];\n                        currentPoint[pj] = oldX[pj];\n                    }\n                    tmpVec    = weightedResidual;\n                    weightedResidual = oldRes;\n                    oldRes    = tmpVec;\n                    tmpVec    = currentObjective;\n                    currentObjective = oldObj;\n                    oldObj    = tmpVec;\n                    // Reset \"current\" to previous values.\n                    current = new PointVectorValuePair(currentPoint.clone(), currentObjective.clone());\n                }\n\n                // Default convergence criteria.\n                if ((FastMath.abs(actRed) <= costRelativeTolerance &&\n                     preRed <= costRelativeTolerance &&\n                     ratio <= 2.0) ||\n                    delta <= parRelativeTolerance * xNorm) {\n                    setCost(currentCost);\n                    return current;\n                }\n\n                // tests for termination and stringent tolerances\n                // (2.2204e-16 is the machine epsilon for IEEE754)\n                if ((FastMath.abs(actRed) <= 2.2204e-16) && (preRed <= 2.2204e-16) && (ratio <= 2.0)) {\n                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_COST_RELATIVE_TOLERANCE,\n                                                   costRelativeTolerance);\n                } else if (delta <= 2.2204e-16 * xNorm) {\n                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE,\n                                                   parRelativeTolerance);\n                } else if (maxCosine <= 2.2204e-16)  {\n                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_ORTHOGONALITY_TOLERANCE,\n                                                   orthoTolerance);\n                }\n            }\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointVectorValuePair doOptimize() {\n        checkParameters();\n\n        final int nR = getTarget().length; // Number of observed data.\n        final double[] currentPoint = getStartPoint();\n        final int nC = currentPoint.length; // Number of parameters.\n\n        // arrays shared with the other private methods\n        solvedCols  = FastMath.min(nR, nC);\n        diagR       = new double[nC];\n        jacNorm     = new double[nC];\n        beta        = new double[nC];\n        permutation = new int[nC];\n        lmDir       = new double[nC];\n\n        // local point\n        double   delta   = 0;\n        double   xNorm   = 0;\n        double[] diag    = new double[nC];\n        double[] oldX    = new double[nC];\n        double[] oldRes  = new double[nR];\n        double[] oldObj  = new double[nR];\n        double[] qtf     = new double[nR];\n        double[] work1   = new double[nC];\n        double[] work2   = new double[nC];\n        double[] work3   = new double[nC];\n\n        final RealMatrix weightMatrixSqrt = getWeightSquareRoot();\n\n        // Evaluate the function at the starting point and calculate its norm.\n        double[] currentObjective = computeObjectiveValue(currentPoint);\n        double[] currentResiduals = computeResiduals(currentObjective);\n        PointVectorValuePair current = new PointVectorValuePair(currentPoint, currentObjective);\n        double currentCost = computeCost(currentResiduals);\n\n        // Outer loop.\n        lmPar = 0;\n        boolean firstIteration = true;\n        int iter = 0;\n        final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n        while (true) {\n            ++iter;\n            final PointVectorValuePair previous = current;\n\n            // QR decomposition of the jacobian matrix\n            qrDecomposition(computeWeightedJacobian(currentPoint));\n\n            weightedResidual = weightMatrixSqrt.operate(currentResiduals);\n            System.arraycopy(weightedResidual, 0, qtf, 0, nR);\n\n            // compute Qt.res\n            qTy(qtf);\n\n            // now we don't need Q anymore,\n            // so let jacobian contain the R matrix with its diagonal elements\n            for (int k = 0; k < solvedCols; ++k) {\n                int pk = permutation[k];\n                weightedJacobian[k][pk] = diagR[pk];\n            }\n\n            if (firstIteration) {\n                // scale the point according to the norms of the columns\n                // of the initial jacobian\n                xNorm = 0;\n                for (int k = 0; k < nC; ++k) {\n                    double dk = jacNorm[k];\n                    if (dk == 0) {\n                        dk = 1.0;\n                    }\n                    double xk = dk * currentPoint[k];\n                    xNorm  += xk * xk;\n                    diag[k] = dk;\n                }\n                xNorm = FastMath.sqrt(xNorm);\n\n                // initialize the step bound delta\n                delta = (xNorm == 0) ? initialStepBoundFactor : (initialStepBoundFactor * xNorm);\n            }\n\n            // check orthogonality between function vector and jacobian columns\n            double maxCosine = 0;\n            if (currentCost != 0) {\n                for (int j = 0; j < solvedCols; ++j) {\n                    int    pj = permutation[j];\n                    double s  = jacNorm[pj];\n                    if (s != 0) {\n                        double sum = 0;\n                        for (int i = 0; i <= j; ++i) {\n                            sum += weightedJacobian[i][pj] * qtf[i];\n                        }\n                        maxCosine = FastMath.max(maxCosine, FastMath.abs(sum) / (s * currentCost));\n                    }\n                }\n            }\n            if (maxCosine <= orthoTolerance) {\n                // Convergence has been reached.\n                setCost(currentCost);\n                return current;\n            }\n\n            // rescale if necessary\n            for (int j = 0; j < nC; ++j) {\n                diag[j] = FastMath.max(diag[j], jacNorm[j]);\n            }\n\n            // Inner loop.\n            for (double ratio = 0; ratio < 1.0e-4;) {\n\n                // save the state\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    oldX[pj] = currentPoint[pj];\n                }\n                final double previousCost = currentCost;\n                double[] tmpVec = weightedResidual;\n                weightedResidual = oldRes;\n                oldRes    = tmpVec;\n                tmpVec    = currentObjective;\n                currentObjective = oldObj;\n                oldObj    = tmpVec;\n\n                // determine the Levenberg-Marquardt parameter\n                determineLMParameter(qtf, delta, diag, work1, work2, work3);\n\n                // compute the new point and the norm of the evolution direction\n                double lmNorm = 0;\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    lmDir[pj] = -lmDir[pj];\n                    currentPoint[pj] = oldX[pj] + lmDir[pj];\n                    double s = diag[pj] * lmDir[pj];\n                    lmNorm  += s * s;\n                }\n                lmNorm = FastMath.sqrt(lmNorm);\n                // on the first iteration, adjust the initial step bound.\n                if (firstIteration) {\n                    delta = FastMath.min(delta, lmNorm);\n                }\n\n                // Evaluate the function at x + p and calculate its norm.\n                currentObjective = computeObjectiveValue(currentPoint);\n                currentResiduals = computeResiduals(currentObjective);\n                current = new PointVectorValuePair(currentPoint, currentObjective);\n                currentCost = computeCost(currentResiduals);\n\n                // compute the scaled actual reduction\n                double actRed = -1.0;\n                if (0.1 * currentCost < previousCost) {\n                    double r = currentCost / previousCost;\n                    actRed = 1.0 - r * r;\n                }\n\n                // compute the scaled predicted reduction\n                // and the scaled directional derivative\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    double dirJ = lmDir[pj];\n                    work1[j] = 0;\n                    for (int i = 0; i <= j; ++i) {\n                        work1[i] += weightedJacobian[i][pj] * dirJ;\n                    }\n                }\n                double coeff1 = 0;\n                for (int j = 0; j < solvedCols; ++j) {\n                    coeff1 += work1[j] * work1[j];\n                }\n                double pc2 = previousCost * previousCost;\n                coeff1 = coeff1 / pc2;\n                double coeff2 = lmPar * lmNorm * lmNorm / pc2;\n                double preRed = coeff1 + 2 * coeff2;\n                double dirDer = -(coeff1 + coeff2);\n\n                // ratio of the actual to the predicted reduction\n                ratio = (preRed == 0) ? 0 : (actRed / preRed);\n\n                // update the step bound\n                if (ratio <= 0.25) {\n                    double tmp =\n                        (actRed < 0) ? (0.5 * dirDer / (dirDer + 0.5 * actRed)) : 0.5;\n                    if ((0.1 * currentCost >= previousCost) || (tmp < 0.1)) {\n                        tmp = 0.1;\n                    }\n                    delta = tmp * FastMath.min(delta, 10.0 * lmNorm);\n                    lmPar /= tmp;\n                } else if ((lmPar == 0) || (ratio >= 0.75)) {\n                    delta = 2 * lmNorm;\n                    lmPar *= 0.5;\n                }\n\n                // test for successful iteration.\n                if (ratio >= 1.0e-4) {\n                    // successful iteration, update the norm\n                    firstIteration = false;\n                    xNorm = 0;\n                    for (int k = 0; k < nC; ++k) {\n                        double xK = diag[k] * currentPoint[k];\n                        xNorm += xK * xK;\n                    }\n                    xNorm = FastMath.sqrt(xNorm);\n\n                    // tests for convergence.\n                    if (checker != null) {\n                        // we use the vectorial convergence checker\n                        if (checker.converged(iter, previous, current)) {\n                            setCost(currentCost);\n                            return current;\n                        }\n                    }\n                } else {\n                    // failed iteration, reset the previous values\n                    currentCost = previousCost;\n                    for (int j = 0; j < solvedCols; ++j) {\n                        int pj = permutation[j];\n                        currentPoint[pj] = oldX[pj];\n                    }\n                    tmpVec    = weightedResidual;\n                    weightedResidual = oldRes;\n                    oldRes    = tmpVec;\n                    tmpVec    = currentObjective;\n                    currentObjective = oldObj;\n                    oldObj    = tmpVec;\n                    // Reset \"current\" to previous values.\n                    current = new PointVectorValuePair(currentPoint, currentObjective);\n                }\n\n                // Default convergence criteria.\n                if ((FastMath.abs(actRed) <= costRelativeTolerance &&\n                     preRed <= costRelativeTolerance &&\n                     ratio <= 2.0) ||\n                    delta <= parRelativeTolerance * xNorm) {\n                    setCost(currentCost);\n                    return current;\n                }\n\n                // tests for termination and stringent tolerances\n                // (2.2204e-16 is the machine epsilon for IEEE754)\n                if ((FastMath.abs(actRed) <= 2.2204e-16) && (preRed <= 2.2204e-16) && (ratio <= 2.0)) {\n                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_COST_RELATIVE_TOLERANCE,\n                                                   costRelativeTolerance);\n                } else if (delta <= 2.2204e-16 * xNorm) {\n                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE,\n                                                   parRelativeTolerance);\n                } else if (maxCosine <= 2.2204e-16)  {\n                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_ORTHOGONALITY_TOLERANCE,\n                                                   orthoTolerance);\n                }\n            }\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointVectorValuePair doOptimize() {\n        checkParameters();\n\n        final int nR = getTarget().length; // Number of observed data.\n        final double[] currentPoint = getStartPoint();\n        final int nC = currentPoint.length; // Number of parameters.\n\n        // arrays shared with the other private methods\n        solvedCols  = FastMath.min(nR, nC);\n        diagR       = new double[nC];\n        jacNorm     = new double[nC];\n        beta        = new double[nC];\n        permutation = new int[nC];\n        lmDir       = new double[nC];\n\n        // local point\n        double   delta   = 0;\n        double   xNorm   = 0;\n        double[] diag    = new double[nC];\n        double[] oldX    = new double[nC];\n        double[] oldRes  = new double[nR];\n        double[] oldObj  = new double[nR];\n        double[] qtf     = new double[nR];\n        double[] work1   = new double[nC];\n        double[] work2   = new double[nC];\n        double[] work3   = new double[nC];\n\n        final RealMatrix weightMatrixSqrt = getWeightSquareRoot();\n\n        // Evaluate the function at the starting point and calculate its norm.\n        double[] currentObjective = computeObjectiveValue(currentPoint);\n        double[] currentResiduals = computeResiduals(currentObjective);\n        PointVectorValuePair current = new PointVectorValuePair(currentPoint, currentObjective);\n        double currentCost = computeCost(currentResiduals);\n\n        // Outer loop.\n        lmPar = 0;\n        boolean firstIteration = true;\n        int iter = 0;\n        final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n        while (true) {\n            ++iter;\n            final PointVectorValuePair previous = current;\n\n            // QR decomposition of the jacobian matrix\n            qrDecomposition(computeWeightedJacobian(currentPoint));\n\n            weightedResidual = weightMatrixSqrt.operate(currentResiduals);\n            for (int i = 0; i < nR; i++) {\n                qtf[i] = weightedResidual[i];\n            }\n\n            // compute Qt.res\n            qTy(qtf);\n\n            // now we don't need Q anymore,\n            // so let jacobian contain the R matrix with its diagonal elements\n            for (int k = 0; k < solvedCols; ++k) {\n                int pk = permutation[k];\n                weightedJacobian[k][pk] = diagR[pk];\n            }\n\n            if (firstIteration) {\n                // scale the point according to the norms of the columns\n                // of the initial jacobian\n                xNorm = 0;\n                for (int k = 0; k < nC; ++k) {\n                    double dk = jacNorm[k];\n                    if (dk == 0) {\n                        dk = 1.0;\n                    }\n                    double xk = dk * currentPoint[k];\n                    xNorm  += xk * xk;\n                    diag[k] = dk;\n                }\n                xNorm = FastMath.sqrt(xNorm);\n\n                // initialize the step bound delta\n                delta = (xNorm == 0) ? initialStepBoundFactor : (initialStepBoundFactor * xNorm);\n            }\n\n            // check orthogonality between function vector and jacobian columns\n            double maxCosine = 0;\n            if (currentCost != 0) {\n                for (int j = 0; j < solvedCols; ++j) {\n                    int    pj = permutation[j];\n                    double s  = jacNorm[pj];\n                    if (s != 0) {\n                        double sum = 0;\n                        for (int i = 0; i <= j; ++i) {\n                            sum += weightedJacobian[i][pj] * qtf[i];\n                        }\n                        maxCosine = FastMath.max(maxCosine, FastMath.abs(sum) / (s * currentCost));\n                    }\n                }\n            }\n            if (maxCosine <= orthoTolerance) {\n                // Convergence has been reached.\n                setCost(currentCost);\n                return current;\n            }\n\n            // rescale if necessary\n            for (int j = 0; j < nC; ++j) {\n                diag[j] = FastMath.max(diag[j], jacNorm[j]);\n            }\n\n            // Inner loop.\n            for (double ratio = 0; ratio < 1.0e-4;) {\n\n                // save the state\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    oldX[pj] = currentPoint[pj];\n                }\n                final double previousCost = currentCost;\n                double[] tmpVec = weightedResidual;\n                weightedResidual = oldRes;\n                oldRes    = tmpVec;\n                tmpVec    = currentObjective;\n                currentObjective = oldObj;\n                oldObj    = tmpVec;\n\n                // determine the Levenberg-Marquardt parameter\n                determineLMParameter(qtf, delta, diag, work1, work2, work3);\n\n                // compute the new point and the norm of the evolution direction\n                double lmNorm = 0;\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    lmDir[pj] = -lmDir[pj];\n                    currentPoint[pj] = oldX[pj] + lmDir[pj];\n                    double s = diag[pj] * lmDir[pj];\n                    lmNorm  += s * s;\n                }\n                lmNorm = FastMath.sqrt(lmNorm);\n                // on the first iteration, adjust the initial step bound.\n                if (firstIteration) {\n                    delta = FastMath.min(delta, lmNorm);\n                }\n\n                // Evaluate the function at x + p and calculate its norm.\n                currentObjective = computeObjectiveValue(currentPoint);\n                currentResiduals = computeResiduals(currentObjective);\n                current = new PointVectorValuePair(currentPoint, currentObjective);\n                currentCost = computeCost(currentResiduals);\n\n                // compute the scaled actual reduction\n                double actRed = -1.0;\n                if (0.1 * currentCost < previousCost) {\n                    double r = currentCost / previousCost;\n                    actRed = 1.0 - r * r;\n                }\n\n                // compute the scaled predicted reduction\n                // and the scaled directional derivative\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    double dirJ = lmDir[pj];\n                    work1[j] = 0;\n                    for (int i = 0; i <= j; ++i) {\n                        work1[i] += weightedJacobian[i][pj] * dirJ;\n                    }\n                }\n                double coeff1 = 0;\n                for (int j = 0; j < solvedCols; ++j) {\n                    coeff1 += work1[j] * work1[j];\n                }\n                double pc2 = previousCost * previousCost;\n                coeff1 = coeff1 / pc2;\n                double coeff2 = lmPar * lmNorm * lmNorm / pc2;\n                double preRed = coeff1 + 2 * coeff2;\n                double dirDer = -(coeff1 + coeff2);\n\n                // ratio of the actual to the predicted reduction\n                ratio = (preRed == 0) ? 0 : (actRed / preRed);\n\n                // update the step bound\n                if (ratio <= 0.25) {\n                    double tmp =\n                        (actRed < 0) ? (0.5 * dirDer / (dirDer + 0.5 * actRed)) : 0.5;\n                        if ((0.1 * currentCost >= previousCost) || (tmp < 0.1)) {\n                            tmp = 0.1;\n                        }\n                        delta = tmp * FastMath.min(delta, 10.0 * lmNorm);\n                        lmPar /= tmp;\n                } else if ((lmPar == 0) || (ratio >= 0.75)) {\n                    delta = 2 * lmNorm;\n                    lmPar *= 0.5;\n                }\n\n                // test for successful iteration.\n                if (ratio >= 1.0e-4) {\n                    // successful iteration, update the norm\n                    firstIteration = false;\n                    xNorm = 0;\n                    for (int k = 0; k < nC; ++k) {\n                        double xK = diag[k] * currentPoint[k];\n                        xNorm += xK * xK;\n                    }\n                    xNorm = FastMath.sqrt(xNorm);\n\n                    // tests for convergence.\n                    if (checker != null) {\n                        // we use the vectorial convergence checker\n                        if (checker.converged(iter, previous, current)) {\n                            setCost(currentCost);\n                            return current;\n                        }\n                    }\n                } else {\n                    // failed iteration, reset the previous values\n                    currentCost = previousCost;\n                    for (int j = 0; j < solvedCols; ++j) {\n                        int pj = permutation[j];\n                        currentPoint[pj] = oldX[pj];\n                    }\n                    tmpVec    = weightedResidual;\n                    weightedResidual = oldRes;\n                    oldRes    = tmpVec;\n                    tmpVec    = currentObjective;\n                    currentObjective = oldObj;\n                    oldObj    = tmpVec;\n                    // Reset \"current\" to previous values.\n                    current = new PointVectorValuePair(currentPoint, currentObjective);\n                }\n\n                // Default convergence criteria.\n                if ((FastMath.abs(actRed) <= costRelativeTolerance &&\n                     preRed <= costRelativeTolerance &&\n                     ratio <= 2.0) ||\n                    delta <= parRelativeTolerance * xNorm) {\n                    setCost(currentCost);\n                    return current;\n                }\n\n                // tests for termination and stringent tolerances\n                // (2.2204e-16 is the machine epsilon for IEEE754)\n                if ((FastMath.abs(actRed) <= 2.2204e-16) && (preRed <= 2.2204e-16) && (ratio <= 2.0)) {\n                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_COST_RELATIVE_TOLERANCE,\n                                                   costRelativeTolerance);\n                } else if (delta <= 2.2204e-16 * xNorm) {\n                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE,\n                                                   parRelativeTolerance);\n                } else if (maxCosine <= 2.2204e-16)  {\n                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_ORTHOGONALITY_TOLERANCE,\n                                                   orthoTolerance);\n                }\n            }\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointVectorValuePair doOptimize() {\n        checkParameters();\n\n        final int nR = getTarget().length; // Number of observed data.\n        final double[] currentPoint = getStartPoint();\n        final int nC = currentPoint.length; // Number of parameters.\n\n        // arrays shared with the other private methods\n        solvedCols  = FastMath.min(nR, nC);\n        diagR       = new double[nC];\n        jacNorm     = new double[nC];\n        beta        = new double[nC];\n        permutation = new int[nC];\n        lmDir       = new double[nC];\n\n        // local point\n        double   delta   = 0;\n        double   xNorm   = 0;\n        double[] diag    = new double[nC];\n        double[] oldX    = new double[nC];\n        double[] oldRes  = new double[nR];\n        double[] oldObj  = new double[nR];\n        double[] qtf     = new double[nR];\n        double[] work1   = new double[nC];\n        double[] work2   = new double[nC];\n        double[] work3   = new double[nC];\n\n        final RealMatrix weightMatrixSqrt = getWeightSquareRoot();\n\n        // Evaluate the function at the starting point and calculate its norm.\n        double[] currentObjective = computeObjectiveValue(currentPoint);\n        double[] currentResiduals = computeResiduals(currentObjective);\n        PointVectorValuePair current = new PointVectorValuePair(currentPoint, currentObjective);\n        double currentCost = computeCost(currentResiduals);\n\n        // Outer loop.\n        lmPar = 0;\n        boolean firstIteration = true;\n        int iter = 0;\n        final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n        while (true) {\n            ++iter;\n            final PointVectorValuePair previous = current;\n\n            // QR decomposition of the jacobian matrix\n            qrDecomposition(computeWeightedJacobian(currentPoint));\n\n            weightedResidual = weightMatrixSqrt.operate(currentResiduals);\n            for (int i = 0; i < nR; i++) {\n                qtf[i] = weightedResidual[i];\n            }\n\n            // compute Qt.res\n            qTy(qtf);\n\n            // now we don't need Q anymore,\n            // so let jacobian contain the R matrix with its diagonal elements\n            for (int k = 0; k < solvedCols; ++k) {\n                int pk = permutation[k];\n                weightedJacobian[k][pk] = diagR[pk];\n            }\n\n            if (firstIteration) {\n                // scale the point according to the norms of the columns\n                // of the initial jacobian\n                xNorm = 0;\n                for (int k = 0; k < nC; ++k) {\n                    double dk = jacNorm[k];\n                    if (dk == 0) {\n                        dk = 1.0;\n                    }\n                    double xk = dk * currentPoint[k];\n                    xNorm  += xk * xk;\n                    diag[k] = dk;\n                }\n                xNorm = FastMath.sqrt(xNorm);\n\n                // initialize the step bound delta\n                delta = (xNorm == 0) ? initialStepBoundFactor : (initialStepBoundFactor * xNorm);\n            }\n\n            // check orthogonality between function vector and jacobian columns\n            double maxCosine = 0;\n            if (currentCost != 0) {\n                for (int j = 0; j < solvedCols; ++j) {\n                    int    pj = permutation[j];\n                    double s  = jacNorm[pj];\n                    if (s != 0) {\n                        double sum = 0;\n                        for (int i = 0; i <= j; ++i) {\n                            sum += weightedJacobian[i][pj] * qtf[i];\n                        }\n                        maxCosine = FastMath.max(maxCosine, FastMath.abs(sum) / (s * currentCost));\n                    }\n                }\n            }\n            if (maxCosine <= orthoTolerance) {\n                // Convergence has been reached.\n                setCost(currentCost);\n                return current;\n            }\n\n            // rescale if necessary\n            for (int j = 0; j < nC; ++j) {\n                diag[j] = FastMath.max(diag[j], jacNorm[j]);\n            }\n\n            // Inner loop.\n            for (double ratio = 0; ratio < 1.0e-4;) {\n\n                // save the state\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    oldX[pj] = currentPoint[pj];\n                }\n                final double previousCost = currentCost;\n                double[] tmpVec = weightedResidual;\n                weightedResidual = oldRes;\n                oldRes    = tmpVec;\n                tmpVec    = currentObjective;\n                currentObjective = oldObj;\n                oldObj    = tmpVec;\n\n                // determine the Levenberg-Marquardt parameter\n                determineLMParameter(qtf, delta, diag, work1, work2, work3);\n\n                // compute the new point and the norm of the evolution direction\n                double lmNorm = 0;\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    lmDir[pj] = -lmDir[pj];\n                    currentPoint[pj] = oldX[pj] + lmDir[pj];\n                    double s = diag[pj] * lmDir[pj];\n                    lmNorm  += s * s;\n                }\n                lmNorm = FastMath.sqrt(lmNorm);\n                // on the first iteration, adjust the initial step bound.\n                if (firstIteration) {\n                    delta = FastMath.min(delta, lmNorm);\n                }\n\n                // Evaluate the function at x + p and calculate its norm.\n                currentObjective = computeObjectiveValue(currentPoint);\n                currentResiduals = computeResiduals(currentObjective);\n                current = new PointVectorValuePair(currentPoint, currentObjective);\n                currentCost = computeCost(currentResiduals);\n\n                // compute the scaled actual reduction\n                double actRed = -1.0;\n                if (0.1 * currentCost < previousCost) {\n                    double r = currentCost / previousCost;\n                    actRed = 1.0 - r * r;\n                }\n\n                // compute the scaled predicted reduction\n                // and the scaled directional derivative\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    double dirJ = lmDir[pj];\n                    work1[j] = 0;\n                    for (int i = 0; i <= j; ++i) {\n                        work1[i] += weightedJacobian[i][pj] * dirJ;\n                    }\n                }\n                double coeff1 = 0;\n                for (int j = 0; j < solvedCols; ++j) {\n                    coeff1 += work1[j] * work1[j];\n                }\n                double pc2 = previousCost * previousCost;\n                coeff1 = coeff1 / pc2;\n                double coeff2 = lmPar * lmNorm * lmNorm / pc2;\n                double preRed = coeff1 + 2 * coeff2;\n                double dirDer = -(coeff1 + coeff2);\n\n                // ratio of the actual to the predicted reduction\n                ratio = (preRed == 0) ? 0 : (actRed / preRed);\n\n                // update the step bound\n                if (ratio <= 0.25) {\n                    double tmp =\n                        (actRed < 0) ? (0.5 * dirDer / (dirDer + 0.5 * actRed)) : 0.5;\n                        if ((0.1 * currentCost >= previousCost) || (tmp < 0.1)) {\n                            tmp = 0.1;\n                        }\n                        delta = tmp * FastMath.min(delta, 10.0 * lmNorm);\n                        lmPar /= tmp;\n                } else if ((lmPar == 0) || (ratio >= 0.75)) {\n                    delta = 2 * lmNorm;\n                    lmPar *= 0.5;\n                }\n\n                // test for successful iteration.\n                if (ratio >= 1.0e-4) {\n                    // successful iteration, update the norm\n                    firstIteration = false;\n                    xNorm = 0;\n                    for (int k = 0; k < nC; ++k) {\n                        double xK = diag[k] * currentPoint[k];\n                        xNorm += xK * xK;\n                    }\n                    xNorm = FastMath.sqrt(xNorm);\n\n                    // tests for convergence.\n                    if (checker != null) {\n                        // we use the vectorial convergence checker\n                        if (checker.converged(iter, previous, current)) {\n                            setCost(currentCost);\n                            return current;\n                        }\n                    }\n                } else {\n                    // failed iteration, reset the previous values\n                    currentCost = previousCost;\n                    for (int j = 0; j < solvedCols; ++j) {\n                        int pj = permutation[j];\n                        currentPoint[pj] = oldX[pj];\n                    }\n                    tmpVec    = weightedResidual;\n                    weightedResidual = oldRes;\n                    oldRes    = tmpVec;\n                    tmpVec    = currentObjective;\n                    currentObjective = oldObj;\n                    oldObj    = tmpVec;\n                    // Reset \"current\" to previous values.\n                    current = new PointVectorValuePair(currentPoint, currentObjective);\n                }\n\n                // Default convergence criteria.\n                if ((FastMath.abs(actRed) <= costRelativeTolerance &&\n                     preRed <= costRelativeTolerance &&\n                     ratio <= 2.0) ||\n                    delta <= parRelativeTolerance * xNorm) {\n                    setCost(currentCost);\n                    return current;\n                }\n\n                // tests for termination and stringent tolerances\n                // (2.2204e-16 is the machine epsilon for IEEE754)\n                if ((FastMath.abs(actRed) <= 2.2204e-16) && (preRed <= 2.2204e-16) && (ratio <= 2.0)) {\n                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_COST_RELATIVE_TOLERANCE,\n                                                   costRelativeTolerance);\n                } else if (delta <= 2.2204e-16 * xNorm) {\n                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE,\n                                                   parRelativeTolerance);\n                } else if (maxCosine <= 2.2204e-16)  {\n                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_ORTHOGONALITY_TOLERANCE,\n                                                   orthoTolerance);\n                }\n            }\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointVectorValuePair doOptimize() {\n        checkParameters();\n\n        final int nR = getTarget().length; // Number of observed data.\n        final double[] currentPoint = getStartPoint();\n        final int nC = currentPoint.length; // Number of parameters.\n\n        // arrays shared with the other private methods\n        solvedCols  = FastMath.min(nR, nC);\n        diagR       = new double[nC];\n        jacNorm     = new double[nC];\n        beta        = new double[nC];\n        permutation = new int[nC];\n        lmDir       = new double[nC];\n\n        // local point\n        double   delta   = 0;\n        double   xNorm   = 0;\n        double[] diag    = new double[nC];\n        double[] oldX    = new double[nC];\n        double[] oldRes  = new double[nR];\n        double[] oldObj  = new double[nR];\n        double[] qtf     = new double[nR];\n        double[] work1   = new double[nC];\n        double[] work2   = new double[nC];\n        double[] work3   = new double[nC];\n\n        final RealMatrix weightMatrixSqrt = getWeightSquareRoot();\n\n        // Evaluate the function at the starting point and calculate its norm.\n        double[] currentObjective = computeObjectiveValue(currentPoint);\n        double[] currentResiduals = computeResiduals(currentObjective);\n        PointVectorValuePair current = new PointVectorValuePair(currentPoint.clone(), currentObjective.clone());\n        double currentCost = computeCost(currentResiduals);\n\n        // Outer loop.\n        lmPar = 0;\n        boolean firstIteration = true;\n        int iter = 0;\n        final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n        while (true) {\n            ++iter;\n            final PointVectorValuePair previous = current;\n\n            // QR decomposition of the jacobian matrix\n            qrDecomposition(computeWeightedJacobian(currentPoint));\n\n            weightedResidual = weightMatrixSqrt.operate(currentResiduals);\n            for (int i = 0; i < nR; i++) {\n                qtf[i] = weightedResidual[i];\n            }\n\n            // compute Qt.res\n            qTy(qtf);\n\n            // now we don't need Q anymore,\n            // so let jacobian contain the R matrix with its diagonal elements\n            for (int k = 0; k < solvedCols; ++k) {\n                int pk = permutation[k];\n                weightedJacobian[k][pk] = diagR[pk];\n            }\n\n            if (firstIteration) {\n                // scale the point according to the norms of the columns\n                // of the initial jacobian\n                xNorm = 0;\n                for (int k = 0; k < nC; ++k) {\n                    double dk = jacNorm[k];\n                    if (dk == 0) {\n                        dk = 1.0;\n                    }\n                    double xk = dk * currentPoint[k];\n                    xNorm  += xk * xk;\n                    diag[k] = dk;\n                }\n                xNorm = FastMath.sqrt(xNorm);\n\n                // initialize the step bound delta\n                delta = (xNorm == 0) ? initialStepBoundFactor : (initialStepBoundFactor * xNorm);\n            }\n\n            // check orthogonality between function vector and jacobian columns\n            double maxCosine = 0;\n            if (currentCost != 0) {\n                for (int j = 0; j < solvedCols; ++j) {\n                    int    pj = permutation[j];\n                    double s  = jacNorm[pj];\n                    if (s != 0) {\n                        double sum = 0;\n                        for (int i = 0; i <= j; ++i) {\n                            sum += weightedJacobian[i][pj] * qtf[i];\n                        }\n                        maxCosine = FastMath.max(maxCosine, FastMath.abs(sum) / (s * currentCost));\n                    }\n                }\n            }\n            if (maxCosine <= orthoTolerance) {\n                // Convergence has been reached.\n                setCost(currentCost);\n                return current;\n            }\n\n            // rescale if necessary\n            for (int j = 0; j < nC; ++j) {\n                diag[j] = FastMath.max(diag[j], jacNorm[j]);\n            }\n\n            // Inner loop.\n            for (double ratio = 0; ratio < 1.0e-4;) {\n\n                // save the state\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    oldX[pj] = currentPoint[pj];\n                }\n                final double previousCost = currentCost;\n                double[] tmpVec = weightedResidual;\n                weightedResidual = oldRes;\n                oldRes    = tmpVec;\n                tmpVec    = currentObjective;\n                currentObjective = oldObj;\n                oldObj    = tmpVec;\n\n                // determine the Levenberg-Marquardt parameter\n                determineLMParameter(qtf, delta, diag, work1, work2, work3);\n\n                // compute the new point and the norm of the evolution direction\n                double lmNorm = 0;\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    lmDir[pj] = -lmDir[pj];\n                    currentPoint[pj] = oldX[pj] + lmDir[pj];\n                    double s = diag[pj] * lmDir[pj];\n                    lmNorm  += s * s;\n                }\n                lmNorm = FastMath.sqrt(lmNorm);\n                // on the first iteration, adjust the initial step bound.\n                if (firstIteration) {\n                    delta = FastMath.min(delta, lmNorm);\n                }\n\n                // Evaluate the function at x + p and calculate its norm.\n                currentObjective = computeObjectiveValue(currentPoint);\n                currentResiduals = computeResiduals(currentObjective);\n                current = new PointVectorValuePair(currentPoint.clone(), currentObjective.clone());\n                currentCost = computeCost(currentResiduals);\n\n                // compute the scaled actual reduction\n                double actRed = -1.0;\n                if (0.1 * currentCost < previousCost) {\n                    double r = currentCost / previousCost;\n                    actRed = 1.0 - r * r;\n                }\n\n                // compute the scaled predicted reduction\n                // and the scaled directional derivative\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    double dirJ = lmDir[pj];\n                    work1[j] = 0;\n                    for (int i = 0; i <= j; ++i) {\n                        work1[i] += weightedJacobian[i][pj] * dirJ;\n                    }\n                }\n                double coeff1 = 0;\n                for (int j = 0; j < solvedCols; ++j) {\n                    coeff1 += work1[j] * work1[j];\n                }\n                double pc2 = previousCost * previousCost;\n                coeff1 = coeff1 / pc2;\n                double coeff2 = lmPar * lmNorm * lmNorm / pc2;\n                double preRed = coeff1 + 2 * coeff2;\n                double dirDer = -(coeff1 + coeff2);\n\n                // ratio of the actual to the predicted reduction\n                ratio = (preRed == 0) ? 0 : (actRed / preRed);\n\n                // update the step bound\n                if (ratio <= 0.25) {\n                    double tmp =\n                        (actRed < 0) ? (0.5 * dirDer / (dirDer + 0.5 * actRed)) : 0.5;\n                        if ((0.1 * currentCost >= previousCost) || (tmp < 0.1)) {\n                            tmp = 0.1;\n                        }\n                        delta = tmp * FastMath.min(delta, 10.0 * lmNorm);\n                        lmPar /= tmp;\n                } else if ((lmPar == 0) || (ratio >= 0.75)) {\n                    delta = 2 * lmNorm;\n                    lmPar *= 0.5;\n                }\n\n                // test for successful iteration.\n                if (ratio >= 1.0e-4) {\n                    // successful iteration, update the norm\n                    firstIteration = false;\n                    xNorm = 0;\n                    for (int k = 0; k < nC; ++k) {\n                        double xK = diag[k] * currentPoint[k];\n                        xNorm += xK * xK;\n                    }\n                    xNorm = FastMath.sqrt(xNorm);\n\n                    // tests for convergence.\n                    if (checker != null) {\n                        // we use the vectorial convergence checker\n                        if (checker.converged(iter, previous, current)) {\n                            setCost(currentCost);\n                            return current;\n                        }\n                    }\n                } else {\n                    // failed iteration, reset the previous values\n                    currentCost = previousCost;\n                    for (int j = 0; j < solvedCols; ++j) {\n                        int pj = permutation[j];\n                        currentPoint[pj] = oldX[pj];\n                    }\n                    tmpVec    = weightedResidual;\n                    weightedResidual = oldRes;\n                    oldRes    = tmpVec;\n                    tmpVec    = currentObjective;\n                    currentObjective = oldObj;\n                    oldObj    = tmpVec;\n                    // Reset \"current\" to previous values.\n                    current = new PointVectorValuePair(currentPoint.clone(), currentObjective.clone());\n                }\n\n                // Default convergence criteria.\n                if ((FastMath.abs(actRed) <= costRelativeTolerance &&\n                     preRed <= costRelativeTolerance &&\n                     ratio <= 2.0) ||\n                    delta <= parRelativeTolerance * xNorm) {\n                    setCost(currentCost);\n                    return current;\n                }\n\n                // tests for termination and stringent tolerances\n                // (2.2204e-16 is the machine epsilon for IEEE754)\n                if ((FastMath.abs(actRed) <= 2.2204e-16) && (preRed <= 2.2204e-16) && (ratio <= 2.0)) {\n                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_COST_RELATIVE_TOLERANCE,\n                                                   costRelativeTolerance);\n                } else if (delta <= 2.2204e-16 * xNorm) {\n                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE,\n                                                   parRelativeTolerance);\n                } else if (maxCosine <= 2.2204e-16)  {\n                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_ORTHOGONALITY_TOLERANCE,\n                                                   orthoTolerance);\n                }\n            }\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointVectorValuePair doOptimize() {\n        checkParameters();\n\n        final int nR = getTarget().length; // Number of observed data.\n        final double[] currentPoint = getStartPoint();\n        final int nC = currentPoint.length; // Number of parameters.\n\n        // arrays shared with the other private methods\n        solvedCols  = FastMath.min(nR, nC);\n        diagR       = new double[nC];\n        jacNorm     = new double[nC];\n        beta        = new double[nC];\n        permutation = new int[nC];\n        lmDir       = new double[nC];\n\n        // local point\n        double   delta   = 0;\n        double   xNorm   = 0;\n        double[] diag    = new double[nC];\n        double[] oldX    = new double[nC];\n        double[] oldRes  = new double[nR];\n        double[] oldObj  = new double[nR];\n        double[] qtf     = new double[nR];\n        double[] work1   = new double[nC];\n        double[] work2   = new double[nC];\n        double[] work3   = new double[nC];\n\n        final RealMatrix weightMatrixSqrt = getWeightSquareRoot();\n\n        // Evaluate the function at the starting point and calculate its norm.\n        double[] currentObjective = computeObjectiveValue(currentPoint);\n        double[] currentResiduals = computeResiduals(currentObjective);\n        PointVectorValuePair current = new PointVectorValuePair(currentPoint.clone(), currentObjective.clone());\n        double currentCost = computeCost(currentResiduals);\n\n        // Outer loop.\n        lmPar = 0;\n        boolean firstIteration = true;\n        int iter = 0;\n        final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n        while (true) {\n            ++iter;\n            final PointVectorValuePair previous = current;\n\n            // QR decomposition of the jacobian matrix\n            qrDecomposition(computeWeightedJacobian(currentPoint));\n\n            weightedResidual = weightMatrixSqrt.operate(currentResiduals);\n            for (int i = 0; i < nR; i++) {\n                qtf[i] = weightedResidual[i];\n            }\n\n            // compute Qt.res\n            qTy(qtf);\n\n            // now we don't need Q anymore,\n            // so let jacobian contain the R matrix with its diagonal elements\n            for (int k = 0; k < solvedCols; ++k) {\n                int pk = permutation[k];\n                weightedJacobian[k][pk] = diagR[pk];\n            }\n\n            if (firstIteration) {\n                // scale the point according to the norms of the columns\n                // of the initial jacobian\n                xNorm = 0;\n                for (int k = 0; k < nC; ++k) {\n                    double dk = jacNorm[k];\n                    if (dk == 0) {\n                        dk = 1.0;\n                    }\n                    double xk = dk * currentPoint[k];\n                    xNorm  += xk * xk;\n                    diag[k] = dk;\n                }\n                xNorm = FastMath.sqrt(xNorm);\n\n                // initialize the step bound delta\n                delta = (xNorm == 0) ? initialStepBoundFactor : (initialStepBoundFactor * xNorm);\n            }\n\n            // check orthogonality between function vector and jacobian columns\n            double maxCosine = 0;\n            if (currentCost != 0) {\n                for (int j = 0; j < solvedCols; ++j) {\n                    int    pj = permutation[j];\n                    double s  = jacNorm[pj];\n                    if (s != 0) {\n                        double sum = 0;\n                        for (int i = 0; i <= j; ++i) {\n                            sum += weightedJacobian[i][pj] * qtf[i];\n                        }\n                        maxCosine = FastMath.max(maxCosine, FastMath.abs(sum) / (s * currentCost));\n                    }\n                }\n            }\n            if (maxCosine <= orthoTolerance) {\n                // Convergence has been reached.\n                setCost(currentCost);\n                return current;\n            }\n\n            // rescale if necessary\n            for (int j = 0; j < nC; ++j) {\n                diag[j] = FastMath.max(diag[j], jacNorm[j]);\n            }\n\n            // Inner loop.\n            for (double ratio = 0; ratio < 1.0e-4;) {\n\n                // save the state\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    oldX[pj] = currentPoint[pj];\n                }\n                final double previousCost = currentCost;\n                double[] tmpVec = weightedResidual;\n                weightedResidual = oldRes;\n                oldRes    = tmpVec;\n                tmpVec    = currentObjective;\n                currentObjective = oldObj;\n                oldObj    = tmpVec;\n\n                // determine the Levenberg-Marquardt parameter\n                determineLMParameter(qtf, delta, diag, work1, work2, work3);\n\n                // compute the new point and the norm of the evolution direction\n                double lmNorm = 0;\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    lmDir[pj] = -lmDir[pj];\n                    currentPoint[pj] = oldX[pj] + lmDir[pj];\n                    double s = diag[pj] * lmDir[pj];\n                    lmNorm  += s * s;\n                }\n                lmNorm = FastMath.sqrt(lmNorm);\n                // on the first iteration, adjust the initial step bound.\n                if (firstIteration) {\n                    delta = FastMath.min(delta, lmNorm);\n                }\n\n                // Evaluate the function at x + p and calculate its norm.\n                currentObjective = computeObjectiveValue(currentPoint);\n                currentResiduals = computeResiduals(currentObjective);\n                current = new PointVectorValuePair(currentPoint.clone(), currentObjective.clone());\n                currentCost = computeCost(currentResiduals);\n\n                // compute the scaled actual reduction\n                double actRed = -1.0;\n                if (0.1 * currentCost < previousCost) {\n                    double r = currentCost / previousCost;\n                    actRed = 1.0 - r * r;\n                }\n\n                // compute the scaled predicted reduction\n                // and the scaled directional derivative\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    double dirJ = lmDir[pj];\n                    work1[j] = 0;\n                    for (int i = 0; i <= j; ++i) {\n                        work1[i] += weightedJacobian[i][pj] * dirJ;\n                    }\n                }\n                double coeff1 = 0;\n                for (int j = 0; j < solvedCols; ++j) {\n                    coeff1 += work1[j] * work1[j];\n                }\n                double pc2 = previousCost * previousCost;\n                coeff1 = coeff1 / pc2;\n                double coeff2 = lmPar * lmNorm * lmNorm / pc2;\n                double preRed = coeff1 + 2 * coeff2;\n                double dirDer = -(coeff1 + coeff2);\n\n                // ratio of the actual to the predicted reduction\n                ratio = (preRed == 0) ? 0 : (actRed / preRed);\n\n                // update the step bound\n                if (ratio <= 0.25) {\n                    double tmp =\n                        (actRed < 0) ? (0.5 * dirDer / (dirDer + 0.5 * actRed)) : 0.5;\n                        if ((0.1 * currentCost >= previousCost) || (tmp < 0.1)) {\n                            tmp = 0.1;\n                        }\n                        delta = tmp * FastMath.min(delta, 10.0 * lmNorm);\n                        lmPar /= tmp;\n                } else if ((lmPar == 0) || (ratio >= 0.75)) {\n                    delta = 2 * lmNorm;\n                    lmPar *= 0.5;\n                }\n\n                // test for successful iteration.\n                if (ratio >= 1.0e-4) {\n                    // successful iteration, update the norm\n                    firstIteration = false;\n                    xNorm = 0;\n                    for (int k = 0; k < nC; ++k) {\n                        double xK = diag[k] * currentPoint[k];\n                        xNorm += xK * xK;\n                    }\n                    xNorm = FastMath.sqrt(xNorm);\n\n                    // tests for convergence.\n                    if (checker != null) {\n                        // we use the vectorial convergence checker\n                        if (checker.converged(iter, previous, current)) {\n                            setCost(currentCost);\n                            return current;\n                        }\n                    }\n                } else {\n                    // failed iteration, reset the previous values\n                    currentCost = previousCost;\n                    for (int j = 0; j < solvedCols; ++j) {\n                        int pj = permutation[j];\n                        currentPoint[pj] = oldX[pj];\n                    }\n                    tmpVec    = weightedResidual;\n                    weightedResidual = oldRes;\n                    oldRes    = tmpVec;\n                    tmpVec    = currentObjective;\n                    currentObjective = oldObj;\n                    oldObj    = tmpVec;\n                    // Reset \"current\" to previous values.\n                    current = new PointVectorValuePair(currentPoint.clone(), currentObjective.clone());\n                }\n\n                // Default convergence criteria.\n                if ((FastMath.abs(actRed) <= costRelativeTolerance &&\n                     preRed <= costRelativeTolerance &&\n                     ratio <= 2.0) ||\n                    delta <= parRelativeTolerance * xNorm) {\n                    setCost(currentCost);\n                    return current;\n                }\n\n                // tests for termination and stringent tolerances\n                // (2.2204e-16 is the machine epsilon for IEEE754)\n                if ((FastMath.abs(actRed) <= 2.2204e-16) && (preRed <= 2.2204e-16) && (ratio <= 2.0)) {\n                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_COST_RELATIVE_TOLERANCE,\n                                                   costRelativeTolerance);\n                } else if (delta <= 2.2204e-16 * xNorm) {\n                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE,\n                                                   parRelativeTolerance);\n                } else if (maxCosine <= 2.2204e-16)  {\n                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_ORTHOGONALITY_TOLERANCE,\n                                                   orthoTolerance);\n                }\n            }\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointVectorValuePair doOptimize() {\n        checkParameters();\n\n        final int nR = getTarget().length; // Number of observed data.\n        final double[] currentPoint = getStartPoint();\n        final int nC = currentPoint.length; // Number of parameters.\n\n        // arrays shared with the other private methods\n        solvedCols  = FastMath.min(nR, nC);\n        diagR       = new double[nC];\n        jacNorm     = new double[nC];\n        beta        = new double[nC];\n        permutation = new int[nC];\n        lmDir       = new double[nC];\n\n        // local point\n        double   delta   = 0;\n        double   xNorm   = 0;\n        double[] diag    = new double[nC];\n        double[] oldX    = new double[nC];\n        double[] oldRes  = new double[nR];\n        double[] oldObj  = new double[nR];\n        double[] qtf     = new double[nR];\n        double[] work1   = new double[nC];\n        double[] work2   = new double[nC];\n        double[] work3   = new double[nC];\n\n        final RealMatrix weightMatrixSqrt = getWeightSquareRoot();\n\n        // Evaluate the function at the starting point and calculate its norm.\n        double[] currentObjective = computeObjectiveValue(currentPoint);\n        double[] currentResiduals = computeResiduals(currentObjective);\n        PointVectorValuePair current = new PointVectorValuePair(currentPoint.clone(), currentObjective);\n        double currentCost = computeCost(currentResiduals);\n\n        // Outer loop.\n        lmPar = 0;\n        boolean firstIteration = true;\n        int iter = 0;\n        final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n        while (true) {\n            ++iter;\n            final PointVectorValuePair previous = current;\n\n            // QR decomposition of the jacobian matrix\n            qrDecomposition(computeWeightedJacobian(currentPoint));\n\n            weightedResidual = weightMatrixSqrt.operate(currentResiduals);\n            for (int i = 0; i < nR; i++) {\n                qtf[i] = weightedResidual[i];\n            }\n\n            // compute Qt.res\n            qTy(qtf);\n\n            // now we don't need Q anymore,\n            // so let jacobian contain the R matrix with its diagonal elements\n            for (int k = 0; k < solvedCols; ++k) {\n                int pk = permutation[k];\n                weightedJacobian[k][pk] = diagR[pk];\n            }\n\n            if (firstIteration) {\n                // scale the point according to the norms of the columns\n                // of the initial jacobian\n                xNorm = 0;\n                for (int k = 0; k < nC; ++k) {\n                    double dk = jacNorm[k];\n                    if (dk == 0) {\n                        dk = 1.0;\n                    }\n                    double xk = dk * currentPoint[k];\n                    xNorm  += xk * xk;\n                    diag[k] = dk;\n                }\n                xNorm = FastMath.sqrt(xNorm);\n\n                // initialize the step bound delta\n                delta = (xNorm == 0) ? initialStepBoundFactor : (initialStepBoundFactor * xNorm);\n            }\n\n            // check orthogonality between function vector and jacobian columns\n            double maxCosine = 0;\n            if (currentCost != 0) {\n                for (int j = 0; j < solvedCols; ++j) {\n                    int    pj = permutation[j];\n                    double s  = jacNorm[pj];\n                    if (s != 0) {\n                        double sum = 0;\n                        for (int i = 0; i <= j; ++i) {\n                            sum += weightedJacobian[i][pj] * qtf[i];\n                        }\n                        maxCosine = FastMath.max(maxCosine, FastMath.abs(sum) / (s * currentCost));\n                    }\n                }\n            }\n            if (maxCosine <= orthoTolerance) {\n                // Convergence has been reached.\n                setCost(currentCost);\n                return current;\n            }\n\n            // rescale if necessary\n            for (int j = 0; j < nC; ++j) {\n                diag[j] = FastMath.max(diag[j], jacNorm[j]);\n            }\n\n            // Inner loop.\n            for (double ratio = 0; ratio < 1.0e-4;) {\n\n                // save the state\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    oldX[pj] = currentPoint[pj];\n                }\n                final double previousCost = currentCost;\n                double[] tmpVec = weightedResidual;\n                weightedResidual = oldRes;\n                oldRes    = tmpVec;\n                tmpVec    = currentObjective;\n                currentObjective = oldObj;\n                oldObj    = tmpVec;\n\n                // determine the Levenberg-Marquardt parameter\n                determineLMParameter(qtf, delta, diag, work1, work2, work3);\n\n                // compute the new point and the norm of the evolution direction\n                double lmNorm = 0;\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    lmDir[pj] = -lmDir[pj];\n                    currentPoint[pj] = oldX[pj] + lmDir[pj];\n                    double s = diag[pj] * lmDir[pj];\n                    lmNorm  += s * s;\n                }\n                lmNorm = FastMath.sqrt(lmNorm);\n                // on the first iteration, adjust the initial step bound.\n                if (firstIteration) {\n                    delta = FastMath.min(delta, lmNorm);\n                }\n\n                // Evaluate the function at x + p and calculate its norm.\n                currentObjective = computeObjectiveValue(currentPoint);\n                currentResiduals = computeResiduals(currentObjective);\n                current = new PointVectorValuePair(currentPoint.clone(), currentObjective);\n                currentCost = computeCost(currentResiduals);\n\n                // compute the scaled actual reduction\n                double actRed = -1.0;\n                if (0.1 * currentCost < previousCost) {\n                    double r = currentCost / previousCost;\n                    actRed = 1.0 - r * r;\n                }\n\n                // compute the scaled predicted reduction\n                // and the scaled directional derivative\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    double dirJ = lmDir[pj];\n                    work1[j] = 0;\n                    for (int i = 0; i <= j; ++i) {\n                        work1[i] += weightedJacobian[i][pj] * dirJ;\n                    }\n                }\n                double coeff1 = 0;\n                for (int j = 0; j < solvedCols; ++j) {\n                    coeff1 += work1[j] * work1[j];\n                }\n                double pc2 = previousCost * previousCost;\n                coeff1 = coeff1 / pc2;\n                double coeff2 = lmPar * lmNorm * lmNorm / pc2;\n                double preRed = coeff1 + 2 * coeff2;\n                double dirDer = -(coeff1 + coeff2);\n\n                // ratio of the actual to the predicted reduction\n                ratio = (preRed == 0) ? 0 : (actRed / preRed);\n\n                // update the step bound\n                if (ratio <= 0.25) {\n                    double tmp =\n                        (actRed < 0) ? (0.5 * dirDer / (dirDer + 0.5 * actRed)) : 0.5;\n                        if ((0.1 * currentCost >= previousCost) || (tmp < 0.1)) {\n                            tmp = 0.1;\n                        }\n                        delta = tmp * FastMath.min(delta, 10.0 * lmNorm);\n                        lmPar /= tmp;\n                } else if ((lmPar == 0) || (ratio >= 0.75)) {\n                    delta = 2 * lmNorm;\n                    lmPar *= 0.5;\n                }\n\n                // test for successful iteration.\n                if (ratio >= 1.0e-4) {\n                    // successful iteration, update the norm\n                    firstIteration = false;\n                    xNorm = 0;\n                    for (int k = 0; k < nC; ++k) {\n                        double xK = diag[k] * currentPoint[k];\n                        xNorm += xK * xK;\n                    }\n                    xNorm = FastMath.sqrt(xNorm);\n\n                    // tests for convergence.\n                    if (checker != null) {\n                        // we use the vectorial convergence checker\n                        if (checker.converged(iter, previous, current)) {\n                            setCost(currentCost);\n                            return current;\n                        }\n                    }\n                } else {\n                    // failed iteration, reset the previous values\n                    currentCost = previousCost;\n                    for (int j = 0; j < solvedCols; ++j) {\n                        int pj = permutation[j];\n                        currentPoint[pj] = oldX[pj];\n                    }\n                    tmpVec    = weightedResidual;\n                    weightedResidual = oldRes;\n                    oldRes    = tmpVec;\n                    tmpVec    = currentObjective;\n                    currentObjective = oldObj;\n                    oldObj    = tmpVec;\n                    // Reset \"current\" to previous values.\n                    current = new PointVectorValuePair(currentPoint.clone(), currentObjective);\n                }\n\n                // Default convergence criteria.\n                if ((FastMath.abs(actRed) <= costRelativeTolerance &&\n                     preRed <= costRelativeTolerance &&\n                     ratio <= 2.0) ||\n                    delta <= parRelativeTolerance * xNorm) {\n                    setCost(currentCost);\n                    return current;\n                }\n\n                // tests for termination and stringent tolerances\n                // (2.2204e-16 is the machine epsilon for IEEE754)\n                if ((FastMath.abs(actRed) <= 2.2204e-16) && (preRed <= 2.2204e-16) && (ratio <= 2.0)) {\n                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_COST_RELATIVE_TOLERANCE,\n                                                   costRelativeTolerance);\n                } else if (delta <= 2.2204e-16 * xNorm) {\n                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE,\n                                                   parRelativeTolerance);\n                } else if (maxCosine <= 2.2204e-16)  {\n                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_ORTHOGONALITY_TOLERANCE,\n                                                   orthoTolerance);\n                }\n            }\n        }\n    }\n",
                            "\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointVectorValuePair doOptimize() {\n        checkParameters();\n\n        final int nR = getTarget().length; // Number of observations.\n        final double[] currentPoint = getStartPoint();\n        final int nC = currentPoint.length; // Number of parameters.\n\n        // Arrays shared with the other private methods.\n        solvedCols  = FastMath.min(nR, nC);\n        diagR       = new double[nC];\n        jacNorm     = new double[nC];\n        beta        = new double[nC];\n        permutation = new int[nC];\n        lmDir       = new double[nC];\n\n        // Local point.\n        double   delta   = 0;\n        double   xNorm   = 0;\n        double[] diag    = new double[nC];\n        double[] oldX    = new double[nC];\n        double[] oldRes  = new double[nR];\n        double[] oldObj  = new double[nR];\n        double[] qtf     = new double[nR];\n        double[] work1   = new double[nC];\n        double[] work2   = new double[nC];\n        double[] work3   = new double[nC];\n\n        final RealMatrix weightMatrixSqrt = getWeightSquareRoot();\n\n        // Evaluate the function at the starting point and calculate its norm.\n        double[] currentObjective = computeObjectiveValue(currentPoint);\n        double[] currentResiduals = computeResiduals(currentObjective);\n        PointVectorValuePair current = new PointVectorValuePair(currentPoint, currentObjective);\n        double currentCost = computeCost(currentResiduals);\n\n        // Outer loop.\n        lmPar = 0;\n        boolean firstIteration = true;\n        int iter = 0;\n        final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n        while (true) {\n            ++iter;\n            final PointVectorValuePair previous = current;\n\n            // QR decomposition of the jacobian matrix\n            qrDecomposition(computeWeightedJacobian(currentPoint));\n\n            weightedResidual = weightMatrixSqrt.operate(currentResiduals);\n            for (int i = 0; i < nR; i++) {\n                qtf[i] = weightedResidual[i];\n            }\n\n            // Compute Qt.res\n            qTy(qtf);\n\n            // Now we don't need Q anymore,\n            // so let jacobian contain the R matrix with its diagonal elements.\n            for (int k = 0; k < solvedCols; ++k) {\n                int pk = permutation[k];\n                weightedJacobian[k][pk] = diagR[pk];\n            }\n\n            if (firstIteration) {\n                // Scale the point according to the norms of the columns of the initial jacobian.\n                xNorm = 0;\n                for (int k = 0; k < nC; ++k) {\n                    double dk = jacNorm[k];\n                    if (dk == 0) {\n                        dk = 1.0;\n                    }\n                    double xk = dk * currentPoint[k];\n                    xNorm += xk * xk;\n                    diag[k] = dk;\n                }\n                xNorm = FastMath.sqrt(xNorm);\n\n                // Initialize the step bound delta.\n                delta = (xNorm == 0) ? initialStepBoundFactor : (initialStepBoundFactor * xNorm);\n                firstIteration = false;\n            }\n\n            // Check orthogonality between function vector and jacobian columns.\n            double maxCosine = 0;\n            if (currentCost != 0) {\n                for (int j = 0; j < solvedCols; ++j) {\n                    int    pj = permutation[j];\n                    double s  = jacNorm[pj];\n                    if (s != 0) {\n                        double sum = 0;\n                        for (int i = 0; i <= j; ++i) {\n                            sum += weightedJacobian[i][pj] * qtf[i];\n                        }\n                        maxCosine = FastMath.max(maxCosine, FastMath.abs(sum) / (s * currentCost));\n                    }\n                }\n            }\n            if (maxCosine <= orthoTolerance) {\n                // Convergence has been reached.\n                setCost(currentCost);\n                return current;\n            }\n\n            // Rescale if necessary.\n            for (int j = 0; j < nC; ++j) {\n                diag[j] = FastMath.max(diag[j], jacNorm[j]);\n            }\n\n            // Inner loop.\n            for (double ratio = 0; ratio < 1.0e-4;) {\n\n                // Save the state.\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    oldX[pj] = currentPoint[pj];\n                }\n                final double previousCost = currentCost;\n                double[] tmpVec = weightedResidual;\n                weightedResidual = oldRes;\n                oldRes = tmpVec;\n                tmpVec = currentObjective;\n                currentObjective = oldObj;\n                oldObj = tmpVec;\n\n                // Determine the Levenberg-Marquardt parameter.\n                determineLMParameter(qtf, delta, diag, work1, work2, work3);\n\n                // Compute the new point and the norm of the evolution direction.\n                double lmNorm = 0;\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    lmDir[pj] = -lmDir[pj];\n                    currentPoint[pj] = oldX[pj] + lmDir[pj];\n                    double s = diag[pj] * lmDir[pj];\n                    lmNorm += s * s;\n                }\n                lmNorm = FastMath.sqrt(lmNorm);\n\n                // Evaluate the function at x + p and calculate its norm.\n                currentObjective = computeObjectiveValue(currentPoint);\n                currentResiduals = computeResiduals(currentObjective);\n                current = new PointVectorValuePair(currentPoint, currentObjective);\n                currentCost = computeCost(currentResiduals);\n\n                // Compute the scaled actual reduction.\n                double actRed = -1.0;\n                if (0.1 * currentCost < previousCost) {\n                    double r = currentCost / previousCost;\n                    actRed = 1.0 - r * r;\n                }\n\n                // Compute the scaled predicted reduction\n                // and the scaled directional derivative.\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    double dirJ = lmDir[pj];\n                    work1[j] = 0;\n                    for (int i = 0; i <= j; ++i) {\n                        work1[i] += weightedJacobian[i][pj] * dirJ;\n                    }\n                }\n                double coeff1 = 0;\n                for (int j = 0; j < solvedCols; ++j) {\n                    coeff1 += work1[j] * work1[j];\n                }\n                double pc2 = previousCost * previousCost;\n                coeff1 = coeff1 / pc2;\n                double coeff2 = lmPar * lmNorm * lmNorm / pc2;\n                double preRed = coeff1 + 2 * coeff2;\n                double dirDer = -(coeff1 + coeff2);\n\n                // Ratio of the actual to the predicted reduction.\n                ratio = (preRed == 0) ? 0 : (actRed / preRed);\n\n                // Update the step bound.\n                if (ratio <= 0.25) {\n                    double tmp =\n                        (actRed < 0) ? (0.5 * dirDer / (dirDer + 0.5 * actRed)) : 0.5;\n                    if ((0.1 * currentCost >= previousCost) || (tmp < 0.1)) {\n                        tmp = 0.1;\n                    }\n                    delta = tmp * FastMath.min(delta, 10.0 * lmNorm);\n                    lmPar /= tmp;\n                } else if ((lmPar == 0) || (ratio >= 0.75)) {\n                    delta = 2 * lmNorm;\n                    lmPar *= 0.5;\n                }\n\n                // Test for successful iteration.\n                if (ratio >= 1.0e-4) {\n                    // Successful iteration, update the norm.\n                    xNorm = 0;\n                    for (int k = 0; k < nC; ++k) {\n                        double xK = diag[k] * currentPoint[k];\n                        xNorm += xK * xK;\n                    }\n                    xNorm = FastMath.sqrt(xNorm);\n\n                    // Tests for convergence.\n                    if (checker != null) {\n                        // We use the vectorial convergence checker.\n                        if (checker.converged(iter, previous, current)) {\n                            setCost(currentCost);\n                            return current;\n                        }\n                    }\n                } else {\n                    // Failed iteration, reset the previous values.\n                    currentCost = previousCost;\n                    for (int j = 0; j < solvedCols; ++j) {\n                        int pj = permutation[j];\n                        currentPoint[pj] = oldX[pj];\n                    }\n                    tmpVec = weightedResidual;\n                    weightedResidual = oldRes;\n                    oldRes = tmpVec;\n                    tmpVec = currentObjective;\n                    currentObjective = oldObj;\n                    oldObj = tmpVec;\n                    // Reset \"current\" to previous values.\n                    current = new PointVectorValuePair(currentPoint, currentObjective);\n                }\n\n                // Default convergence criteria.\n                if ((FastMath.abs(actRed) <= costRelativeTolerance &&\n                     preRed <= costRelativeTolerance &&\n                     ratio <= 2.0) ||\n                    delta <= parRelativeTolerance * xNorm) {\n                    setCost(currentCost);\n                    return current;\n                }\n\n                // Tests for termination and stringent tolerances\n                // (2.2204e-16 is the machine epsilon for IEEE754)\n                if ((FastMath.abs(actRed) <= 2.2204e-16) && (preRed <= 2.2204e-16) && (ratio <= 2.0)) {\n                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_COST_RELATIVE_TOLERANCE,\n                                                   costRelativeTolerance);\n                } else if (delta <= 2.2204e-16 * xNorm) {\n                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE,\n                                                   parRelativeTolerance);\n                } else if (maxCosine <= 2.2204e-16) {\n                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_ORTHOGONALITY_TOLERANCE,\n                                                   orthoTolerance);\n                }\n            }\n        }\n    }\n"
                        ]
                    },
                    "2": {
                        "prompt": "You are an expert Java developer. Below is a buggy method from a large Java project. This method causes one or more test failures.\nYour task is to fix the bug in this method. Use the provided test failure(s) and relevant context to guide your reasoning.\n\n---\n\n## Buggy Method\n\n```java\n    /** {@inheritDoc} */\n\n    @Override\n    protected PointVectorValuePair doOptimize() {\n        checkParameters();\n\n        final int nR = getTarget().length; // Number of observed data.\n        final double[] currentPoint = getStartPoint();\n        final int nC = currentPoint.length; // Number of parameters.\n\n        // arrays shared with the other private methods\n        solvedCols  = FastMath.min(nR, nC);\n        diagR       = new double[nC];\n        jacNorm     = new double[nC];\n        beta        = new double[nC];\n        permutation = new int[nC];\n        lmDir       = new double[nC];\n\n        // local point\n        double   delta   = 0;\n        double   xNorm   = 0;\n        double[] diag    = new double[nC];\n        double[] oldX    = new double[nC];\n        double[] oldRes  = new double[nR];\n        double[] oldObj  = new double[nR];\n        double[] qtf     = new double[nR];\n        double[] work1   = new double[nC];\n        double[] work2   = new double[nC];\n        double[] work3   = new double[nC];\n\n        final RealMatrix weightMatrixSqrt = getWeightSquareRoot();\n\n        // Evaluate the function at the starting point and calculate its norm.\n        double[] currentObjective = computeObjectiveValue(currentPoint);\n        double[] currentResiduals = computeResiduals(currentObjective);\n        PointVectorValuePair current = new PointVectorValuePair(currentPoint, currentObjective);\n        double currentCost = computeCost(currentResiduals);\n\n        // Outer loop.\n        lmPar = 0;\n        boolean firstIteration = true;\n        int iter = 0;\n        final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n        while (true) {\n            ++iter;\n            final PointVectorValuePair previous = current;\n\n            // QR decomposition of the jacobian matrix\n            qrDecomposition(computeWeightedJacobian(currentPoint));\n\n            weightedResidual = weightMatrixSqrt.operate(currentResiduals);\n            for (int i = 0; i < nR; i++) {\n                qtf[i] = weightedResidual[i];\n            }\n\n            // compute Qt.res\n            qTy(qtf);\n\n            // now we don't need Q anymore,\n            // so let jacobian contain the R matrix with its diagonal elements\n            for (int k = 0; k < solvedCols; ++k) {\n                int pk = permutation[k];\n                weightedJacobian[k][pk] = diagR[pk];\n            }\n\n            if (firstIteration) {\n                // scale the point according to the norms of the columns\n                // of the initial jacobian\n                xNorm = 0;\n                for (int k = 0; k < nC; ++k) {\n                    double dk = jacNorm[k];\n                    if (dk == 0) {\n                        dk = 1.0;\n                    }\n                    double xk = dk * currentPoint[k];\n                    xNorm  += xk * xk;\n                    diag[k] = dk;\n                }\n                xNorm = FastMath.sqrt(xNorm);\n\n                // initialize the step bound delta\n                delta = (xNorm == 0) ? initialStepBoundFactor : (initialStepBoundFactor * xNorm);\n            }\n\n            // check orthogonality between function vector and jacobian columns\n            double maxCosine = 0;\n            if (currentCost != 0) {\n                for (int j = 0; j < solvedCols; ++j) {\n                    int    pj = permutation[j];\n                    double s  = jacNorm[pj];\n                    if (s != 0) {\n                        double sum = 0;\n                        for (int i = 0; i <= j; ++i) {\n                            sum += weightedJacobian[i][pj] * qtf[i];\n                        }\n                        maxCosine = FastMath.max(maxCosine, FastMath.abs(sum) / (s * currentCost));\n                    }\n                }\n            }\n            if (maxCosine <= orthoTolerance) {\n                // Convergence has been reached.\n                setCost(currentCost);\n                return current;\n            }\n\n            // rescale if necessary\n            for (int j = 0; j < nC; ++j) {\n                diag[j] = FastMath.max(diag[j], jacNorm[j]);\n            }\n\n            // Inner loop.\n            for (double ratio = 0; ratio < 1.0e-4;) {\n\n                // save the state\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    oldX[pj] = currentPoint[pj];\n                }\n                final double previousCost = currentCost;\n                double[] tmpVec = weightedResidual;\n                weightedResidual = oldRes;\n                oldRes    = tmpVec;\n                tmpVec    = currentObjective;\n                currentObjective = oldObj;\n                oldObj    = tmpVec;\n\n                // determine the Levenberg-Marquardt parameter\n                determineLMParameter(qtf, delta, diag, work1, work2, work3);\n\n                // compute the new point and the norm of the evolution direction\n                double lmNorm = 0;\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    lmDir[pj] = -lmDir[pj];\n                    currentPoint[pj] = oldX[pj] + lmDir[pj];\n                    double s = diag[pj] * lmDir[pj];\n                    lmNorm  += s * s;\n                }\n                lmNorm = FastMath.sqrt(lmNorm);\n                // on the first iteration, adjust the initial step bound.\n                if (firstIteration) {\n                    delta = FastMath.min(delta, lmNorm);\n                }\n\n                // Evaluate the function at x + p and calculate its norm.\n                currentObjective = computeObjectiveValue(currentPoint);\n                currentResiduals = computeResiduals(currentObjective);\n                current = new PointVectorValuePair(currentPoint, currentObjective);\n                currentCost = computeCost(currentResiduals);\n\n                // compute the scaled actual reduction\n                double actRed = -1.0;\n                if (0.1 * currentCost < previousCost) {\n                    double r = currentCost / previousCost;\n                    actRed = 1.0 - r * r;\n                }\n\n                // compute the scaled predicted reduction\n                // and the scaled directional derivative\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    double dirJ = lmDir[pj];\n                    work1[j] = 0;\n                    for (int i = 0; i <= j; ++i) {\n                        work1[i] += weightedJacobian[i][pj] * dirJ;\n                    }\n                }\n                double coeff1 = 0;\n                for (int j = 0; j < solvedCols; ++j) {\n                    coeff1 += work1[j] * work1[j];\n                }\n                double pc2 = previousCost * previousCost;\n                coeff1 = coeff1 / pc2;\n                double coeff2 = lmPar * lmNorm * lmNorm / pc2;\n                double preRed = coeff1 + 2 * coeff2;\n                double dirDer = -(coeff1 + coeff2);\n\n                // ratio of the actual to the predicted reduction\n                ratio = (preRed == 0) ? 0 : (actRed / preRed);\n\n                // update the step bound\n                if (ratio <= 0.25) {\n                    double tmp =\n                        (actRed < 0) ? (0.5 * dirDer / (dirDer + 0.5 * actRed)) : 0.5;\n                        if ((0.1 * currentCost >= previousCost) || (tmp < 0.1)) {\n                            tmp = 0.1;\n                        }\n                        delta = tmp * FastMath.min(delta, 10.0 * lmNorm);\n                        lmPar /= tmp;\n                } else if ((lmPar == 0) || (ratio >= 0.75)) {\n                    delta = 2 * lmNorm;\n                    lmPar *= 0.5;\n                }\n\n                // test for successful iteration.\n                if (ratio >= 1.0e-4) {\n                    // successful iteration, update the norm\n                    firstIteration = false;\n                    xNorm = 0;\n                    for (int k = 0; k < nC; ++k) {\n                        double xK = diag[k] * currentPoint[k];\n                        xNorm += xK * xK;\n                    }\n                    xNorm = FastMath.sqrt(xNorm);\n\n                    // tests for convergence.\n                    if (checker != null) {\n                        // we use the vectorial convergence checker\n                        if (checker.converged(iter, previous, current)) {\n                            setCost(currentCost);\n                            return current;\n                        }\n                    }\n                } else {\n                    // failed iteration, reset the previous values\n                    currentCost = previousCost;\n                    for (int j = 0; j < solvedCols; ++j) {\n                        int pj = permutation[j];\n                        currentPoint[pj] = oldX[pj];\n                    }\n                    tmpVec    = weightedResidual;\n                    weightedResidual = oldRes;\n                    oldRes    = tmpVec;\n                    tmpVec    = currentObjective;\n                    currentObjective = oldObj;\n                    oldObj    = tmpVec;\n                    // Reset \"current\" to previous values.\n                    current = new PointVectorValuePair(currentPoint, currentObjective);\n                }\n\n                // Default convergence criteria.\n                if ((FastMath.abs(actRed) <= costRelativeTolerance &&\n                     preRed <= costRelativeTolerance &&\n                     ratio <= 2.0) ||\n                    delta <= parRelativeTolerance * xNorm) {\n                    setCost(currentCost);\n                    return current;\n                }\n\n                // tests for termination and stringent tolerances\n                // (2.2204e-16 is the machine epsilon for IEEE754)\n                if ((FastMath.abs(actRed) <= 2.2204e-16) && (preRed <= 2.2204e-16) && (ratio <= 2.0)) {\n                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_COST_RELATIVE_TOLERANCE,\n                                                   costRelativeTolerance);\n                } else if (delta <= 2.2204e-16 * xNorm) {\n                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE,\n                                                   parRelativeTolerance);\n                } else if (maxCosine <= 2.2204e-16)  {\n                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_ORTHOGONALITY_TOLERANCE,\n                                                   orthoTolerance);\n                }\n            }\n        }\n    }\n\n```\n---\n\n## Failing Test Case(s)\n\n\n\n## \ud83d\udca1 Context from Project\n\n            \n```java\n@Override protected PointVectorValuePair doOptimize() { final int nR = getTarget().length; final double[] currentPoint = getStartPoint(); final int nC = currentPoint.length; solvedCols = FastMath.min(nR, nC); diagR = new double[nC]; jacNorm = new double[nC]; beta = new double[nC]; permutation = new int[nC]; lmDir = new double[nC]; double delta = 0; double xNorm = 0; double[] diag = new double[nC]; double[] oldX = new double[nC]; double[] oldRes = new double[nR]; double[] oldObj = new double[nR]; double[] qtf = new double[nR]; double[] work1 = new double[nC]; double[] work2 = new double[nC]; double[] work3 = new double[nC]; final RealMatrix weightMatrixSqrt = getWeightSquareRoot(); double[] currentObjective = computeObjectiveValue(currentPoint); double[] currentResiduals = computeResiduals(currentObjective); PointVectorValuePair current = new PointVectorValuePair(currentPoint, currentObjective); double currentCost = computeCost(currentResiduals); lmPar = 0; boolean firstIteration = true; int iter = 0; final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker(); while (true) { ++iter; final PointVectorValuePair previous = current; qrDecomposition(computeWeightedJacobian(currentPoint)); weightedResidual = weightMatrixSqrt.operate(currentResiduals); for (int i = 0; i < nR; i++) { qtf[i] = weightedResidual[i]; } qTy(qtf); for (int k = 0; k < solvedCols; ++k) { int pk = permutation[k]; weightedJacobian[k][pk] = diagR[pk]; } if (firstIteration) { xNorm = 0; for (int k = 0; k < nC; ++k) { double dk = jacNorm[k]; if (dk == 0) { dk = 1.0; } double xk = dk * currentPoint[k]; xNorm += xk * xk; diag[k] = dk; } xNorm = FastMath.sqrt(xNorm); delta = (xNorm == 0) ? initialStepBoundFactor : (initialStepBoundFactor * xNorm); } double maxCosine = 0; if (currentCost != 0) { for (int j = 0; j < solvedCols; ++j) { int pj = permutation[j]; double s = jacNorm[pj]; if (s != 0) { double sum = 0; for (int i = 0; i <= j; ++i) { sum += weightedJacobian[i][pj] * qtf[i]; } maxCosine = FastMath.max(maxCosine, FastMath.abs(sum) / (s * currentCost)); } } } if (maxCosine <= orthoTolerance) { setCost(currentCost); point = current.getPoint(); return current; } for (int j = 0; j < nC; ++j) { diag[j] = FastMath.max(diag[j], jacNorm[j]); } for (double ratio = 0; ratio < 1.0e-4; ) { for (int j = 0; j < solvedCols; ++j) { int pj = permutation[j]; oldX[pj] = currentPoint[pj]; } final double previousCost = currentCost; double[] tmpVec = weightedResidual; weightedResidual = oldRes; oldRes = tmpVec; tmpVec = currentObjective; currentObjective = oldObj; oldObj = tmpVec; determineLMParameter(qtf, delta, diag, work1, work2, work3); double lmNorm = 0; for (int j = 0; j < solvedCols; ++j) { int pj = permutation[j]; lmDir[pj] = -lmDir[pj]; currentPoint[pj] = oldX[pj] + lmDir[pj]; double s = diag[pj] * lmDir[pj]; lmNorm += s * s; } lmNorm = FastMath.sqrt(lmNorm); if (firstIteration) { delta = FastMath.min(delta, lmNorm); } currentObjective = computeObjectiveValue(currentPoint); currentResiduals = computeResiduals(currentObjective); current = new PointVectorValuePair(currentPoint, currentObjective); currentCost = computeCost(currentResiduals); double actRed = -1.0; if (0.1 * currentCost < previousCost) { double r = currentCost / previousCost; actRed = 1.0 - r * r; } for (int j = 0; j < solvedCols; ++j) { int pj = permutation[j]; double dirJ = lmDir[pj]; work1[j] = 0; for (int i = 0; i <= j; ++i) { work1[i] += weightedJacobian[i][pj] * dirJ; } } double coeff1 = 0; for (int j = 0; j < solvedCols; ++j) { coeff1 += work1[j] * work1[j]; } double pc2 = previousCost * previousCost; coeff1 = coeff1 / pc2; double coeff2 = lmPar * lmNorm * lmNorm / pc2; double preRed = coeff1 + 2 * coeff2; double dirDer = -(coeff1 + coeff2); ratio = (preRed == 0) ? 0 : (actRed / preRed); if (ratio <= 0.25) { double tmp = (actRed < 0) ? (0.5 * dirDer / (dirDer + 0.5 * actRed)) : 0.5; if ((0.1 * currentCost >= previousCost) || (tmp < 0.1)) { tmp = 0.1; } delta = tmp * FastMath.min(delta, 10.0 * lmNorm); lmPar /= tmp; } else if ((lmPar == 0) || (ratio >= 0.75)) { delta = 2 * lmNorm; lmPar *= 0.5; } if (ratio >= 1.0e-4) { firstIteration = false; xNorm = 0; for (int k = 0; k < nC; ++k) { double xK = diag[k] * currentPoint[k]; xNorm += xK * xK; } xNorm = FastMath.sqrt(xNorm); if (checker != null) { if (checker.converged(iter, previous, current)) { setCost(currentCost); point = current.getPoint(); return current; } } } else { currentCost = previousCost; for (int j = 0; j < solvedCols; ++j) { int pj = permutation[j]; currentPoint[pj] = oldX[pj]; } tmpVec = weightedResidual; weightedResidual = oldRes; oldRes = tmpVec; tmpVec = currentObjective; currentObjective = oldObj; oldObj = tmpVec; current = new PointVectorValuePair(currentPoint, currentObjective); } if ((FastMath.abs(actRed) <= costRelativeTolerance && preRed <= costRelativeTolerance && ratio <= 2.0) || delta <= parRelativeTolerance * xNorm) { setCost(currentCost); point = current.getPoint(); return current; } if ((FastMath.abs(actRed) <= 2.2204e-16) && (preRed <= 2.2204e-16) && (ratio <= 2.0)) { throw new ConvergenceException(LocalizedFormats.TOO_SMALL_COST_RELATIVE_TOLERANCE, costRelativeTolerance); } else if (delta <= 2.2204e-16 * xNorm) { throw new ConvergenceException(LocalizedFormats.TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE, parRelativeTolerance); } else if (maxCosine <= 2.2204e-16) { throw new ConvergenceException(LocalizedFormats.TOO_SMALL_ORTHOGONALITY_TOLERANCE, orthoTolerance); } } } }\n```\n\n            \n```java\nprivate double bobyqb(double[] lowerBound, double[] upperBound) { printMethod(); final int n = currentBest.getDimension(); final int npt = numberOfInterpolationPoints; final int np = n + 1; final int nptm = npt - np; final int nh = n * np / 2; final ArrayRealVector work1 = new ArrayRealVector(n); final ArrayRealVector work2 = new ArrayRealVector(npt); final ArrayRealVector work3 = new ArrayRealVector(npt); double cauchy = Double.NaN; double alpha = Double.NaN; double dsq = Double.NaN; double crvmin = Double.NaN; trustRegionCenterInterpolationPointIndex = 0; prelim(lowerBound, upperBound); double xoptsq = ZERO; for (int i = 0; i < n; i++) { trustRegionCenterOffset.setEntry(i, interpolationPoints.getEntry(trustRegionCenterInterpolationPointIndex, i)); final double deltaOne = trustRegionCenterOffset.getEntry(i); xoptsq += deltaOne * deltaOne; } double fsave = fAtInterpolationPoints.getEntry(0); final int kbase = 0; int ntrits = 0; int itest = 0; int knew = 0; int nfsav = getEvaluations(); double rho = initialTrustRegionRadius; double delta = rho; double diffa = ZERO; double diffb = ZERO; double diffc = ZERO; double f = ZERO; double beta = ZERO; double adelt = ZERO; double denom = ZERO; double ratio = ZERO; double dnorm = ZERO; double scaden = ZERO; double biglsq = ZERO; double distsq = ZERO; int state = 20; for (; ; ) switch(state) { case 20: { printState(20); if (trustRegionCenterInterpolationPointIndex != kbase) { int ih = 0; for (int j = 0; j < n; j++) { for (int i = 0; i <= j; i++) { if (i < j) { gradientAtTrustRegionCenter.setEntry(j, gradientAtTrustRegionCenter.getEntry(j) + modelSecondDerivativesValues.getEntry(ih) * trustRegionCenterOffset.getEntry(i)); } gradientAtTrustRegionCenter.setEntry(i, gradientAtTrustRegionCenter.getEntry(i) + modelSecondDerivativesValues.getEntry(ih) * trustRegionCenterOffset.getEntry(j)); ih++; } } if (getEvaluations() > npt) { for (int k = 0; k < npt; k++) { double temp = ZERO; for (int j = 0; j < n; j++) { temp += interpolationPoints.getEntry(k, j) * trustRegionCenterOffset.getEntry(j); } temp *= modelSecondDerivativesParameters.getEntry(k); for (int i = 0; i < n; i++) { gradientAtTrustRegionCenter.setEntry(i, gradientAtTrustRegionCenter.getEntry(i) + temp * interpolationPoints.getEntry(k, i)); } } } } } case 60: { printState(60); final ArrayRealVector gnew = new ArrayRealVector(n); final ArrayRealVector xbdi = new ArrayRealVector(n); final ArrayRealVector s = new ArrayRealVector(n); final ArrayRealVector hs = new ArrayRealVector(n); final ArrayRealVector hred = new ArrayRealVector(n); final double[] dsqCrvmin = trsbox(delta, gnew, xbdi, s, hs, hred); dsq = dsqCrvmin[0]; crvmin = dsqCrvmin[1]; double deltaOne = delta; double deltaTwo = Math.sqrt(dsq); dnorm = Math.min(deltaOne, deltaTwo); if (dnorm < HALF * rho) { ntrits = -1; deltaOne = TEN * rho; distsq = deltaOne * deltaOne; if (getEvaluations() <= nfsav + 2) { state = 650; break; } deltaOne = Math.max(diffa, diffb); final double errbig = Math.max(deltaOne, diffc); final double frhosq = rho * ONE_OVER_EIGHT * rho; if (crvmin > ZERO && errbig > frhosq * crvmin) { state = 650; break; } final double bdtol = errbig / rho; for (int j = 0; j < n; j++) { double bdtest = bdtol; if (newPoint.getEntry(j) == lowerDifference.getEntry(j)) { bdtest = work1.getEntry(j); } if (newPoint.getEntry(j) == upperDifference.getEntry(j)) { bdtest = -work1.getEntry(j); } if (bdtest < bdtol) { double curv = modelSecondDerivativesValues.getEntry((j + j * j) / 2); for (int k = 0; k < npt; k++) { final double d1 = interpolationPoints.getEntry(k, j); curv += modelSecondDerivativesParameters.getEntry(k) * (d1 * d1); } bdtest += HALF * curv * rho; if (bdtest < bdtol) { state = 650; break; } } } state = 680; break; } ++ntrits; } case 90: { printState(90); if (dsq <= xoptsq * ONE_OVER_A_THOUSAND) { final double fracsq = xoptsq * ONE_OVER_FOUR; double sumpq = ZERO; for (int k = 0; k < npt; k++) { sumpq += modelSecondDerivativesParameters.getEntry(k); double sum = -HALF * xoptsq; for (int i = 0; i < n; i++) { sum += interpolationPoints.getEntry(k, i) * trustRegionCenterOffset.getEntry(i); } work2.setEntry(k, sum); final double temp = fracsq - HALF * sum; for (int i = 0; i < n; i++) { work1.setEntry(i, bMatrix.getEntry(k, i)); lagrangeValuesAtNewPoint.setEntry(i, sum * interpolationPoints.getEntry(k, i) + temp * trustRegionCenterOffset.getEntry(i)); final int ip = npt + i; for (int j = 0; j <= i; j++) { bMatrix.setEntry(ip, j, bMatrix.getEntry(ip, j) + work1.getEntry(i) * lagrangeValuesAtNewPoint.getEntry(j) + lagrangeValuesAtNewPoint.getEntry(i) * work1.getEntry(j)); } } } for (int m = 0; m < nptm; m++) { double sumz = ZERO; double sumw = ZERO; for (int k = 0; k < npt; k++) { sumz += zMatrix.getEntry(k, m); lagrangeValuesAtNewPoint.setEntry(k, work2.getEntry(k) * zMatrix.getEntry(k, m)); sumw += lagrangeValuesAtNewPoint.getEntry(k); } for (int j = 0; j < n; j++) { double sum = (fracsq * sumz - HALF * sumw) * trustRegionCenterOffset.getEntry(j); for (int k = 0; k < npt; k++) { sum += lagrangeValuesAtNewPoint.getEntry(k) * interpolationPoints.getEntry(k, j); } work1.setEntry(j, sum); for (int k = 0; k < npt; k++) { bMatrix.setEntry(k, j, bMatrix.getEntry(k, j) + sum * zMatrix.getEntry(k, m)); } } for (int i = 0; i < n; i++) { final int ip = i + npt; final double temp = work1.getEntry(i); for (int j = 0; j <= i; j++) { bMatrix.setEntry(ip, j, bMatrix.getEntry(ip, j) + temp * work1.getEntry(j)); } } } int ih = 0; for (int j = 0; j < n; j++) { work1.setEntry(j, -HALF * sumpq * trustRegionCenterOffset.getEntry(j)); for (int k = 0; k < npt; k++) { work1.setEntry(j, work1.getEntry(j) + modelSecondDerivativesParameters.getEntry(k) * interpolationPoints.getEntry(k, j)); interpolationPoints.setEntry(k, j, interpolationPoints.getEntry(k, j) - trustRegionCenterOffset.getEntry(j)); } for (int i = 0; i <= j; i++) { modelSecondDerivativesValues.setEntry(ih, modelSecondDerivativesValues.getEntry(ih) + work1.getEntry(i) * trustRegionCenterOffset.getEntry(j) + trustRegionCenterOffset.getEntry(i) * work1.getEntry(j)); bMatrix.setEntry(npt + i, j, bMatrix.getEntry(npt + j, i)); ih++; } } for (int i = 0; i < n; i++) { originShift.setEntry(i, originShift.getEntry(i) + trustRegionCenterOffset.getEntry(i)); newPoint.setEntry(i, newPoint.getEntry(i) - trustRegionCenterOffset.getEntry(i)); lowerDifference.setEntry(i, lowerDifference.getEntry(i) - trustRegionCenterOffset.getEntry(i)); upperDifference.setEntry(i, upperDifference.getEntry(i) - trustRegionCenterOffset.getEntry(i)); trustRegionCenterOffset.setEntry(i, ZERO); } xoptsq = ZERO; } if (ntrits == 0) { state = 210; break; } state = 230; break; } case 210: { printState(210); final double[] alphaCauchy = altmov(knew, adelt); alpha = alphaCauchy[0]; cauchy = alphaCauchy[1]; for (int i = 0; i < n; i++) { trialStepPoint.setEntry(i, newPoint.getEntry(i) - trustRegionCenterOffset.getEntry(i)); } } case 230: { printState(230); for (int k = 0; k < npt; k++) { double suma = ZERO; double sumb = ZERO; double sum = ZERO; for (int j = 0; j < n; j++) { suma += interpolationPoints.getEntry(k, j) * trialStepPoint.getEntry(j); sumb += interpolationPoints.getEntry(k, j) * trustRegionCenterOffset.getEntry(j); sum += bMatrix.getEntry(k, j) * trialStepPoint.getEntry(j); } work3.setEntry(k, suma * (HALF * suma + sumb)); lagrangeValuesAtNewPoint.setEntry(k, sum); work2.setEntry(k, suma); } beta = ZERO; for (int m = 0; m < nptm; m++) { double sum = ZERO; for (int k = 0; k < npt; k++) { sum += zMatrix.getEntry(k, m) * work3.getEntry(k); } beta -= sum * sum; for (int k = 0; k < npt; k++) { lagrangeValuesAtNewPoint.setEntry(k, lagrangeValuesAtNewPoint.getEntry(k) + sum * zMatrix.getEntry(k, m)); } } dsq = ZERO; double bsum = ZERO; double dx = ZERO; for (int j = 0; j < n; j++) { final double d1 = trialStepPoint.getEntry(j); dsq += d1 * d1; double sum = ZERO; for (int k = 0; k < npt; k++) { sum += work3.getEntry(k) * bMatrix.getEntry(k, j); } bsum += sum * trialStepPoint.getEntry(j); final int jp = npt + j; for (int i = 0; i < n; i++) { sum += bMatrix.getEntry(jp, i) * trialStepPoint.getEntry(i); } lagrangeValuesAtNewPoint.setEntry(jp, sum); bsum += sum * trialStepPoint.getEntry(j); dx += trialStepPoint.getEntry(j) * trustRegionCenterOffset.getEntry(j); } beta = dx * dx + dsq * (xoptsq + dx + dx + HALF * dsq) + beta - bsum; lagrangeValuesAtNewPoint.setEntry(trustRegionCenterInterpolationPointIndex, lagrangeValuesAtNewPoint.getEntry(trustRegionCenterInterpolationPointIndex) + ONE); if (ntrits == 0) { final double d1 = lagrangeValuesAtNewPoint.getEntry(knew); denom = d1 * d1 + alpha * beta; if (denom < cauchy && cauchy > ZERO) { for (int i = 0; i < n; i++) { newPoint.setEntry(i, alternativeNewPoint.getEntry(i)); trialStepPoint.setEntry(i, newPoint.getEntry(i) - trustRegionCenterOffset.getEntry(i)); } cauchy = ZERO; state = 230; break; } } else { final double delsq = delta * delta; scaden = ZERO; biglsq = ZERO; knew = 0; for (int k = 0; k < npt; k++) { if (k == trustRegionCenterInterpolationPointIndex) { continue; } double hdiag = ZERO; for (int m = 0; m < nptm; m++) { final double d1 = zMatrix.getEntry(k, m); hdiag += d1 * d1; } final double d2 = lagrangeValuesAtNewPoint.getEntry(k); final double den = beta * hdiag + d2 * d2; distsq = ZERO; for (int j = 0; j < n; j++) { final double d3 = interpolationPoints.getEntry(k, j) - trustRegionCenterOffset.getEntry(j); distsq += d3 * d3; } final double d4 = distsq / delsq; final double temp = Math.max(ONE, d4 * d4); if (temp * den > scaden) { scaden = temp * den; knew = k; denom = den; } final double d5 = lagrangeValuesAtNewPoint.getEntry(k); biglsq = Math.max(biglsq, temp * (d5 * d5)); } } } case 360: { printState(360); for (int i = 0; i < n; i++) { final double d3 = lowerBound[i]; final double d4 = originShift.getEntry(i) + newPoint.getEntry(i); final double d1 = Math.max(d3, d4); final double d2 = upperBound[i]; currentBest.setEntry(i, Math.min(d1, d2)); if (newPoint.getEntry(i) == lowerDifference.getEntry(i)) { currentBest.setEntry(i, lowerBound[i]); } if (newPoint.getEntry(i) == upperDifference.getEntry(i)) { currentBest.setEntry(i, upperBound[i]); } } f = computeObjectiveValue(currentBest.toArray()); if (!isMinimize) f = -f; if (ntrits == -1) { fsave = f; state = 720; break; } final double fopt = fAtInterpolationPoints.getEntry(trustRegionCenterInterpolationPointIndex); double vquad = ZERO; int ih = 0; for (int j = 0; j < n; j++) { vquad += trialStepPoint.getEntry(j) * gradientAtTrustRegionCenter.getEntry(j); for (int i = 0; i <= j; i++) { double temp = trialStepPoint.getEntry(i) * trialStepPoint.getEntry(j); if (i == j) { temp *= HALF; } vquad += modelSecondDerivativesValues.getEntry(ih) * temp; ih++; } } for (int k = 0; k < npt; k++) { final double d1 = work2.getEntry(k); final double d2 = d1 * d1; vquad += HALF * modelSecondDerivativesParameters.getEntry(k) * d2; } final double diff = f - fopt - vquad; diffc = diffb; diffb = diffa; diffa = Math.abs(diff); if (dnorm > rho) { nfsav = getEvaluations(); } if (ntrits > 0) { if (vquad >= ZERO) { throw new MathIllegalStateException(LocalizedFormats.TRUST_REGION_STEP_FAILED, vquad); } ratio = (f - fopt) / vquad; final double hDelta = HALF * delta; if (ratio <= ONE_OVER_TEN) { delta = Math.min(hDelta, dnorm); } else if (ratio <= .7) { delta = Math.max(hDelta, dnorm); } else { delta = Math.max(hDelta, 2 * dnorm); } if (delta <= rho * 1.5) { delta = rho; } if (f < fopt) { final int ksav = knew; final double densav = denom; final double delsq = delta * delta; scaden = ZERO; biglsq = ZERO; knew = 0; for (int k = 0; k < npt; k++) { double hdiag = ZERO; for (int m = 0; m < nptm; m++) { final double d1 = zMatrix.getEntry(k, m); hdiag += d1 * d1; } final double d1 = lagrangeValuesAtNewPoint.getEntry(k); final double den = beta * hdiag + d1 * d1; distsq = ZERO; for (int j = 0; j < n; j++) { final double d2 = interpolationPoints.getEntry(k, j) - newPoint.getEntry(j); distsq += d2 * d2; } final double d3 = distsq / delsq; final double temp = Math.max(ONE, d3 * d3); if (temp * den > scaden) { scaden = temp * den; knew = k; denom = den; } final double d4 = lagrangeValuesAtNewPoint.getEntry(k); final double d5 = temp * (d4 * d4); biglsq = Math.max(biglsq, d5); } if (scaden <= HALF * biglsq) { knew = ksav; denom = densav; } } } update(beta, denom, knew); ih = 0; final double pqold = modelSecondDerivativesParameters.getEntry(knew); modelSecondDerivativesParameters.setEntry(knew, ZERO); for (int i = 0; i < n; i++) { final double temp = pqold * interpolationPoints.getEntry(knew, i); for (int j = 0; j <= i; j++) { modelSecondDerivativesValues.setEntry(ih, modelSecondDerivativesValues.getEntry(ih) + temp * interpolationPoints.getEntry(knew, j)); ih++; } } for (int m = 0; m < nptm; m++) { final double temp = diff * zMatrix.getEntry(knew, m); for (int k = 0; k < npt; k++) { modelSecondDerivativesParameters.setEntry(k, modelSecondDerivativesParameters.getEntry(k) + temp * zMatrix.getEntry(k, m)); } } fAtInterpolationPoints.setEntry(knew, f); for (int i = 0; i < n; i++) { interpolationPoints.setEntry(knew, i, newPoint.getEntry(i)); work1.setEntry(i, bMatrix.getEntry(knew, i)); } for (int k = 0; k < npt; k++) { double suma = ZERO; for (int m = 0; m < nptm; m++) { suma += zMatrix.getEntry(knew, m) * zMatrix.getEntry(k, m); } double sumb = ZERO; for (int j = 0; j < n; j++) { sumb += interpolationPoints.getEntry(k, j) * trustRegionCenterOffset.getEntry(j); } final double temp = suma * sumb; for (int i = 0; i < n; i++) { work1.setEntry(i, work1.getEntry(i) + temp * interpolationPoints.getEntry(k, i)); } } for (int i = 0; i < n; i++) { gradientAtTrustRegionCenter.setEntry(i, gradientAtTrustRegionCenter.getEntry(i) + diff * work1.getEntry(i)); } if (f < fopt) { trustRegionCenterInterpolationPointIndex = knew; xoptsq = ZERO; ih = 0; for (int j = 0; j < n; j++) { trustRegionCenterOffset.setEntry(j, newPoint.getEntry(j)); final double d1 = trustRegionCenterOffset.getEntry(j); xoptsq += d1 * d1; for (int i = 0; i <= j; i++) { if (i < j) { gradientAtTrustRegionCenter.setEntry(j, gradientAtTrustRegionCenter.getEntry(j) + modelSecondDerivativesValues.getEntry(ih) * trialStepPoint.getEntry(i)); } gradientAtTrustRegionCenter.setEntry(i, gradientAtTrustRegionCenter.getEntry(i) + modelSecondDerivativesValues.getEntry(ih) * trialStepPoint.getEntry(j)); ih++; } } for (int k = 0; k < npt; k++) { double temp = ZERO; for (int j = 0; j < n; j++) { temp += interpolationPoints.getEntry(k, j) * trialStepPoint.getEntry(j); } temp *= modelSecondDerivativesParameters.getEntry(k); for (int i = 0; i < n; i++) { gradientAtTrustRegionCenter.setEntry(i, gradientAtTrustRegionCenter.getEntry(i) + temp * interpolationPoints.getEntry(k, i)); } } } if (ntrits > 0) { for (int k = 0; k < npt; k++) { lagrangeValuesAtNewPoint.setEntry(k, fAtInterpolationPoints.getEntry(k) - fAtInterpolationPoints.getEntry(trustRegionCenterInterpolationPointIndex)); work3.setEntry(k, ZERO); } for (int j = 0; j < nptm; j++) { double sum = ZERO; for (int k = 0; k < npt; k++) { sum += zMatrix.getEntry(k, j) * lagrangeValuesAtNewPoint.getEntry(k); } for (int k = 0; k < npt; k++) { work3.setEntry(k, work3.getEntry(k) + sum * zMatrix.getEntry(k, j)); } } for (int k = 0; k < npt; k++) { double sum = ZERO; for (int j = 0; j < n; j++) { sum += interpolationPoints.getEntry(k, j) * trustRegionCenterOffset.getEntry(j); } work2.setEntry(k, work3.getEntry(k)); work3.setEntry(k, sum * work3.getEntry(k)); } double gqsq = ZERO; double gisq = ZERO; for (int i = 0; i < n; i++) { double sum = ZERO; for (int k = 0; k < npt; k++) { sum += bMatrix.getEntry(k, i) * lagrangeValuesAtNewPoint.getEntry(k) + interpolationPoints.getEntry(k, i) * work3.getEntry(k); } if (trustRegionCenterOffset.getEntry(i) == lowerDifference.getEntry(i)) { final double d1 = Math.min(ZERO, gradientAtTrustRegionCenter.getEntry(i)); gqsq += d1 * d1; final double d2 = Math.min(ZERO, sum); gisq += d2 * d2; } else if (trustRegionCenterOffset.getEntry(i) == upperDifference.getEntry(i)) { final double d1 = Math.max(ZERO, gradientAtTrustRegionCenter.getEntry(i)); gqsq += d1 * d1; final double d2 = Math.max(ZERO, sum); gisq += d2 * d2; } else { final double d1 = gradientAtTrustRegionCenter.getEntry(i); gqsq += d1 * d1; gisq += sum * sum; } lagrangeValuesAtNewPoint.setEntry(npt + i, sum); } ++itest; if (gqsq < TEN * gisq) { itest = 0; } if (itest >= 3) { for (int i = 0, max = Math.max(npt, nh); i < max; i++) { if (i < n) { gradientAtTrustRegionCenter.setEntry(i, lagrangeValuesAtNewPoint.getEntry(npt + i)); } if (i < npt) { modelSecondDerivativesParameters.setEntry(i, work2.getEntry(i)); } if (i < nh) { modelSecondDerivativesValues.setEntry(i, ZERO); } itest = 0; } } } if (ntrits == 0) { state = 60; break; } if (f <= fopt + ONE_OVER_TEN * vquad) { state = 60; break; } final double d1 = TWO * delta; final double d2 = TEN * rho; distsq = Math.max(d1 * d1, d2 * d2); } case 650: { printState(650); knew = -1; for (int k = 0; k < npt; k++) { double sum = ZERO; for (int j = 0; j < n; j++) { final double d1 = interpolationPoints.getEntry(k, j) - trustRegionCenterOffset.getEntry(j); sum += d1 * d1; } if (sum > distsq) { knew = k; distsq = sum; } } if (knew >= 0) { final double dist = Math.sqrt(distsq); if (ntrits == -1) { delta = Math.min(ONE_OVER_TEN * delta, HALF * dist); if (delta <= rho * 1.5) { delta = rho; } } ntrits = 0; final double d1 = Math.min(ONE_OVER_TEN * dist, delta); adelt = Math.max(d1, rho); dsq = adelt * adelt; state = 90; break; } if (ntrits == -1) { state = 680; break; } if (ratio > ZERO) { state = 60; break; } if (Math.max(delta, dnorm) > rho) { state = 60; break; } } case 680: { printState(680); if (rho > stoppingTrustRegionRadius) { delta = HALF * rho; ratio = rho / stoppingTrustRegionRadius; if (ratio <= SIXTEEN) { rho = stoppingTrustRegionRadius; } else if (ratio <= TWO_HUNDRED_FIFTY) { rho = Math.sqrt(ratio) * stoppingTrustRegionRadius; } else { rho *= ONE_OVER_TEN; } delta = Math.max(delta, rho); ntrits = 0; nfsav = getEvaluations(); state = 60; break; } if (ntrits == -1) { state = 360; break; } } case 720: { printState(720); if (fAtInterpolationPoints.getEntry(trustRegionCenterInterpolationPointIndex) <= fsave) { for (int i = 0; i < n; i++) { final double d3 = lowerBound[i]; final double d4 = originShift.getEntry(i) + trustRegionCenterOffset.getEntry(i); final double d1 = Math.max(d3, d4); final double d2 = upperBound[i]; currentBest.setEntry(i, Math.min(d1, d2)); if (trustRegionCenterOffset.getEntry(i) == lowerDifference.getEntry(i)) { currentBest.setEntry(i, lowerBound[i]); } if (trustRegionCenterOffset.getEntry(i) == upperDifference.getEntry(i)) { currentBest.setEntry(i, upperBound[i]); } } f = fAtInterpolationPoints.getEntry(trustRegionCenterInterpolationPointIndex); } return f; } default: { throw new MathIllegalStateException(LocalizedFormats.SIMPLE_MESSAGE, \\\"bobyqb\\\"); } } }\n```\n\n            \n```java\nprivate double bobyqb(double[] lowerBound, double[] upperBound) { printMethod(); final int n = currentBest.getDimension(); final int npt = numberOfInterpolationPoints; final int np = n + 1; final int nptm = npt - np; final int nh = n * np / 2; final ArrayRealVector work1 = new ArrayRealVector(n); final ArrayRealVector work2 = new ArrayRealVector(npt); final ArrayRealVector work3 = new ArrayRealVector(npt); double cauchy = Double.NaN; double alpha = Double.NaN; double dsq = Double.NaN; double crvmin = Double.NaN; trustRegionCenterInterpolationPointIndex = 0; prelim(lowerBound, upperBound); double xoptsq = ZERO; for (int i = 0; i < n; i++) { trustRegionCenterOffset.setEntry(i, interpolationPoints.getEntry(trustRegionCenterInterpolationPointIndex, i)); final double deltaOne = trustRegionCenterOffset.getEntry(i); xoptsq += deltaOne * deltaOne; } double fsave = fAtInterpolationPoints.getEntry(0); final int kbase = 0; int ntrits = 0; int itest = 0; int knew = 0; int nfsav = getEvaluations(); double rho = initialTrustRegionRadius; double delta = rho; double diffa = ZERO; double diffb = ZERO; double diffc = ZERO; double f = ZERO; double beta = ZERO; double adelt = ZERO; double denom = ZERO; double ratio = ZERO; double dnorm = ZERO; double scaden = ZERO; double biglsq = ZERO; double distsq = ZERO; int state = 20; for (; ; ) switch(state) { case 20: { printState(20); if (trustRegionCenterInterpolationPointIndex != kbase) { int ih = 0; for (int j = 0; j < n; j++) { for (int i = 0; i <= j; i++) { if (i < j) { gradientAtTrustRegionCenter.setEntry(j, gradientAtTrustRegionCenter.getEntry(j) + modelSecondDerivativesValues.getEntry(ih) * trustRegionCenterOffset.getEntry(i)); } gradientAtTrustRegionCenter.setEntry(i, gradientAtTrustRegionCenter.getEntry(i) + modelSecondDerivativesValues.getEntry(ih) * trustRegionCenterOffset.getEntry(j)); ih++; } } if (getEvaluations() > npt) { for (int k = 0; k < npt; k++) { double temp = ZERO; for (int j = 0; j < n; j++) { temp += interpolationPoints.getEntry(k, j) * trustRegionCenterOffset.getEntry(j); } temp *= modelSecondDerivativesParameters.getEntry(k); for (int i = 0; i < n; i++) { gradientAtTrustRegionCenter.setEntry(i, gradientAtTrustRegionCenter.getEntry(i) + temp * interpolationPoints.getEntry(k, i)); } } } } } case 60: { printState(60); final ArrayRealVector gnew = new ArrayRealVector(n); final ArrayRealVector xbdi = new ArrayRealVector(n); final ArrayRealVector s = new ArrayRealVector(n); final ArrayRealVector hs = new ArrayRealVector(n); final ArrayRealVector hred = new ArrayRealVector(n); final double[] dsqCrvmin = trsbox(delta, gnew, xbdi, s, hs, hred); dsq = dsqCrvmin[0]; crvmin = dsqCrvmin[1]; double deltaOne = delta; double deltaTwo = Math.sqrt(dsq); dnorm = Math.min(deltaOne, deltaTwo); if (dnorm < HALF * rho) { ntrits = -1; deltaOne = TEN * rho; distsq = deltaOne * deltaOne; if (getEvaluations() <= nfsav + 2) { state = 650; break; } deltaOne = Math.max(diffa, diffb); final double errbig = Math.max(deltaOne, diffc); final double frhosq = rho * ONE_OVER_EIGHT * rho; if (crvmin > ZERO && errbig > frhosq * crvmin) { state = 650; break; } final double bdtol = errbig / rho; for (int j = 0; j < n; j++) { double bdtest = bdtol; if (newPoint.getEntry(j) == lowerDifference.getEntry(j)) { bdtest = work1.getEntry(j); } if (newPoint.getEntry(j) == upperDifference.getEntry(j)) { bdtest = -work1.getEntry(j); } if (bdtest < bdtol) { double curv = modelSecondDerivativesValues.getEntry((j + j * j) / 2); for (int k = 0; k < npt; k++) { final double d1 = interpolationPoints.getEntry(k, j); curv += modelSecondDerivativesParameters.getEntry(k) * (d1 * d1); } bdtest += HALF * curv * rho; if (bdtest < bdtol) { state = 650; break; } } } state = 680; break; } ++ntrits; } case 90: { printState(90); if (dsq <= xoptsq * ONE_OVER_A_THOUSAND) { final double fracsq = xoptsq * ONE_OVER_FOUR; double sumpq = ZERO; for (int k = 0; k < npt; k++) { sumpq += modelSecondDerivativesParameters.getEntry(k); double sum = -HALF * xoptsq; for (int i = 0; i < n; i++) { sum += interpolationPoints.getEntry(k, i) * trustRegionCenterOffset.getEntry(i); } work2.setEntry(k, sum); final double temp = fracsq - HALF * sum; for (int i = 0; i < n; i++) { work1.setEntry(i, bMatrix.getEntry(k, i)); lagrangeValuesAtNewPoint.setEntry(i, sum * interpolationPoints.getEntry(k, i) + temp * trustRegionCenterOffset.getEntry(i)); final int ip = npt + i; for (int j = 0; j <= i; j++) { bMatrix.setEntry(ip, j, bMatrix.getEntry(ip, j) + work1.getEntry(i) * lagrangeValuesAtNewPoint.getEntry(j) + lagrangeValuesAtNewPoint.getEntry(i) * work1.getEntry(j)); } } } for (int m = 0; m < nptm; m++) { double sumz = ZERO; double sumw = ZERO; for (int k = 0; k < npt; k++) { sumz += zMatrix.getEntry(k, m); lagrangeValuesAtNewPoint.setEntry(k, work2.getEntry(k) * zMatrix.getEntry(k, m)); sumw += lagrangeValuesAtNewPoint.getEntry(k); } for (int j = 0; j < n; j++) { double sum = (fracsq * sumz - HALF * sumw) * trustRegionCenterOffset.getEntry(j); for (int k = 0; k < npt; k++) { sum += lagrangeValuesAtNewPoint.getEntry(k) * interpolationPoints.getEntry(k, j); } work1.setEntry(j, sum); for (int k = 0; k < npt; k++) { bMatrix.setEntry(k, j, bMatrix.getEntry(k, j) + sum * zMatrix.getEntry(k, m)); } } for (int i = 0; i < n; i++) { final int ip = i + npt; final double temp = work1.getEntry(i); for (int j = 0; j <= i; j++) { bMatrix.setEntry(ip, j, bMatrix.getEntry(ip, j) + temp * work1.getEntry(j)); } } } int ih = 0; for (int j = 0; j < n; j++) { work1.setEntry(j, -HALF * sumpq * trustRegionCenterOffset.getEntry(j)); for (int k = 0; k < npt; k++) { work1.setEntry(j, work1.getEntry(j) + modelSecondDerivativesParameters.getEntry(k) * interpolationPoints.getEntry(k, j)); interpolationPoints.setEntry(k, j, interpolationPoints.getEntry(k, j) - trustRegionCenterOffset.getEntry(j)); } for (int i = 0; i <= j; i++) { modelSecondDerivativesValues.setEntry(ih, modelSecondDerivativesValues.getEntry(ih) + work1.getEntry(i) * trustRegionCenterOffset.getEntry(j) + trustRegionCenterOffset.getEntry(i) * work1.getEntry(j)); bMatrix.setEntry(npt + i, j, bMatrix.getEntry(npt + j, i)); ih++; } } for (int i = 0; i < n; i++) { originShift.setEntry(i, originShift.getEntry(i) + trustRegionCenterOffset.getEntry(i)); newPoint.setEntry(i, newPoint.getEntry(i) - trustRegionCenterOffset.getEntry(i)); lowerDifference.setEntry(i, lowerDifference.getEntry(i) - trustRegionCenterOffset.getEntry(i)); upperDifference.setEntry(i, upperDifference.getEntry(i) - trustRegionCenterOffset.getEntry(i)); trustRegionCenterOffset.setEntry(i, ZERO); } xoptsq = ZERO; } if (ntrits == 0) { state = 210; break; } state = 230; break; } case 210: { printState(210); final double[] alphaCauchy = altmov(knew, adelt); alpha = alphaCauchy[0]; cauchy = alphaCauchy[1]; for (int i = 0; i < n; i++) { trialStepPoint.setEntry(i, newPoint.getEntry(i) - trustRegionCenterOffset.getEntry(i)); } } case 230: { printState(230); for (int k = 0; k < npt; k++) { double suma = ZERO; double sumb = ZERO; double sum = ZERO; for (int j = 0; j < n; j++) { suma += interpolationPoints.getEntry(k, j) * trialStepPoint.getEntry(j); sumb += interpolationPoints.getEntry(k, j) * trustRegionCenterOffset.getEntry(j); sum += bMatrix.getEntry(k, j) * trialStepPoint.getEntry(j); } work3.setEntry(k, suma * (HALF * suma + sumb)); lagrangeValuesAtNewPoint.setEntry(k, sum); work2.setEntry(k, suma); } beta = ZERO; for (int m = 0; m < nptm; m++) { double sum = ZERO; for (int k = 0; k < npt; k++) { sum += zMatrix.getEntry(k, m) * work3.getEntry(k); } beta -= sum * sum; for (int k = 0; k < npt; k++) { lagrangeValuesAtNewPoint.setEntry(k, lagrangeValuesAtNewPoint.getEntry(k) + sum * zMatrix.getEntry(k, m)); } } dsq = ZERO; double bsum = ZERO; double dx = ZERO; for (int j = 0; j < n; j++) { final double d1 = trialStepPoint.getEntry(j); dsq += d1 * d1; double sum = ZERO; for (int k = 0; k < npt; k++) { sum += work3.getEntry(k) * bMatrix.getEntry(k, j); } bsum += sum * trialStepPoint.getEntry(j); final int jp = npt + j; for (int i = 0; i < n; i++) { sum += bMatrix.getEntry(jp, i) * trialStepPoint.getEntry(i); } lagrangeValuesAtNewPoint.setEntry(jp, sum); bsum += sum * trialStepPoint.getEntry(j); dx += trialStepPoint.getEntry(j) * trustRegionCenterOffset.getEntry(j); } beta = dx * dx + dsq * (xoptsq + dx + dx + HALF * dsq) + beta - bsum; lagrangeValuesAtNewPoint.setEntry(trustRegionCenterInterpolationPointIndex, lagrangeValuesAtNewPoint.getEntry(trustRegionCenterInterpolationPointIndex) + ONE); if (ntrits == 0) { final double d1 = lagrangeValuesAtNewPoint.getEntry(knew); denom = d1 * d1 + alpha * beta; if (denom < cauchy && cauchy > ZERO) { for (int i = 0; i < n; i++) { newPoint.setEntry(i, alternativeNewPoint.getEntry(i)); trialStepPoint.setEntry(i, newPoint.getEntry(i) - trustRegionCenterOffset.getEntry(i)); } cauchy = ZERO; state = 230; break; } } else { final double delsq = delta * delta; scaden = ZERO; biglsq = ZERO; knew = 0; for (int k = 0; k < npt; k++) { if (k == trustRegionCenterInterpolationPointIndex) { continue; } double hdiag = ZERO; for (int m = 0; m < nptm; m++) { final double d1 = zMatrix.getEntry(k, m); hdiag += d1 * d1; } final double d2 = lagrangeValuesAtNewPoint.getEntry(k); final double den = beta * hdiag + d2 * d2; distsq = ZERO; for (int j = 0; j < n; j++) { final double d3 = interpolationPoints.getEntry(k, j) - trustRegionCenterOffset.getEntry(j); distsq += d3 * d3; } final double d4 = distsq / delsq; final double temp = Math.max(ONE, d4 * d4); if (temp * den > scaden) { scaden = temp * den; knew = k; denom = den; } final double d5 = lagrangeValuesAtNewPoint.getEntry(k); biglsq = Math.max(biglsq, temp * (d5 * d5)); } } } case 360: { printState(360); for (int i = 0; i < n; i++) { final double d3 = lowerBound[i]; final double d4 = originShift.getEntry(i) + newPoint.getEntry(i); final double d1 = Math.max(d3, d4); final double d2 = upperBound[i]; currentBest.setEntry(i, Math.min(d1, d2)); if (newPoint.getEntry(i) == lowerDifference.getEntry(i)) { currentBest.setEntry(i, lowerBound[i]); } if (newPoint.getEntry(i) == upperDifference.getEntry(i)) { currentBest.setEntry(i, upperBound[i]); } } f = computeObjectiveValue(currentBest.toArray()); if (!isMinimize) f = -f; if (ntrits == -1) { fsave = f; state = 720; break; } final double fopt = fAtInterpolationPoints.getEntry(trustRegionCenterInterpolationPointIndex); double vquad = ZERO; int ih = 0; for (int j = 0; j < n; j++) { vquad += trialStepPoint.getEntry(j) * gradientAtTrustRegionCenter.getEntry(j); for (int i = 0; i <= j; i++) { double temp = trialStepPoint.getEntry(i) * trialStepPoint.getEntry(j); if (i == j) { temp *= HALF; } vquad += modelSecondDerivativesValues.getEntry(ih) * temp; ih++; } } for (int k = 0; k < npt; k++) { final double d1 = work2.getEntry(k); final double d2 = d1 * d1; vquad += HALF * modelSecondDerivativesParameters.getEntry(k) * d2; } final double diff = f - fopt - vquad; diffc = diffb; diffb = diffa; diffa = Math.abs(diff); if (dnorm > rho) { nfsav = getEvaluations(); } if (ntrits > 0) { if (vquad >= ZERO) { throw new MathIllegalStateException(LocalizedFormats.TRUST_REGION_STEP_FAILED, vquad); } ratio = (f - fopt) / vquad; final double hDelta = HALF * delta; if (ratio <= ONE_OVER_TEN) { delta = Math.min(hDelta, dnorm); } else if (ratio <= .7) { delta = Math.max(hDelta, dnorm); } else { delta = Math.max(hDelta, 2 * dnorm); } if (delta <= rho * 1.5) { delta = rho; } if (f < fopt) { final int ksav = knew; final double densav = denom; final double delsq = delta * delta; scaden = ZERO; biglsq = ZERO; knew = 0; for (int k = 0; k < npt; k++) { double hdiag = ZERO; for (int m = 0; m < nptm; m++) { final double d1 = zMatrix.getEntry(k, m); hdiag += d1 * d1; } final double d1 = lagrangeValuesAtNewPoint.getEntry(k); final double den = beta * hdiag + d1 * d1; distsq = ZERO; for (int j = 0; j < n; j++) { final double d2 = interpolationPoints.getEntry(k, j) - newPoint.getEntry(j); distsq += d2 * d2; } final double d3 = distsq / delsq; final double temp = Math.max(ONE, d3 * d3); if (temp * den > scaden) { scaden = temp * den; knew = k; denom = den; } final double d4 = lagrangeValuesAtNewPoint.getEntry(k); final double d5 = temp * (d4 * d4); biglsq = Math.max(biglsq, d5); } if (scaden <= HALF * biglsq) { knew = ksav; denom = densav; } } } update(beta, denom, knew); ih = 0; final double pqold = modelSecondDerivativesParameters.getEntry(knew); modelSecondDerivativesParameters.setEntry(knew, ZERO); for (int i = 0; i < n; i++) { final double temp = pqold * interpolationPoints.getEntry(knew, i); for (int j = 0; j <= i; j++) { modelSecondDerivativesValues.setEntry(ih, modelSecondDerivativesValues.getEntry(ih) + temp * interpolationPoints.getEntry(knew, j)); ih++; } } for (int m = 0; m < nptm; m++) { final double temp = diff * zMatrix.getEntry(knew, m); for (int k = 0; k < npt; k++) { modelSecondDerivativesParameters.setEntry(k, modelSecondDerivativesParameters.getEntry(k) + temp * zMatrix.getEntry(k, m)); } } fAtInterpolationPoints.setEntry(knew, f); for (int i = 0; i < n; i++) { interpolationPoints.setEntry(knew, i, newPoint.getEntry(i)); work1.setEntry(i, bMatrix.getEntry(knew, i)); } for (int k = 0; k < npt; k++) { double suma = ZERO; for (int m = 0; m < nptm; m++) { suma += zMatrix.getEntry(knew, m) * zMatrix.getEntry(k, m); } double sumb = ZERO; for (int j = 0; j < n; j++) { sumb += interpolationPoints.getEntry(k, j) * trustRegionCenterOffset.getEntry(j); } final double temp = suma * sumb; for (int i = 0; i < n; i++) { work1.setEntry(i, work1.getEntry(i) + temp * interpolationPoints.getEntry(k, i)); } } for (int i = 0; i < n; i++) { gradientAtTrustRegionCenter.setEntry(i, gradientAtTrustRegionCenter.getEntry(i) + diff * work1.getEntry(i)); } if (f < fopt) { trustRegionCenterInterpolationPointIndex = knew; xoptsq = ZERO; ih = 0; for (int j = 0; j < n; j++) { trustRegionCenterOffset.setEntry(j, newPoint.getEntry(j)); final double d1 = trustRegionCenterOffset.getEntry(j); xoptsq += d1 * d1; for (int i = 0; i <= j; i++) { if (i < j) { gradientAtTrustRegionCenter.setEntry(j, gradientAtTrustRegionCenter.getEntry(j) + modelSecondDerivativesValues.getEntry(ih) * trialStepPoint.getEntry(i)); } gradientAtTrustRegionCenter.setEntry(i, gradientAtTrustRegionCenter.getEntry(i) + modelSecondDerivativesValues.getEntry(ih) * trialStepPoint.getEntry(j)); ih++; } } for (int k = 0; k < npt; k++) { double temp = ZERO; for (int j = 0; j < n; j++) { temp += interpolationPoints.getEntry(k, j) * trialStepPoint.getEntry(j); } temp *= modelSecondDerivativesParameters.getEntry(k); for (int i = 0; i < n; i++) { gradientAtTrustRegionCenter.setEntry(i, gradientAtTrustRegionCenter.getEntry(i) + temp * interpolationPoints.getEntry(k, i)); } } } if (ntrits > 0) { for (int k = 0; k < npt; k++) { lagrangeValuesAtNewPoint.setEntry(k, fAtInterpolationPoints.getEntry(k) - fAtInterpolationPoints.getEntry(trustRegionCenterInterpolationPointIndex)); work3.setEntry(k, ZERO); } for (int j = 0; j < nptm; j++) { double sum = ZERO; for (int k = 0; k < npt; k++) { sum += zMatrix.getEntry(k, j) * lagrangeValuesAtNewPoint.getEntry(k); } for (int k = 0; k < npt; k++) { work3.setEntry(k, work3.getEntry(k) + sum * zMatrix.getEntry(k, j)); } } for (int k = 0; k < npt; k++) { double sum = ZERO; for (int j = 0; j < n; j++) { sum += interpolationPoints.getEntry(k, j) * trustRegionCenterOffset.getEntry(j); } work2.setEntry(k, work3.getEntry(k)); work3.setEntry(k, sum * work3.getEntry(k)); } double gqsq = ZERO; double gisq = ZERO; for (int i = 0; i < n; i++) { double sum = ZERO; for (int k = 0; k < npt; k++) { sum += bMatrix.getEntry(k, i) * lagrangeValuesAtNewPoint.getEntry(k) + interpolationPoints.getEntry(k, i) * work3.getEntry(k); } if (trustRegionCenterOffset.getEntry(i) == lowerDifference.getEntry(i)) { final double d1 = Math.min(ZERO, gradientAtTrustRegionCenter.getEntry(i)); gqsq += d1 * d1; final double d2 = Math.min(ZERO, sum); gisq += d2 * d2; } else if (trustRegionCenterOffset.getEntry(i) == upperDifference.getEntry(i)) { final double d1 = Math.max(ZERO, gradientAtTrustRegionCenter.getEntry(i)); gqsq += d1 * d1; final double d2 = Math.max(ZERO, sum); gisq += d2 * d2; } else { final double d1 = gradientAtTrustRegionCenter.getEntry(i); gqsq += d1 * d1; gisq += sum * sum; } lagrangeValuesAtNewPoint.setEntry(npt + i, sum); } ++itest; if (gqsq < TEN * gisq) { itest = 0; } if (itest >= 3) { for (int i = 0, max = Math.max(npt, nh); i < max; i++) { if (i < n) { gradientAtTrustRegionCenter.setEntry(i, lagrangeValuesAtNewPoint.getEntry(npt + i)); } if (i < npt) { modelSecondDerivativesParameters.setEntry(i, work2.getEntry(i)); } if (i < nh) { modelSecondDerivativesValues.setEntry(i, ZERO); } itest = 0; } } } if (ntrits == 0) { state = 60; break; } if (f <= fopt + ONE_OVER_TEN * vquad) { state = 60; break; } final double d1 = TWO * delta; final double d2 = TEN * rho; distsq = Math.max(d1 * d1, d2 * d2); } case 650: { printState(650); knew = -1; for (int k = 0; k < npt; k++) { double sum = ZERO; for (int j = 0; j < n; j++) { final double d1 = interpolationPoints.getEntry(k, j) - trustRegionCenterOffset.getEntry(j); sum += d1 * d1; } if (sum > distsq) { knew = k; distsq = sum; } } if (knew >= 0) { final double dist = Math.sqrt(distsq); if (ntrits == -1) { delta = Math.min(ONE_OVER_TEN * delta, HALF * dist); if (delta <= rho * 1.5) { delta = rho; } } ntrits = 0; final double d1 = Math.min(ONE_OVER_TEN * dist, delta); adelt = Math.max(d1, rho); dsq = adelt * adelt; state = 90; break; } if (ntrits == -1) { state = 680; break; } if (ratio > ZERO) { state = 60; break; } if (Math.max(delta, dnorm) > rho) { state = 60; break; } } case 680: { printState(680); if (rho > stoppingTrustRegionRadius) { delta = HALF * rho; ratio = rho / stoppingTrustRegionRadius; if (ratio <= SIXTEEN) { rho = stoppingTrustRegionRadius; } else if (ratio <= TWO_HUNDRED_FIFTY) { rho = Math.sqrt(ratio) * stoppingTrustRegionRadius; } else { rho *= ONE_OVER_TEN; } delta = Math.max(delta, rho); ntrits = 0; nfsav = getEvaluations(); state = 60; break; } if (ntrits == -1) { state = 360; break; } } case 720: { printState(720); if (fAtInterpolationPoints.getEntry(trustRegionCenterInterpolationPointIndex) <= fsave) { for (int i = 0; i < n; i++) { final double d3 = lowerBound[i]; final double d4 = originShift.getEntry(i) + trustRegionCenterOffset.getEntry(i); final double d1 = Math.max(d3, d4); final double d2 = upperBound[i]; currentBest.setEntry(i, Math.min(d1, d2)); if (trustRegionCenterOffset.getEntry(i) == lowerDifference.getEntry(i)) { currentBest.setEntry(i, lowerBound[i]); } if (trustRegionCenterOffset.getEntry(i) == upperDifference.getEntry(i)) { currentBest.setEntry(i, upperBound[i]); } } f = fAtInterpolationPoints.getEntry(trustRegionCenterInterpolationPointIndex); } return f; } default: { throw new MathIllegalStateException(LocalizedFormats.SIMPLE_MESSAGE, \\\"bobyqb\\\"); } } }\n```\n\n            \n```java\npublic final double[] smooth(final double[] xval, final double[] yval, final double[] weights) throws NonMonotonicSequenceException, DimensionMismatchException, NoDataException, NotFiniteNumberException, NumberIsTooSmallException { if (xval.length != yval.length) { throw new DimensionMismatchException(xval.length, yval.length); } final int n = xval.length; if (n == 0) { throw new NoDataException(); } checkAllFiniteReal(xval); checkAllFiniteReal(yval); checkAllFiniteReal(weights); MathArrays.checkOrder(xval); if (n == 1) { return new double[] { yval[0] }; } if (n == 2) { return new double[] { yval[0], yval[1] }; } int bandwidthInPoints = (int) (bandwidth * n); if (bandwidthInPoints < 2) { throw new NumberIsTooSmallException(LocalizedFormats.BANDWIDTH, bandwidthInPoints, 2, true); } final double[] res = new double[n]; final double[] residuals = new double[n]; final double[] sortedResiduals = new double[n]; final double[] robustnessWeights = new double[n]; Arrays.fill(robustnessWeights, 1); for (int iter = 0; iter <= robustnessIters; ++iter) { final int[] bandwidthInterval = { 0, bandwidthInPoints - 1 }; for (int i = 0; i < n; ++i) { final double x = xval[i]; if (i > 0) { updateBandwidthInterval(xval, weights, i, bandwidthInterval); } final int ileft = bandwidthInterval[0]; final int iright = bandwidthInterval[1]; final int edge; if (xval[i] - xval[ileft] > xval[iright] - xval[i]) { edge = ileft; } else { edge = iright; } double sumWeights = 0; double sumX = 0; double sumXSquared = 0; double sumY = 0; double sumXY = 0; double denom = FastMath.abs(1.0 / (xval[edge] - x)); for (int k = ileft; k <= iright; ++k) { final double xk = xval[k]; final double yk = yval[k]; final double dist = (k < i) ? x - xk : xk - x; final double w = tricube(dist * denom) * robustnessWeights[k] * weights[k]; final double xkw = xk * w; sumWeights += w; sumX += xkw; sumXSquared += xk * xkw; sumY += yk * w; sumXY += yk * xkw; } final double meanX = sumX / sumWeights; final double meanY = sumY / sumWeights; final double meanXY = sumXY / sumWeights; final double meanXSquared = sumXSquared / sumWeights; final double beta; if (FastMath.sqrt(FastMath.abs(meanXSquared - meanX * meanX)) < accuracy) { beta = 0; } else { beta = (meanXY - meanX * meanY) / (meanXSquared - meanX * meanX); } final double alpha = meanY - beta * meanX; res[i] = beta * x + alpha; residuals[i] = FastMath.abs(yval[i] - res[i]); } if (iter == robustnessIters) { break; } System.arraycopy(residuals, 0, sortedResiduals, 0, n); Arrays.sort(sortedResiduals); final double medianResidual = sortedResiduals[n / 2]; if (FastMath.abs(medianResidual) < accuracy) { break; } for (int i = 0; i < n; ++i) { final double arg = residuals[i] / (6 * medianResidual); if (arg >= 1) { robustnessWeights[i] = 0; } else { final double w = 1 - arg * arg; robustnessWeights[i] = w * w; } } } return res; }\n```\n\n            \n```java\n@Override public void integrate(final ExpandableStatefulODE equations, final double t) throws NumberIsTooSmallException, DimensionMismatchException, MaxCountExceededException, NoBracketingException { sanityChecks(equations, t); setEquations(equations); final boolean forward = t > equations.getTime(); final double[] y0 = equations.getCompleteState(); final double[] y = y0.clone(); final double[] yDot = new double[y.length]; final double[] yTmp = new double[y.length]; final double[] predictedScaled = new double[y.length]; Array2DRowRealMatrix nordsieckTmp = null; final NordsieckStepInterpolator interpolator = new NordsieckStepInterpolator(); interpolator.reinitialize(y, forward, equations.getPrimaryMapper(), equations.getSecondaryMappers()); initIntegration(equations.getTime(), y0, t); start(equations.getTime(), y, t); interpolator.reinitialize(stepStart, stepSize, scaled, nordsieck); interpolator.storeTime(stepStart); double hNew = stepSize; interpolator.rescale(hNew); isLastStep = false; do { double error = 10; while (error >= 1.0) { stepSize = hNew; final double stepEnd = stepStart + stepSize; interpolator.setInterpolatedTime(stepEnd); System.arraycopy(interpolator.getInterpolatedState(), 0, yTmp, 0, y0.length); computeDerivatives(stepEnd, yTmp, yDot); for (int j = 0; j < y0.length; ++j) { predictedScaled[j] = stepSize * yDot[j]; } nordsieckTmp = updateHighOrderDerivativesPhase1(nordsieck); updateHighOrderDerivativesPhase2(scaled, predictedScaled, nordsieckTmp); error = nordsieckTmp.walkInOptimizedOrder(new Corrector(y, predictedScaled, yTmp)); if (error >= 1.0) { final double factor = computeStepGrowShrinkFactor(error); hNew = filterStep(stepSize * factor, forward, false); interpolator.rescale(hNew); } } final double stepEnd = stepStart + stepSize; computeDerivatives(stepEnd, yTmp, yDot); final double[] correctedScaled = new double[y0.length]; for (int j = 0; j < y0.length; ++j) { correctedScaled[j] = stepSize * yDot[j]; } updateHighOrderDerivativesPhase2(predictedScaled, correctedScaled, nordsieckTmp); System.arraycopy(yTmp, 0, y, 0, y.length); interpolator.reinitialize(stepEnd, stepSize, correctedScaled, nordsieckTmp); interpolator.storeTime(stepStart); interpolator.shift(); interpolator.storeTime(stepEnd); stepStart = acceptStep(interpolator, y, yDot, t); scaled = correctedScaled; nordsieck = nordsieckTmp; if (!isLastStep) { interpolator.storeTime(stepStart); if (resetOccurred) { start(stepStart, y, t); interpolator.reinitialize(stepStart, stepSize, scaled, nordsieck); } final double factor = computeStepGrowShrinkFactor(error); final double scaledH = stepSize * factor; final double nextT = stepStart + scaledH; final boolean nextIsLast = forward ? (nextT >= t) : (nextT <= t); hNew = filterStep(scaledH, forward, nextIsLast); final double filteredNextT = stepStart + hNew; final boolean filteredNextIsLast = forward ? (filteredNextT >= t) : (filteredNextT <= t); if (filteredNextIsLast) { hNew = t - stepStart; } interpolator.rescale(hNew); } } while (!isLastStep); equations.setTime(stepStart); equations.setCompleteState(y); resetInternalState(); }\n```\n\n---\n\n## Your Goal\n\nFix the buggy method. Return only the fixed Java method. Do not include explanations or other text.\n",
                        "patches": [
                            "\n@Override\nprotected PointVectorValuePair doOptimize() {\n    checkParameters();\n\n    final int nR = getTarget().length; // Number of observed data.\n    final double[] startPoint = getStartPoint();\n    final int nC = startPoint.length; // Number of parameters.\n\n    // arrays shared with the other private methods\n    solvedCols = FastMath.min(nR, nC);\n    diagR = new double[nC];\n    jacNorm = new double[nC];\n    beta = new double[nC];\n    permutation = new int[nC];\n    lmDir = new double[nC];\n\n    // local point\n    double delta = 0;\n    double xNorm = 0;\n    double[] diag = new double[nC];\n    double[] oldX = new double[nC];\n    double[] oldRes = new double[nR];\n    double[] oldObj = new double[nR];\n    double[] qtf = new double[nR];\n    double[] work1 = new double[nC];\n    double[] work2 = new double[nC];\n    double[] work3 = new double[nC];\n\n    final RealMatrix weightMatrixSqrt = getWeightSquareRoot();\n\n    // Evaluate the function at the starting point and calculate its norm.\n    double[] currentObjective = computeObjectiveValue(startPoint);\n    double[] currentResiduals = computeResiduals(currentObjective);\n    PointVectorValuePair current = new PointVectorValuePair(startPoint, currentObjective);\n    double currentCost = computeCost(currentResiduals);\n\n    // Outer loop.\n    lmPar = 0;\n    boolean firstIteration = true;\n    int iter = 0;\n    final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n    while (true) {\n        ++iter;\n        final PointVectorValuePair previous = current;\n\n        // QR decomposition of the jacobian matrix\n        qrDecomposition(computeWeightedJacobian(current.getPoint()));\n\n        weightedResidual = weightMatrixSqrt.operate(currentResiduals);\n        System.arraycopy(weightedResidual, 0, qtf, 0, nR);\n\n        // compute Qt.res\n        qTy(qtf);\n\n        // now we don't need Q anymore,\n        // so let jacobian contain the R matrix with its diagonal elements\n        for (int k = 0; k < solvedCols; ++k) {\n            int pk = permutation[k];\n            weightedJacobian[k][pk] = diagR[pk];\n        }\n\n        if (firstIteration) {\n            // scale the point according to the norms of the columns\n            // of the initial jacobian\n            xNorm = 0;\n            for (int k = 0; k < nC; ++k) {\n                double dk = jacNorm[k];\n                if (dk == 0) {\n                    dk = 1.0;\n                }\n                double xk = dk * startPoint[k];\n                xNorm = FastMath.hypot(xNorm, xk);\n                diag[k] = dk;\n            }\n\n            // initialize the step bound delta\n            delta = (xNorm == 0) ? initialStepBoundFactor : (initialStepBoundFactor * xNorm);\n        }\n\n        // check orthogonality between function vector and jacobian columns\n        double maxCosine = 0;\n        if (currentCost != 0) {\n            for (int j = 0; j < solvedCols; ++j) {\n                int pj = permutation[j];\n                double s = jacNorm[pj];\n                if (s != 0) {\n                    double sum = 0;\n                    for (int i = 0; i <= j; ++i) {\n                        sum += weightedJacobian[i][pj] * qtf[i];\n                    }\n                    maxCosine = FastMath.max(maxCosine, FastMath.abs(sum) / (s * currentCost));\n                }\n            }\n        }\n        if (maxCosine <= orthoTolerance) {\n            // Convergence has been reached.\n            setCost(currentCost);\n            return current;\n        }\n\n        // rescale if necessary\n        for (int j = 0; j < nC; ++j) {\n            diag[j] = FastMath.max(diag[j], jacNorm[j]);\n        }\n\n        // Inner loop.\n        for (double ratio = 0; ratio < 1.0e-4;) {\n\n            // save the state\n            System.arraycopy(startPoint, 0, oldX, 0, nC);\n\n            final double previousCost = currentCost;\n            double[] tmpVec = weightedResidual;\n            weightedResidual = oldRes;\n            oldRes = tmpVec;\n            tmpVec = currentObjective;\n            currentObjective = oldObj;\n            oldObj = tmpVec;\n\n            // determine the Levenberg-Marquardt parameter\n            determineLMParameter(qtf, delta, diag, work1, work2, work3);\n\n            // compute the new point and the norm of the evolution direction\n            double lmNorm = 0;\n            for (int j = 0; j < solvedCols; ++j) {\n                int pj = permutation[j];\n                lmDir[pj] = -lmDir[pj];\n                startPoint[pj] = oldX[pj] + lmDir[pj];\n                double s = diag[pj] * lmDir[pj];\n                lmNorm = FastMath.hypot(lmNorm, s);\n            }\n            \n            // on the first iteration, adjust the initial step bound.\n            if (firstIteration) {\n                delta = FastMath.min(delta, lmNorm);\n            }\n\n            // Evaluate the function at x + p and calculate its norm.\n            currentObjective = computeObjectiveValue(startPoint);\n            currentResiduals = computeResiduals(currentObjective);\n            current = new PointVectorValuePair(startPoint, currentObjective);\n            currentCost = computeCost(currentResiduals);\n\n            // compute the scaled actual reduction\n            double actRed = -1.0;\n            if (0.1 * currentCost < previousCost) {\n                double r = currentCost / previousCost;\n                actRed = 1.0 - r * r;\n            }\n\n            // compute the scaled predicted reduction\n            // and the scaled directional derivative\n            for (int j = 0; j < solvedCols; ++j) {\n                int pj = permutation[j];\n                double dirJ = lmDir[pj];\n                work1[j] = 0;\n                for (int i = 0; i <= j; ++i) {\n                    work1[i] += weightedJacobian[i][pj] * dirJ;\n                }\n            }\n            double coeff1 = 0;\n            for (int j = 0; j < solvedCols; ++j) {\n                coeff1 += work1[j] * work1[j];\n            }\n            double pc2 = previousCost * previousCost;\n            coeff1 = coeff1 / pc2;\n            double coeff2 = lmPar * lmNorm * lmNorm / pc2;\n            double preRed = coeff1 + 2 * coeff2;\n            double dirDer = -(coeff1 + coeff2);\n\n            // ratio of the actual to the predicted reduction\n            ratio = (preRed == 0) ? 0 : (actRed / preRed);\n\n            // update the step bound\n            if (ratio <= 0.25) {\n                double tmp = (actRed < 0) ? (0.5 * dirDer / (dirDer + 0.5 * actRed)) : 0.5;\n                if ((0.1 * currentCost >= previousCost) || (tmp < 0.1)) {\n                    tmp = 0.1;\n                }\n                delta = tmp * FastMath.min(delta, 10.0 * lmNorm);\n                lmPar /= tmp;\n            } else if ((lmPar == 0) || (ratio >= 0.75)) {\n                delta = 2 * lmNorm;\n                lmPar *= 0.5;\n            }\n\n            // test for successful iteration.\n            if (ratio >= 1.0e-4) {\n                // successful iteration, update the norm\n                firstIteration = false;\n                xNorm = 0;\n                for (int k = 0; k < nC; ++k) {\n                    double xK = diag[k] * startPoint[k];\n                    xNorm = FastMath.hypot(xNorm, xK);\n                }\n\n                // tests for convergence.\n                if (checker != null) {\n                    // we use the vectorial convergence checker\n                    if (checker.converged(iter, previous, current)) {\n                        setCost(currentCost);\n                        return current;\n                    }\n                }\n            } else {\n                // failed iteration, reset the previous values\n                currentCost = previousCost;\n                System.arraycopy(oldX, 0, startPoint, 0, nC);\n                tmpVec = weightedResidual;\n                weightedResidual = oldRes;\n                oldRes = tmpVec;\n                tmpVec = currentObjective;\n                currentObjective = oldObj;\n                oldObj = tmpVec;\n                // Reset \"current\" to previous values.\n                current = new PointVectorValuePair(startPoint, currentObjective);\n            }\n\n            // Default convergence criteria.\n            if ((FastMath.abs(actRed) <= costRelativeTolerance &&\n                    preRed <= costRelativeTolerance &&\n                    ratio <= 2.0) ||\n                delta <= parRelativeTolerance * xNorm) {\n                setCost(currentCost);\n                return current;\n            }\n\n            // tests for termination and stringent tolerances\n            // (2.2204e-16 is the machine epsilon for IEEE754)\n            if ((FastMath.abs(actRed) <= 2.2204e-16) && (preRed <= 2.2204e-16) && (ratio <= 2.0)) {\n                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_COST_RELATIVE_TOLERANCE,\n                        costRelativeTolerance);\n            } else if (delta <= 2.2204e-16 * xNorm) {\n                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE,\n                        parRelativeTolerance);\n            } else if (maxCosine <= 2.2204e-16) {\n                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_ORTHOGONALITY_TOLERANCE,\n                        orthoTolerance);\n            }\n        }\n    }\n}\n",
                            "\n@Override\nprotected PointVectorValuePair doOptimize() {\n    checkParameters();\n\n    final int nR = getTarget().length; // Number of observed data.\n    final double[] currentPoint = getStartPoint();\n    final int nC = currentPoint.length; // Number of parameters.\n\n    // arrays shared with the other private methods\n    solvedCols  = FastMath.min(nR, nC);\n    diagR       = new double[nC];\n    jacNorm     = new double[nC];\n    beta        = new double[nC];\n    permutation = new int[nC];\n    lmDir       = new double[nC];\n\n    // local point\n    double   delta   = 0;\n    double   xNorm   = 0;\n    double[] diag    = new double[nC];\n    double[] oldX    = new double[nC];\n    double[] oldRes  = new double[nR];\n    double[] oldObj  = new double[nR];\n    double[] qtf     = new double[nR];\n    double[] work1   = new double[nC];\n    double[] work2   = new double[nC];\n    double[] work3   = new double[nC];\n\n    final RealMatrix weightMatrixSqrt = getWeightSquareRoot();\n\n    // Evaluate the function at the starting point and calculate its norm.\n    double[] currentObjective = computeObjectiveValue(currentPoint);\n    double[] currentResiduals = computeResiduals(currentObjective);\n    PointVectorValuePair current = new PointVectorValuePair(currentPoint.clone(), currentObjective);\n    double currentCost = computeCost(currentResiduals);\n\n    // Outer loop.\n    lmPar = 0;\n    boolean firstIteration = true;\n    int iter = 0;\n    final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n    while (true) {\n        ++iter;\n        final PointVectorValuePair previous = current;\n\n        // QR decomposition of the jacobian matrix\n        qrDecomposition(computeWeightedJacobian(currentPoint));\n\n        weightedResidual = weightMatrixSqrt.operate(currentResiduals);\n        for (int i = 0; i < nR; i++) {\n            qtf[i] = weightedResidual[i];\n        }\n\n        // compute Qt.res\n        qTy(qtf);\n\n        // now we don't need Q anymore,\n        // so let jacobian contain the R matrix with its diagonal elements\n        for (int k = 0; k < solvedCols; ++k) {\n            int pk = permutation[k];\n            weightedJacobian[k][pk] = diagR[pk];\n        }\n\n        if (firstIteration) {\n            // scale the point according to the norms of the columns\n            // of the initial jacobian\n            xNorm = 0;\n            for (int k = 0; k < nC; ++k) {\n                double dk = jacNorm[k];\n                if (dk == 0) {\n                    dk = 1.0;\n                }\n                double xk = dk * currentPoint[k];\n                xNorm  += xk * xk;\n                diag[k] = dk;\n            }\n            xNorm = FastMath.sqrt(xNorm);\n\n            // initialize the step bound delta\n            delta = (xNorm == 0) ? initialStepBoundFactor : (initialStepBoundFactor * xNorm);\n        }\n\n        // check orthogonality between function vector and jacobian columns\n        double maxCosine = 0;\n        if (currentCost != 0) {\n            for (int j = 0; j < solvedCols; ++j) {\n                int    pj = permutation[j];\n                double s  = jacNorm[pj];\n                if (s != 0) {\n                    double sum = 0;\n                    for (int i = 0; i <= j; ++i) {\n                        sum += weightedJacobian[i][pj] * qtf[i];\n                    }\n                    maxCosine = FastMath.max(maxCosine, FastMath.abs(sum) / (s * currentCost));\n                }\n            }\n        }\n        if (maxCosine <= orthoTolerance) {\n            // Convergence has been reached.\n            setCost(currentCost);\n            point = current.getPoint();\n            return current;\n        }\n\n        // rescale if necessary\n        for (int j = 0; j < nC; ++j) {\n            diag[j] = FastMath.max(diag[j], jacNorm[j]);\n        }\n\n        // Inner loop.\n        for (double ratio = 0; ratio < 1.0e-4;) {\n\n            // save the state\n            for (int j = 0; j < solvedCols; ++j) {\n                int pj = permutation[j];\n                oldX[pj] = currentPoint[pj];\n            }\n            final double previousCost = currentCost;\n            double[] tmpVec = weightedResidual;\n            weightedResidual = oldRes;\n            oldRes    = tmpVec;\n            tmpVec    = currentObjective;\n            currentObjective = oldObj;\n            oldObj    = tmpVec;\n\n            // determine the Levenberg-Marquardt parameter\n            determineLMParameter(qtf, delta, diag, work1, work2, work3);\n\n            // compute the new point and the norm of the evolution direction\n            double lmNorm = 0;\n            for (int j = 0; j < solvedCols; ++j) {\n                int pj = permutation[j];\n                lmDir[pj] = -lmDir[pj];\n                currentPoint[pj] = oldX[pj] + lmDir[pj];\n                double s = diag[pj] * lmDir[pj];\n                lmNorm  += s * s;\n            }\n            lmNorm = FastMath.sqrt(lmNorm);\n            // on the first iteration, adjust the initial step bound.\n            if (firstIteration) {\n                delta = FastMath.min(delta, lmNorm);\n            }\n\n            // Evaluate the function at x + p and calculate its norm.\n            currentObjective = computeObjectiveValue(currentPoint);\n            currentResiduals = computeResiduals(currentObjective);\n            current = new PointVectorValuePair(currentPoint.clone(), currentObjective);\n            currentCost = computeCost(currentResiduals);\n\n            // compute the scaled actual reduction\n            double actRed = -1.0;\n            if (0.1 * currentCost < previousCost) {\n                double r = currentCost / previousCost;\n                actRed = 1.0 - r * r;\n            }\n\n            // compute the scaled predicted reduction\n            // and the scaled directional derivative\n            for (int j = 0; j < solvedCols; ++j) {\n                int pj = permutation[j];\n                double dirJ = lmDir[pj];\n                work1[j] = 0;\n                for (int i = 0; i <= j; ++i) {\n                    work1[i] += weightedJacobian[i][pj] * dirJ;\n                }\n            }\n            double coeff1 = 0;\n            for (int j = 0; j < solvedCols; ++j) {\n                coeff1 += work1[j] * work1[j];\n            }\n            double pc2 = previousCost * previousCost;\n            coeff1 = coeff1 / pc2;\n            double coeff2 = lmPar * lmNorm * lmNorm / pc2;\n            double preRed = coeff1 + 2 * coeff2;\n            double dirDer = -(coeff1 + coeff2);\n\n            // ratio of the actual to the predicted reduction\n            ratio = (preRed == 0) ? 0 : (actRed / preRed);\n\n            // update the step bound\n            if (ratio <= 0.25) {\n                double tmp =\n                    (actRed < 0) ? (0.5 * dirDer / (dirDer + 0.5 * actRed)) : 0.5;\n                    if ((0.1 * currentCost >= previousCost) || (tmp < 0.1)) {\n                        tmp = 0.1;\n                    }\n                    delta = tmp * FastMath.min(delta, 10.0 * lmNorm);\n                    lmPar /= tmp;\n            } else if ((lmPar == 0) || (ratio >= 0.75)) {\n                delta = 2 * lmNorm;\n                lmPar *= 0.5;\n            }\n\n            // test for successful iteration.\n            if (ratio >= 1.0e-4) {\n                // successful iteration, update the norm\n                firstIteration = false;\n                xNorm = 0;\n                for (int k = 0; k < nC; ++k) {\n                    double xK = diag[k] * currentPoint[k];\n                    xNorm += xK * xK;\n                }\n                xNorm = FastMath.sqrt(xNorm);\n\n                // tests for convergence.\n                if (checker != null) {\n                    // we use the vectorial convergence checker\n                    if (checker.converged(iter, previous, current)) {\n                        setCost(currentCost);\n                        point = current.getPoint();\n                        return current;\n                    }\n                }\n            } else {\n                // failed iteration, reset the previous values\n                currentCost = previousCost;\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    currentPoint[pj] = oldX[pj];\n                }\n                tmpVec    = weightedResidual;\n                weightedResidual = oldRes;\n                oldRes    = tmpVec;\n                tmpVec    = currentObjective;\n                currentObjective = oldObj;\n                oldObj    = tmpVec;\n                // Reset \"current\" to previous values.\n                current = new PointVectorValuePair(currentPoint.clone(), currentObjective);\n            }\n\n            // Default convergence criteria.\n            if ((FastMath.abs(actRed) <= costRelativeTolerance &&\n                 preRed <= costRelativeTolerance &&\n                 ratio <= 2.0) ||\n                delta <= parRelativeTolerance * xNorm) {\n                setCost(currentCost);\n                point = current.getPoint();\n                return current;\n            }\n\n            // tests for termination and stringent tolerances\n            // (2.2204e-16 is the machine epsilon for IEEE754)\n            if ((FastMath.abs(actRed) <= 2.2204e-16) && (preRed <= 2.2204e-16) && (ratio <= 2.0)) {\n                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_COST_RELATIVE_TOLERANCE,\n                                               costRelativeTolerance);\n            } else if (delta <= 2.2204e-16 * xNorm) {\n                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE,\n                                               parRelativeTolerance);\n            } else if (maxCosine <= 2.2204e-16)  {\n                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_ORTHOGONALITY_TOLERANCE,\n                                               orthoTolerance);\n            }\n        }\n    }\n}\n",
                            "\n@Override\nprotected PointVectorValuePair doOptimize() {\n    checkParameters();\n\n    final int nR = getTarget().length; // Number of observed data.\n    final double[] currentPoint = getStartPoint();\n    final int nC = currentPoint.length; // Number of parameters.\n\n    // arrays shared with the other private methods\n    solvedCols  = FastMath.min(nR, nC);\n    diagR       = new double[nC];\n    jacNorm     = new double[nC];\n    beta        = new double[nC];\n    permutation = new int[nC];\n    lmDir       = new double[nC];\n\n    // local point\n    double   delta   = 0;\n    double   xNorm   = 0;\n    double[] diag    = new double[nC];\n    double[] oldX    = new double[nC];\n    double[] oldRes  = new double[nR];\n    double[] oldObj  = new double[nR];\n    double[] qtf     = new double[nR];\n    double[] work1   = new double[nC];\n    double[] work2   = new double[nC];\n    double[] work3   = new double[nC];\n\n    final RealMatrix weightMatrixSqrt = getWeightSquareRoot();\n\n    // Evaluate the function at the starting point and calculate its norm.\n    double[] currentObjective = computeObjectiveValue(currentPoint);\n    double[] currentResiduals = computeResiduals(currentObjective);\n    PointVectorValuePair current = new PointVectorValuePair(currentPoint, currentObjective);\n    double currentCost = computeCost(currentResiduals);\n\n    // Outer loop.\n    lmPar = 0;\n    boolean firstIteration = true;\n    int iter = 0;\n    final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n    while (true) {\n        ++iter;\n        final PointVectorValuePair previous = current;\n\n        // QR decomposition of the jacobian matrix\n        qrDecomposition(computeWeightedJacobian(currentPoint));\n\n        weightedResidual = weightMatrixSqrt.operate(currentResiduals);\n        for (int i = 0; i < nR; i++) {\n            qtf[i] = weightedResidual[i];\n        }\n\n        // compute Qt.res\n        qTy(qtf);\n\n        // now we don't need Q anymore,\n        // so let jacobian contain the R matrix with its diagonal elements\n        for (int k = 0; k < solvedCols; ++k) {\n            int pk = permutation[k];\n            weightedJacobian[k][pk] = diagR[pk];\n        }\n\n        if (firstIteration) {\n            // scale the point according to the norms of the columns\n            // of the initial jacobian\n            xNorm = 0;\n            for (int k = 0; k < nC; ++k) {\n                double dk = jacNorm[k];\n                if (dk == 0) {\n                    dk = 1.0;\n                }\n                double xk = dk * currentPoint[k];\n                xNorm  += xk * xk;\n                diag[k] = dk;\n            }\n            xNorm = FastMath.sqrt(xNorm);\n\n            // initialize the step bound delta\n            delta = (xNorm == 0) ? initialStepBoundFactor : (initialStepBoundFactor * xNorm);\n        }\n\n        // check orthogonality between function vector and jacobian columns\n        double maxCosine = 0;\n        if (currentCost != 0) {\n            for (int j = 0; j < solvedCols; ++j) {\n                int    pj = permutation[j];\n                double s  = jacNorm[pj];\n                if (s != 0) {\n                    double sum = 0;\n                    for (int i = 0; i <= j; ++i) {\n                        sum += weightedJacobian[i][pj] * qtf[i];\n                    }\n                    maxCosine = FastMath.max(maxCosine, FastMath.abs(sum) / (s * currentCost));\n                }\n            }\n        }\n        if (maxCosine <= orthoTolerance) {\n            // Convergence has been reached.\n            setCost(currentCost);\n            point = current.getPoint(); // Ensure the optimized point is set\n            return current;\n        }\n\n        // rescale if necessary\n        for (int j = 0; j < nC; ++j) {\n            diag[j] = FastMath.max(diag[j], jacNorm[j]);\n        }\n\n        // Inner loop.\n        for (double ratio = 0; ratio < 1.0e-4;) {\n\n            // save the state\n            for (int j = 0; j < solvedCols; ++j) {\n                int pj = permutation[j];\n                oldX[pj] = currentPoint[pj];\n            }\n            final double previousCost = currentCost;\n            double[] tmpVec = weightedResidual;\n            weightedResidual = oldRes;\n            oldRes    = tmpVec;\n            tmpVec    = currentObjective;\n            currentObjective = oldObj;\n            oldObj    = tmpVec;\n\n            // determine the Levenberg-Marquardt parameter\n            determineLMParameter(qtf, delta, diag, work1, work2, work3);\n\n            // compute the new point and the norm of the evolution direction\n            double lmNorm = 0;\n            for (int j = 0; j < solvedCols; ++j) {\n                int pj = permutation[j];\n                lmDir[pj] = -lmDir[pj];\n                currentPoint[pj] = oldX[pj] + lmDir[pj];\n                double s = diag[pj] * lmDir[pj];\n                lmNorm  += s * s;\n            }\n            lmNorm = FastMath.sqrt(lmNorm);\n            // on the first iteration, adjust the initial step bound.\n            if (firstIteration) {\n                delta = FastMath.min(delta, lmNorm);\n            }\n\n            // Evaluate the function at x + p and calculate its norm.\n            currentObjective = computeObjectiveValue(currentPoint);\n            currentResiduals = computeResiduals(currentObjective);\n            current = new PointVectorValuePair(currentPoint, currentObjective);\n            currentCost = computeCost(currentResiduals);\n\n            // compute the scaled actual reduction\n            double actRed = -1.0;\n            if (0.1 * currentCost < previousCost) {\n                double r = currentCost / previousCost;\n                actRed = 1.0 - r * r;\n            }\n\n            // compute the scaled predicted reduction\n            // and the scaled directional derivative\n            for (int j = 0; j < solvedCols; ++j) {\n                int pj = permutation[j];\n                double dirJ = lmDir[pj];\n                work1[j] = 0;\n                for (int i = 0; i <= j; ++i) {\n                    work1[i] += weightedJacobian[i][pj] * dirJ;\n                }\n            }\n            double coeff1 = 0;\n            for (int j = 0; j < solvedCols; ++j) {\n                coeff1 += work1[j] * work1[j];\n            }\n            double pc2 = previousCost * previousCost;\n            coeff1 = coeff1 / pc2;\n            double coeff2 = lmPar * lmNorm * lmNorm / pc2;\n            double preRed = coeff1 + 2 * coeff2;\n            double dirDer = -(coeff1 + coeff2);\n\n            // ratio of the actual to the predicted reduction\n            ratio = (preRed == 0) ? 0 : (actRed / preRed);\n\n            // update the step bound\n            if (ratio <= 0.25) {\n                double tmp =\n                    (actRed < 0) ? (0.5 * dirDer / (dirDer + 0.5 * actRed)) : 0.5;\n                    if ((0.1 * currentCost >= previousCost) || (tmp < 0.1)) {\n                        tmp = 0.1;\n                    }\n                    delta = tmp * FastMath.min(delta, 10.0 * lmNorm);\n                    lmPar /= tmp;\n            } else if ((lmPar == 0) || (ratio >= 0.75)) {\n                delta = 2 * lmNorm;\n                lmPar *= 0.5;\n            }\n\n            // test for successful iteration.\n            if (ratio >= 1.0e-4) {\n                // successful iteration, update the norm\n                firstIteration = false;\n                xNorm = 0;\n                for (int k = 0; k < nC; ++k) {\n                    double xK = diag[k] * currentPoint[k];\n                    xNorm += xK * xK;\n                }\n                xNorm = FastMath.sqrt(xNorm);\n\n                // tests for convergence.\n                if (checker != null) {\n                    // we use the vectorial convergence checker\n                    if (checker.converged(iter, previous, current)) {\n                        setCost(currentCost);\n                        point = current.getPoint(); // Ensure the optimized point is set\n                        return current;\n                    }\n                }\n            } else {\n                // failed iteration, reset the previous values\n                currentCost = previousCost;\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    currentPoint[pj] = oldX[pj];\n                }\n                tmpVec    = weightedResidual;\n                weightedResidual = oldRes;\n                oldRes    = tmpVec;\n                tmpVec    = currentObjective;\n                currentObjective = oldObj;\n                oldObj    = tmpVec;\n                // Reset \"current\" to previous values.\n                current = new PointVectorValuePair(currentPoint, currentObjective);\n            }\n\n            // Default convergence criteria.\n            if ((FastMath.abs(actRed) <= costRelativeTolerance &&\n                 preRed <= costRelativeTolerance &&\n                 ratio <= 2.0) ||\n                delta <= parRelativeTolerance * xNorm) {\n                setCost(currentCost);\n                point = current.getPoint(); // Ensure the optimized point is set\n                return current;\n            }\n\n            // tests for termination and stringent tolerances\n            // (2.2204e-16 is the machine epsilon for IEEE754)\n            if ((FastMath.abs(actRed) <= 2.2204e-16) && (preRed <= 2.2204e-16) && (ratio <= 2.0)) {\n                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_COST_RELATIVE_TOLERANCE,\n                                               costRelativeTolerance);\n            } else if (delta <= 2.2204e-16 * xNorm) {\n                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE,\n                                               parRelativeTolerance);\n            } else if (maxCosine <= 2.2204e-16)  {\n                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_ORTHOGONALITY_TOLERANCE,\n                                               orthoTolerance);\n            }\n        }\n    }\n}\n",
                            "\n@Override\nprotected PointVectorValuePair doOptimize() {\n    checkParameters();\n\n    final int nR = getTarget().length; // Number of observed data.\n    final double[] currentPoint = getStartPoint();\n    final int nC = currentPoint.length; // Number of parameters.\n\n    // arrays shared with the other private methods\n    solvedCols = FastMath.min(nR, nC);\n    diagR = new double[nC];\n    jacNorm = new double[nC];\n    beta = new double[nC];\n    permutation = new int[nC];\n    lmDir = new double[nC];\n\n    // local point\n    double delta = 0;\n    double xNorm = 0;\n    double[] diag = new double[nC];\n    double[] oldX = new double[nC];\n    double[] oldRes = new double[nR];\n    double[] oldObj = new double[nR];\n    double[] qtf = new double[nR];\n    double[] work1 = new double[nC];\n    double[] work2 = new double[nC];\n    double[] work3 = new double[nC];\n\n    final RealMatrix weightMatrixSqrt = getWeightSquareRoot();\n\n    // Evaluate the function at the starting point and calculate its norm.\n    double[] currentObjective = computeObjectiveValue(currentPoint);\n    double[] currentResiduals = computeResiduals(currentObjective);\n    PointVectorValuePair current = new PointVectorValuePair(currentPoint.clone(), currentObjective);\n    double currentCost = computeCost(currentResiduals);\n\n    // Outer loop.\n    lmPar = 0;\n    boolean firstIteration = true;\n    int iter = 0;\n    final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n    while (true) {\n        ++iter;\n        final PointVectorValuePair previous = current;\n\n        // QR decomposition of the jacobian matrix\n        qrDecomposition(computeWeightedJacobian(currentPoint));\n\n        weightedResidual = weightMatrixSqrt.operate(currentResiduals);\n        for (int i = 0; i < nR; i++) {\n            qtf[i] = weightedResidual[i];\n        }\n\n        // compute Qt.res\n        qTy(qtf);\n\n        // now we don't need Q anymore,\n        // so let jacobian contain the R matrix with its diagonal elements\n        for (int k = 0; k < solvedCols; ++k) {\n            int pk = permutation[k];\n            weightedJacobian[k][pk] = diagR[pk];\n        }\n\n        if (firstIteration) {\n            // scale the point according to the norms of the columns\n            // of the initial jacobian\n            xNorm = 0;\n            for (int k = 0; k < nC; ++k) {\n                double dk = jacNorm[k];\n                if (dk == 0) {\n                    dk = 1.0;\n                }\n                double xk = dk * currentPoint[k];\n                xNorm += xk * xk;\n                diag[k] = dk;\n            }\n            xNorm = FastMath.sqrt(xNorm);\n\n            // initialize the step bound delta\n            delta = (xNorm == 0) ? initialStepBoundFactor : (initialStepBoundFactor * xNorm);\n        }\n\n        // check orthogonality between function vector and jacobian columns\n        double maxCosine = 0;\n        if (currentCost != 0) {\n            for (int j = 0; j < solvedCols; ++j) {\n                int pj = permutation[j];\n                double s = jacNorm[pj];\n                if (s != 0) {\n                    double sum = 0;\n                    for (int i = 0; i <= j; ++i) {\n                        sum += weightedJacobian[i][pj] * qtf[i];\n                    }\n                    maxCosine = FastMath.max(maxCosine, FastMath.abs(sum) / (s * currentCost));\n                }\n            }\n        }\n        if (maxCosine <= orthoTolerance) {\n            // Convergence has been reached.\n            setCost(currentCost);\n            return current;\n        }\n\n        // rescale if necessary\n        for (int j = 0; j < nC; ++j) {\n            diag[j] = FastMath.max(diag[j], jacNorm[j]);\n        }\n\n        // Inner loop.\n        for (double ratio = 0; ratio < 1.0e-4;) {\n\n            // save the state\n            for (int j = 0; j < solvedCols; ++j) {\n                int pj = permutation[j];\n                oldX[pj] = currentPoint[pj];\n            }\n            final double previousCost = currentCost;\n            double[] tmpVec = weightedResidual;\n            weightedResidual = oldRes;\n            oldRes = tmpVec;\n            tmpVec = currentObjective;\n            currentObjective = oldObj;\n            oldObj = tmpVec;\n\n            // determine the Levenberg-Marquardt parameter\n            determineLMParameter(qtf, delta, diag, work1, work2, work3);\n\n            // compute the new point and the norm of the evolution direction\n            double lmNorm = 0;\n            for (int j = 0; j < solvedCols; ++j) {\n                int pj = permutation[j];\n                lmDir[pj] = -lmDir[pj];\n                currentPoint[pj] = oldX[pj] + lmDir[pj];\n                double s = diag[pj] * lmDir[pj];\n                lmNorm += s * s;\n            }\n            lmNorm = FastMath.sqrt(lmNorm);\n            // on the first iteration, adjust the initial step bound.\n            if (firstIteration) {\n                delta = FastMath.min(delta, lmNorm);\n            }\n\n            // Evaluate the function at x + p and calculate its norm.\n            currentObjective = computeObjectiveValue(currentPoint);\n            currentResiduals = computeResiduals(currentObjective);\n            current = new PointVectorValuePair(currentPoint.clone(), currentObjective);\n            currentCost = computeCost(currentResiduals);\n\n            // compute the scaled actual reduction\n            double actRed = -1.0;\n            if (0.1 * currentCost < previousCost) {\n                double r = currentCost / previousCost;\n                actRed = 1.0 - r * r;\n            }\n\n            // compute the scaled predicted reduction\n            // and the scaled directional derivative\n            for (int j = 0; j < solvedCols; ++j) {\n                int pj = permutation[j];\n                double dirJ = lmDir[pj];\n                work1[j] = 0;\n                for (int i = 0; i <= j; ++i) {\n                    work1[i] += weightedJacobian[i][pj] * dirJ;\n                }\n            }\n            double coeff1 = 0;\n            for (int j = 0; j < solvedCols; ++j) {\n                coeff1 += work1[j] * work1[j];\n            }\n            double pc2 = previousCost * previousCost;\n            coeff1 = coeff1 / pc2;\n            double coeff2 = lmPar * lmNorm * lmNorm / pc2;\n            double preRed = coeff1 + 2 * coeff2;\n            double dirDer = -(coeff1 + coeff2);\n\n            // ratio of the actual to the predicted reduction\n            ratio = (preRed == 0) ? 0 : (actRed / preRed);\n\n            // update the step bound\n            if (ratio <= 0.25) {\n                double tmp = (actRed < 0) ? (0.5 * dirDer / (dirDer + 0.5 * actRed)) : 0.5;\n                if ((0.1 * currentCost >= previousCost) || (tmp < 0.1)) {\n                    tmp = 0.1;\n                }\n                delta = tmp * FastMath.min(delta, 10.0 * lmNorm);\n                lmPar /= tmp;\n            } else if ((lmPar == 0) || (ratio >= 0.75)) {\n                delta = 2 * lmNorm;\n                lmPar *= 0.5;\n            }\n\n            // test for successful iteration.\n            if (ratio >= 1.0e-4) {\n                // successful iteration, update the norm\n                firstIteration = false;\n                xNorm = 0;\n                for (int k = 0; k < nC; ++k) {\n                    double xK = diag[k] * currentPoint[k];\n                    xNorm += xK * xK;\n                }\n                xNorm = FastMath.sqrt(xNorm);\n\n                // tests for convergence.\n                if (checker != null) {\n                    // we use the vectorial convergence checker\n                    if (checker.converged(iter, previous, current)) {\n                        setCost(currentCost);\n                        return current;\n                    }\n                }\n            } else {\n                // failed iteration, reset the previous values\n                currentCost = previousCost;\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    currentPoint[pj] = oldX[pj];\n                }\n                tmpVec = weightedResidual;\n                weightedResidual = oldRes;\n                oldRes = tmpVec;\n                tmpVec = currentObjective;\n                currentObjective = oldObj;\n                oldObj = tmpVec;\n                // Reset \"current\" to previous values.\n                current = new PointVectorValuePair(currentPoint.clone(), currentObjective);\n            }\n\n            // Default convergence criteria.\n            if ((FastMath.abs(actRed) <= costRelativeTolerance &&\n                 preRed <= costRelativeTolerance &&\n                 ratio <= 2.0) ||\n                delta <= parRelativeTolerance * xNorm) {\n                setCost(currentCost);\n                return current;\n            }\n\n            // tests for termination and stringent tolerances\n            // (2.2204e-16 is the machine epsilon for IEEE754)\n            if ((FastMath.abs(actRed) <= 2.2204e-16) && (preRed <= 2.2204e-16) && (ratio <= 2.0)) {\n                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_COST_RELATIVE_TOLERANCE, costRelativeTolerance);\n            } else if (delta <= 2.2204e-16 * xNorm) {\n                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE, parRelativeTolerance);\n            } else if (maxCosine <= 2.2204e-16) {\n                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_ORTHOGONALITY_TOLERANCE, orthoTolerance);\n            }\n        }\n    }\n}\n",
                            "\n@Override\nprotected PointVectorValuePair doOptimize() {\n    checkParameters();\n\n    final int nR = getTarget().length; // Number of observed data.\n    final double[] currentPoint = getStartPoint();\n    final int nC = currentPoint.length; // Number of parameters.\n\n    // arrays shared with the other private methods\n    solvedCols  = FastMath.min(nR, nC);\n    diagR       = new double[nC];\n    jacNorm     = new double[nC];\n    beta        = new double[nC];\n    permutation = new int[nC];\n    lmDir       = new double[nC];\n\n    // local point\n    double   delta   = 0;\n    double   xNorm   = 0;\n    double[] diag    = new double[nC];\n    double[] oldX    = new double[nC];\n    double[] oldRes  = new double[nR];\n    double[] oldObj  = new double[nR];\n    double[] qtf     = new double[nR];\n    double[] work1   = new double[nC];\n    double[] work2   = new double[nC];\n    double[] work3   = new double[nC];\n\n    final RealMatrix weightMatrixSqrt = getWeightSquareRoot();\n\n    // Evaluate the function at the starting point and calculate its norm.\n    double[] currentObjective = computeObjectiveValue(currentPoint);\n    double[] currentResiduals = computeResiduals(currentObjective);\n    PointVectorValuePair current = new PointVectorValuePair(currentPoint.clone(), currentObjective.clone());\n    double currentCost = computeCost(currentResiduals);\n\n    // Outer loop.\n    lmPar = 0;\n    boolean firstIteration = true;\n    int iter = 0;\n    final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n    while (true) {\n        ++iter;\n        final PointVectorValuePair previous = current;\n\n        // QR decomposition of the jacobian matrix\n        qrDecomposition(computeWeightedJacobian(currentPoint));\n\n        weightedResidual = weightMatrixSqrt.operate(currentResiduals);\n        for (int i = 0; i < nR; i++) {\n            qtf[i] = weightedResidual[i];\n        }\n\n        // compute Qt.res\n        qTy(qtf);\n\n        // now we don't need Q anymore,\n        // so let jacobian contain the R matrix with its diagonal elements\n        for (int k = 0; k < solvedCols; ++k) {\n            int pk = permutation[k];\n            weightedJacobian[k][pk] = diagR[pk];\n        }\n\n        if (firstIteration) {\n            // scale the point according to the norms of the columns\n            // of the initial jacobian\n            xNorm = 0;\n            for (int k = 0; k < nC; ++k) {\n                double dk = jacNorm[k];\n                if (dk == 0) {\n                    dk = 1.0;\n                }\n                double xk = dk * currentPoint[k];\n                xNorm  += xk * xk;\n                diag[k] = dk;\n            }\n            xNorm = FastMath.sqrt(xNorm);\n\n            // initialize the step bound delta\n            delta = (xNorm == 0) ? initialStepBoundFactor : (initialStepBoundFactor * xNorm);\n        }\n\n        // check orthogonality between function vector and jacobian columns\n        double maxCosine = 0;\n        if (currentCost != 0) {\n            for (int j = 0; j < solvedCols; ++j) {\n                int    pj = permutation[j];\n                double s  = jacNorm[pj];\n                if (s != 0) {\n                    double sum = 0;\n                    for (int i = 0; i <= j; ++i) {\n                        sum += weightedJacobian[i][pj] * qtf[i];\n                    }\n                    maxCosine = FastMath.max(maxCosine, FastMath.abs(sum) / (s * currentCost));\n                }\n            }\n        }\n        if (maxCosine <= orthoTolerance) {\n            // Convergence has been reached.\n            setCost(currentCost);\n            return current;\n        }\n\n        // rescale if necessary\n        for (int j = 0; j < nC; ++j) {\n            diag[j] = FastMath.max(diag[j], jacNorm[j]);\n        }\n\n        // Inner loop.\n        for (double ratio = 0; ratio < 1.0e-4;) {\n\n            // save the state\n            for (int j = 0; j < solvedCols; ++j) {\n                int pj = permutation[j];\n                oldX[pj] = currentPoint[pj];\n            }\n            final double previousCost = currentCost;\n            double[] tmpVec = weightedResidual;\n            weightedResidual = oldRes;\n            oldRes    = tmpVec;\n            tmpVec    = currentObjective;\n            currentObjective = oldObj;\n            oldObj    = tmpVec;\n\n            // determine the Levenberg-Marquardt parameter\n            determineLMParameter(qtf, delta, diag, work1, work2, work3);\n\n            // compute the new point and the norm of the evolution direction\n            double lmNorm = 0;\n            for (int j = 0; j < solvedCols; ++j) {\n                int pj = permutation[j];\n                lmDir[pj] = -lmDir[pj];\n                currentPoint[pj] = oldX[pj] + lmDir[pj];\n                double s = diag[pj] * lmDir[pj];\n                lmNorm  += s * s;\n            }\n            lmNorm = FastMath.sqrt(lmNorm);\n            // on the first iteration, adjust the initial step bound.\n            if (firstIteration) {\n                delta = FastMath.min(delta, lmNorm);\n            }\n\n            // Evaluate the function at x + p and calculate its norm.\n            currentObjective = computeObjectiveValue(currentPoint);\n            currentResiduals = computeResiduals(currentObjective);\n            current = new PointVectorValuePair(currentPoint.clone(), currentObjective.clone());\n            currentCost = computeCost(currentResiduals);\n\n            // compute the scaled actual reduction\n            double actRed = -1.0;\n            if (0.1 * currentCost < previousCost) {\n                double r = currentCost / previousCost;\n                actRed = 1.0 - r * r;\n            }\n\n            // compute the scaled predicted reduction\n            // and the scaled directional derivative\n            for (int j = 0; j < solvedCols; ++j) {\n                int pj = permutation[j];\n                double dirJ = lmDir[pj];\n                work1[j] = 0;\n                for (int i = 0; i <= j; ++i) {\n                    work1[i] += weightedJacobian[i][pj] * dirJ;\n                }\n            }\n            double coeff1 = 0;\n            for (int j = 0; j < solvedCols; ++j) {\n                coeff1 += work1[j] * work1[j];\n            }\n            double pc2 = previousCost * previousCost;\n            coeff1 = coeff1 / pc2;\n            double coeff2 = lmPar * lmNorm * lmNorm / pc2;\n            double preRed = coeff1 + 2 * coeff2;\n            double dirDer = -(coeff1 + coeff2);\n\n            // ratio of the actual to the predicted reduction\n            ratio = (preRed == 0) ? 0 : (actRed / preRed);\n\n            // update the step bound\n            if (ratio <= 0.25) {\n                double tmp =\n                    (actRed < 0) ? (0.5 * dirDer / (dirDer + 0.5 * actRed)) : 0.5;\n                    if ((0.1 * currentCost >= previousCost) || (tmp < 0.1)) {\n                        tmp = 0.1;\n                    }\n                    delta = tmp * FastMath.min(delta, 10.0 * lmNorm);\n                    lmPar /= tmp;\n            } else if ((lmPar == 0) || (ratio >= 0.75)) {\n                delta = 2 * lmNorm;\n                lmPar *= 0.5;\n            }\n\n            // test for successful iteration.\n            if (ratio >= 1.0e-4) {\n                // successful iteration, update the norm\n                firstIteration = false;\n                xNorm = 0;\n                for (int k = 0; k < nC; ++k) {\n                    double xK = diag[k] * currentPoint[k];\n                    xNorm += xK * xK;\n                }\n                xNorm = FastMath.sqrt(xNorm);\n\n                // tests for convergence.\n                if (checker != null) {\n                    // we use the vectorial convergence checker\n                    if (checker.converged(iter, previous, current)) {\n                        setCost(currentCost);\n                        return current;\n                    }\n                }\n            } else {\n                // failed iteration, reset the previous values\n                currentCost = previousCost;\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    currentPoint[pj] = oldX[pj];\n                }\n                tmpVec    = weightedResidual;\n                weightedResidual = oldRes;\n                oldRes    = tmpVec;\n                tmpVec    = currentObjective;\n                currentObjective = oldObj;\n                oldObj    = tmpVec;\n                // Reset \"current\" to previous values.\n                current = new PointVectorValuePair(currentPoint.clone(), currentObjective.clone());\n            }\n\n            // Default convergence criteria.\n            if ((FastMath.abs(actRed) <= costRelativeTolerance &&\n                 preRed <= costRelativeTolerance &&\n                 ratio <= 2.0) ||\n                delta <= parRelativeTolerance * xNorm) {\n                setCost(currentCost);\n                return current;\n            }\n\n            // tests for termination and stringent tolerances\n            // (2.2204e-16 is the machine epsilon for IEEE754)\n            if ((FastMath.abs(actRed) <= 2.2204e-16) && (preRed <= 2.2204e-16) && (ratio <= 2.0)) {\n                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_COST_RELATIVE_TOLERANCE,\n                                               costRelativeTolerance);\n            } else if (delta <= 2.2204e-16 * xNorm) {\n                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE,\n                                               parRelativeTolerance);\n            } else if (maxCosine <= 2.2204e-16)  {\n                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_ORTHOGONALITY_TOLERANCE,\n                                               orthoTolerance);\n            }\n        }\n    }\n}\n",
                            "\n@Override\nprotected PointVectorValuePair doOptimize() {\n    checkParameters();\n\n    final int nR = getTarget().length; // Number of observed data.\n    final double[] currentPoint = getStartPoint();\n    final int nC = currentPoint.length; // Number of parameters.\n\n    // arrays shared with the other private methods\n    solvedCols  = FastMath.min(nR, nC);\n    diagR       = new double[nC];\n    jacNorm     = new double[nC];\n    beta        = new double[nC];\n    permutation = new int[nC];\n    lmDir       = new double[nC];\n\n    // local point\n    double delta = 0;\n    double xNorm = 0;\n    double[] diag = new double[nC];\n    double[] oldX = new double[nC];\n    double[] oldRes = new double[nR];\n    double[] oldObj = new double[nR];\n    double[] qtf = new double[nR];\n    double[] work1 = new double[nC];\n    double[] work2 = new double[nC];\n    double[] work3 = new double[nC];\n\n    final RealMatrix weightMatrixSqrt = getWeightSquareRoot();\n\n    // Evaluate the function at the starting point and calculate its norm.\n    double[] currentObjective = computeObjectiveValue(currentPoint);\n    double[] currentResiduals = computeResiduals(currentObjective);\n    PointVectorValuePair current = new PointVectorValuePair(currentPoint, currentObjective);\n    double currentCost = computeCost(currentResiduals);\n\n    // Outer loop.\n    lmPar = 0;\n    boolean firstIteration = true;\n    int iter = 0;\n    final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n    while (true) {\n        ++iter;\n        final PointVectorValuePair previous = current;\n\n        // QR decomposition of the jacobian matrix\n        qrDecomposition(computeWeightedJacobian(currentPoint));\n\n        weightedResidual = weightMatrixSqrt.operate(currentResiduals);\n        for (int i = 0; i < nR; i++) {\n            qtf[i] = weightedResidual[i];\n        }\n\n        // compute Qt.res\n        qTy(qtf);\n\n        // now we don't need Q anymore,\n        // so let jacobian contain the R matrix with its diagonal elements\n        for (int k = 0; k < solvedCols; ++k) {\n            int pk = permutation[k];\n            weightedJacobian[k][pk] = diagR[pk];\n        }\n\n        if (firstIteration) {\n            // scale the point according to the norms of the columns\n            // of the initial jacobian\n            xNorm = 0;\n            for (int k = 0; k < nC; ++k) {\n                double dk = jacNorm[k];\n                if (dk == 0) {\n                    dk = 1.0;\n                }\n                double xk = dk * currentPoint[k];\n                xNorm += xk * xk;\n                diag[k] = dk;\n            }\n            xNorm = FastMath.sqrt(xNorm);\n\n            // initialize the step bound delta\n            delta = (xNorm == 0) ? initialStepBoundFactor : (initialStepBoundFactor * xNorm);\n        }\n\n        // check orthogonality between function vector and jacobian columns\n        double maxCosine = 0;\n        if (currentCost != 0) {\n            for (int j = 0; j < solvedCols; ++j) {\n                int pj = permutation[j];\n                double s = jacNorm[pj];\n                if (s != 0) {\n                    double sum = 0;\n                    for (int i = 0; i <= j; ++i) {\n                        sum += weightedJacobian[i][pj] * qtf[i];\n                    }\n                    maxCosine = FastMath.max(maxCosine, FastMath.abs(sum) / (s * currentCost));\n                }\n            }\n        }\n        if (maxCosine <= orthoTolerance) {\n            // Convergence has been reached.\n            setCost(currentCost);\n            return current;\n        }\n\n        // rescale if necessary\n        for (int j = 0; j < nC; ++j) {\n            diag[j] = FastMath.max(diag[j], jacNorm[j]);\n        }\n\n        // Inner loop.\n        for (double ratio = 0; ratio < 1.0e-4;) {\n\n            // save the state\n            for (int j = 0; j < solvedCols; ++j) {\n                int pj = permutation[j];\n                oldX[pj] = currentPoint[pj];\n            }\n            final double previousCost = currentCost;\n            double[] tmpVec = weightedResidual;\n            weightedResidual = oldRes;\n            oldRes = tmpVec;\n            tmpVec = currentObjective;\n            currentObjective = oldObj;\n            oldObj = tmpVec;\n\n            // determine the Levenberg-Marquardt parameter\n            determineLMParameter(qtf, delta, diag, work1, work2, work3);\n\n            // compute the new point and the norm of the evolution direction\n            double lmNorm = 0;\n            for (int j = 0; j < solvedCols; ++j) {\n                int pj = permutation[j];\n                lmDir[pj] = -lmDir[pj];\n                currentPoint[pj] = oldX[pj] + lmDir[pj];\n                double s = diag[pj] * lmDir[pj];\n                lmNorm += s * s;\n            }\n            lmNorm = FastMath.sqrt(lmNorm);\n            // on the first iteration, adjust the initial step bound.\n            if (firstIteration) {\n                delta = FastMath.min(delta, lmNorm);\n            }\n\n            // Evaluate the function at x + p and calculate its norm.\n            currentObjective = computeObjectiveValue(currentPoint);\n            currentResiduals = computeResiduals(currentObjective);\n            current = new PointVectorValuePair(currentPoint, currentObjective);\n            currentCost = computeCost(currentResiduals);\n\n            // compute the scaled actual reduction\n            double actRed = -1.0;\n            if (0.1 * currentCost < previousCost) {\n                double r = currentCost / previousCost;\n                actRed = 1.0 - r * r;\n            }\n\n            // compute the scaled predicted reduction\n            // and the scaled directional derivative\n            for (int j = 0; j < solvedCols; ++j) {\n                int pj = permutation[j];\n                double dirJ = lmDir[pj];\n                work1[j] = 0;\n                for (int i = 0; i <= j; ++i) {\n                    work1[i] += weightedJacobian[i][pj] * dirJ;\n                }\n            }\n            double coeff1 = 0;\n            for (int j = 0; j < solvedCols; ++j) {\n                coeff1 += work1[j] * work1[j];\n            }\n            double pc2 = previousCost * previousCost;\n            coeff1 = coeff1 / pc2;\n            double coeff2 = lmPar * lmNorm * lmNorm / pc2;\n            double preRed = coeff1 + 2 * coeff2;\n            double dirDer = -(coeff1 + coeff2);\n\n            // ratio of the actual to the predicted reduction\n            ratio = (preRed == 0) ? 0 : (actRed / preRed);\n\n            // update the step bound\n            if (ratio <= 0.25) {\n                double tmp =\n                        (actRed < 0) ? (0.5 * dirDer / (dirDer + 0.5 * actRed)) : 0.5;\n                if ((0.1 * currentCost >= previousCost) || (tmp < 0.1)) {\n                    tmp = 0.1;\n                }\n                delta = tmp * FastMath.min(delta, 10.0 * lmNorm);\n                lmPar /= tmp;\n            } else if ((lmPar == 0) || (ratio >= 0.75)) {\n                delta = 2 * lmNorm;\n                lmPar *= 0.5;\n            }\n\n            // test for successful iteration.\n            if (ratio >= 1.0e-4) {\n                // successful iteration, update the norm\n                firstIteration = false;\n                xNorm = 0;\n                for (int k = 0; k < nC; ++k) {\n                    double xK = diag[k] * currentPoint[k];\n                    xNorm += xK * xK;\n                }\n                xNorm = FastMath.sqrt(xNorm);\n\n                // tests for convergence.\n                if (checker != null) {\n                    // we use the vectorial convergence checker\n                    if (checker.converged(iter, previous, current)) {\n                        setCost(currentCost);\n                        return current;\n                    }\n                }\n            } else {\n                // failed iteration, reset the previous values\n                currentCost = previousCost;\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    currentPoint[pj] = oldX[pj];\n                }\n                tmpVec = weightedResidual;\n                weightedResidual = oldRes;\n                oldRes = tmpVec;\n                tmpVec = currentObjective;\n                currentObjective = oldObj;\n                oldObj = tmpVec;\n                // Reset \"current\" to previous values.\n                current = new PointVectorValuePair(currentPoint, currentObjective);\n            }\n\n            // Default convergence criteria.\n            if ((FastMath.abs(actRed) <= costRelativeTolerance &&\n                    preRed <= costRelativeTolerance &&\n                    ratio <= 2.0) ||\n                    delta <= parRelativeTolerance * xNorm) {\n                setCost(currentCost);\n                return current;\n            }\n\n            // tests for termination and stringent tolerances\n            // (2.2204e-16 is the machine epsilon for IEEE754)\n            if ((FastMath.abs(actRed) <= 2.2204e-16) && (preRed <= 2.2204e-16) && (ratio <= 2.0)) {\n                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_COST_RELATIVE_TOLERANCE,\n                        costRelativeTolerance);\n            } else if (delta <= 2.2204e-16 * xNorm) {\n                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE,\n                        parRelativeTolerance);\n            } else if (maxCosine <= 2.2204e-16) {\n                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_ORTHOGONALITY_TOLERANCE,\n                        orthoTolerance);\n            }\n        }\n    }\n}\n",
                            "\n@Override\nprotected PointVectorValuePair doOptimize() {\n    checkParameters();\n\n    final int nR = getTarget().length; // Number of observed data.\n    final double[] currentPoint = getStartPoint();\n    final int nC = currentPoint.length; // Number of parameters.\n\n    // arrays shared with the other private methods\n    solvedCols = FastMath.min(nR, nC);\n    diagR = new double[nC];\n    jacNorm = new double[nC];\n    beta = new double[nC];\n    permutation = new int[nC];\n    lmDir = new double[nC];\n\n    // local point\n    double delta = 0;\n    double xNorm = 0;\n    double[] diag = new double[nC];\n    double[] oldX = new double[nC];\n    double[] oldRes = new double[nR];\n    double[] oldObj = new double[nR];\n    double[] qtf = new double[nR];\n    double[] work1 = new double[nC];\n    double[] work2 = new double[nC];\n    double[] work3 = new double[nC];\n\n    final RealMatrix weightMatrixSqrt = getWeightSquareRoot();\n\n    // Evaluate the function at the starting point and calculate its norm.\n    double[] currentObjective = computeObjectiveValue(currentPoint);\n    double[] currentResiduals = computeResiduals(currentObjective);\n    PointVectorValuePair current = new PointVectorValuePair(currentPoint, currentObjective);\n    double currentCost = computeCost(currentResiduals);\n\n    // Outer loop.\n    lmPar = 0;\n    boolean firstIteration = true;\n    int iter = 0;\n    final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n    while (true) {\n        ++iter;\n        final PointVectorValuePair previous = current;\n\n        // QR decomposition of the jacobian matrix\n        qrDecomposition(computeWeightedJacobian(currentPoint));\n\n        weightedResidual = weightMatrixSqrt.operate(currentResiduals);\n        for (int i = 0; i < nR; i++) {\n            qtf[i] = weightedResidual[i];\n        }\n\n        // compute Qt.res\n        qTy(qtf);\n\n        // now we don't need Q anymore,\n        // so let jacobian contain the R matrix with its diagonal elements\n        for (int k = 0; k < solvedCols; ++k) {\n            int pk = permutation[k];\n            weightedJacobian[k][pk] = diagR[pk];\n        }\n\n        if (firstIteration) {\n            // scale the point according to the norms of the columns\n            // of the initial jacobian\n            xNorm = 0;\n            for (int k = 0; k < nC; ++k) {\n                double dk = jacNorm[k];\n                if (dk == 0) {\n                    dk = 1.0;\n                }\n                double xk = dk * currentPoint[k];\n                xNorm += xk * xk;\n                diag[k] = dk;\n            }\n            xNorm = FastMath.sqrt(xNorm);\n\n            // initialize the step bound delta\n            delta = (xNorm == 0) ? initialStepBoundFactor : (initialStepBoundFactor * xNorm);\n        }\n\n        // check orthogonality between function vector and jacobian columns\n        double maxCosine = 0;\n        if (currentCost != 0) {\n            for (int j = 0; j < solvedCols; ++j) {\n                int pj = permutation[j];\n                double s = jacNorm[pj];\n                if (s != 0) {\n                    double sum = 0;\n                    for (int i = 0; i <= j; ++i) {\n                        sum += weightedJacobian[i][pj] * qtf[i];\n                    }\n                    maxCosine = FastMath.max(maxCosine, FastMath.abs(sum) / (s * currentCost));\n                }\n            }\n        }\n        if (maxCosine <= orthoTolerance) {\n            // Convergence has been reached.\n            setCost(currentCost);\n            return current;\n        }\n\n        // rescale if necessary\n        for (int j = 0; j < nC; ++j) {\n            diag[j] = FastMath.max(diag[j], jacNorm[j]);\n        }\n\n        // Inner loop.\n        for (double ratio = 0; ; ) {\n\n            // save the state\n            for (int j = 0; j < solvedCols; ++j) {\n                int pj = permutation[j];\n                oldX[pj] = currentPoint[pj];\n            }\n            final double previousCost = currentCost;\n            double[] tmpVec = weightedResidual;\n            weightedResidual = oldRes;\n            oldRes = tmpVec;\n            tmpVec = currentObjective;\n            currentObjective = oldObj;\n            oldObj = tmpVec;\n\n            // determine the Levenberg-Marquardt parameter\n            determineLMParameter(qtf, delta, diag, work1, work2, work3);\n\n            // compute the new point and the norm of the evolution direction\n            double lmNorm = 0;\n            for (int j = 0; j < solvedCols; ++j) {\n                int pj = permutation[j];\n                lmDir[pj] = -lmDir[pj];\n                currentPoint[pj] = oldX[pj] + lmDir[pj];\n                double s = diag[pj] * lmDir[pj];\n                lmNorm += s * s;\n            }\n            lmNorm = FastMath.sqrt(lmNorm);\n            // on the first iteration, adjust the initial step bound.\n            if (firstIteration) {\n                delta = FastMath.min(delta, lmNorm);\n                firstIteration = false;\n            }\n\n            // Evaluate the function at x + p and calculate its norm.\n            currentObjective = computeObjectiveValue(currentPoint);\n            currentResiduals = computeResiduals(currentObjective);\n            current = new PointVectorValuePair(currentPoint, currentObjective);\n            currentCost = computeCost(currentResiduals);\n\n            // compute the scaled actual reduction\n            double actRed = -1.0;\n            if (0.1 * currentCost < previousCost) {\n                double r = currentCost / previousCost;\n                actRed = 1.0 - r * r;\n            }\n\n            // compute the scaled predicted reduction\n            // and the scaled directional derivative\n            for (int j = 0; j < solvedCols; ++j) {\n                int pj = permutation[j];\n                double dirJ = lmDir[pj];\n                work1[j] = 0;\n                for (int i = 0; i <= j; ++i) {\n                    work1[i] += weightedJacobian[i][pj] * dirJ;\n                }\n            }\n            double coeff1 = 0;\n            for (int j = 0; j < solvedCols; ++j) {\n                coeff1 += work1[j] * work1[j];\n            }\n            double pc2 = previousCost * previousCost;\n            coeff1 = coeff1 / pc2;\n            double coeff2 = lmPar * lmNorm * lmNorm / pc2;\n            double preRed = coeff1 + 2 * coeff2;\n            double dirDer = -(coeff1 + coeff2);\n\n            // ratio of the actual to the predicted reduction\n            ratio = (preRed == 0) ? 0 : (actRed / preRed);\n\n            // update the step bound\n            if (ratio <= 0.25) {\n                double tmp = (actRed < 0) ? (0.5 * dirDer / (dirDer + 0.5 * actRed)) : 0.5;\n                if ((0.1 * currentCost >= previousCost) || (tmp < 0.1)) {\n                    tmp = 0.1;\n                }\n                delta = tmp * FastMath.min(delta, 10.0 * lmNorm);\n                lmPar /= tmp;\n            } else if ((lmPar == 0) || (ratio >= 0.75)) {\n                delta = 2 * lmNorm;\n                lmPar *= 0.5;\n            }\n\n            // test for successful iteration.\n            if (ratio >= 1.0e-4) {\n                // successful iteration, update the norm\n                xNorm = 0;\n                for (int k = 0; k < nC; ++k) {\n                    double xK = diag[k] * currentPoint[k];\n                    xNorm += xK * xK;\n                }\n                xNorm = FastMath.sqrt(xNorm);\n\n                // tests for convergence.\n                if (checker != null) {\n                    // we use the vectorial convergence checker\n                    if (checker.converged(iter, previous, current)) {\n                        setCost(currentCost);\n                        return current;\n                    }\n                }\n                break; // Successful iteration, break inner loop and continue with outer loop\n            } else {\n                // failed iteration, reset the previous values\n                currentCost = previousCost;\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    currentPoint[pj] = oldX[pj];\n                }\n                tmpVec = weightedResidual;\n                weightedResidual = oldRes;\n                oldRes = tmpVec;\n                tmpVec = currentObjective;\n                currentObjective = oldObj;\n                oldObj = tmpVec;\n                // Reset \"current\" to previous values.\n                current = new PointVectorValuePair(currentPoint, currentObjective);\n            }\n\n            // Default convergence criteria.\n            if ((FastMath.abs(actRed) <= costRelativeTolerance &&\n                preRed <= costRelativeTolerance &&\n                ratio <= 2.0) ||\n                delta <= parRelativeTolerance * xNorm) {\n                setCost(currentCost);\n                return current;\n            }\n\n            // tests for termination and stringent tolerances\n            // (2.2204e-16 is the machine epsilon for IEEE754)\n            if ((FastMath.abs(actRed) <= 2.2204e-16) && (preRed <= 2.2204e-16) && (ratio <= 2.0)) {\n                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_COST_RELATIVE_TOLERANCE,\n                    costRelativeTolerance);\n            } else if (delta <= 2.2204e-16 * xNorm) {\n                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE,\n                    parRelativeTolerance);\n            } else if (maxCosine <= 2.2204e-16) {\n                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_ORTHOGONALITY_TOLERANCE,\n                    orthoTolerance);\n            }\n        }\n    }\n}\n",
                            "\n@Override\nprotected PointVectorValuePair doOptimize() {\n    checkParameters();\n\n    final int nR = getTarget().length; // Number of observed data.\n    final double[] currentPoint = getStartPoint();\n    final int nC = currentPoint.length; // Number of parameters.\n\n    // arrays shared with the other private methods\n    solvedCols = FastMath.min(nR, nC);\n    diagR = new double[nC];\n    jacNorm = new double[nC];\n    beta = new double[nC];\n    permutation = new int[nC];\n    lmDir = new double[nC];\n\n    // local point\n    double delta = 0;\n    double xNorm = 0;\n    double[] diag = new double[nC];\n    double[] oldX = new double[nC];\n    double[] oldRes = new double[nR];\n    double[] oldObj = new double[nR];\n    double[] qtf = new double[nR];\n    double[] work1 = new double[nC];\n    double[] work2 = new double[nC];\n    double[] work3 = new double[nC];\n\n    final RealMatrix weightMatrixSqrt = getWeightSquareRoot();\n\n    // Evaluate the function at the starting point and calculate its norm.\n    double[] currentObjective = computeObjectiveValue(currentPoint);\n    double[] currentResiduals = computeResiduals(currentObjective);\n    PointVectorValuePair current = new PointVectorValuePair(currentPoint, currentObjective);\n    double currentCost = computeCost(currentResiduals);\n\n    // Outer loop.\n    lmPar = 0;\n    boolean firstIteration = true;\n    int iter = 0;\n    final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n    while (true) {\n        ++iter;\n        final PointVectorValuePair previous = current;\n\n        // QR decomposition of the jacobian matrix\n        qrDecomposition(computeWeightedJacobian(currentPoint));\n\n        weightedResidual = weightMatrixSqrt.operate(currentResiduals);\n        for (int i = 0; i < nR; i++) {\n            qtf[i] = weightedResidual[i];\n        }\n\n        // compute Qt.res\n        qTy(qtf);\n\n        // now we don't need Q anymore,\n        // so let jacobian contain the R matrix with its diagonal elements\n        for (int k = 0; k < solvedCols; ++k) {\n            int pk = permutation[k];\n            weightedJacobian[k][pk] = diagR[pk];\n        }\n\n        if (firstIteration) {\n            // scale the point according to the norms of the columns\n            // of the initial jacobian\n            xNorm = 0;\n            for (int k = 0; k < nC; ++k) {\n                double dk = jacNorm[k];\n                if (dk == 0) {\n                    dk = 1.0;\n                }\n                double xk = dk * currentPoint[k];\n                xNorm += xk * xk;\n                diag[k] = dk;\n            }\n            xNorm = FastMath.sqrt(xNorm);\n\n            // initialize the step bound delta\n            delta = (xNorm == 0) ? initialStepBoundFactor : (initialStepBoundFactor * xNorm);\n        }\n\n        // check orthogonality between function vector and jacobian columns\n        double maxCosine = 0;\n        if (currentCost != 0) {\n            for (int j = 0; j < solvedCols; ++j) {\n                int pj = permutation[j];\n                double s = jacNorm[pj];\n                if (s != 0) {\n                    double sum = 0;\n                    for (int i = 0; i <= j; ++i) {\n                        sum += weightedJacobian[i][pj] * qtf[i];\n                    }\n                    maxCosine = FastMath.max(maxCosine, FastMath.abs(sum) / (s * currentCost));\n                }\n            }\n        }\n        if (maxCosine <= orthoTolerance) {\n            // Convergence has been reached.\n            setCost(currentCost);\n            point = current.getPoint();\n            return current;\n        }\n\n        // rescale if necessary\n        for (int j = 0; j < nC; ++j) {\n            diag[j] = FastMath.max(diag[j], jacNorm[j]);\n        }\n\n        // Inner loop.\n        for (double ratio = 0; ratio < 1.0e-4;) {\n\n            // save the state\n            for (int j = 0; j < solvedCols; ++j) {\n                int pj = permutation[j];\n                oldX[pj] = currentPoint[pj];\n            }\n            final double previousCost = currentCost;\n            double[] tmpVec = weightedResidual;\n            weightedResidual = oldRes;\n            oldRes = tmpVec;\n            tmpVec = currentObjective;\n            currentObjective = oldObj;\n            oldObj = tmpVec;\n\n            // determine the Levenberg-Marquardt parameter\n            determineLMParameter(qtf, delta, diag, work1, work2, work3);\n\n            // compute the new point and the norm of the evolution direction\n            double lmNorm = 0;\n            for (int j = 0; j < solvedCols; ++j) {\n                int pj = permutation[j];\n                lmDir[pj] = -lmDir[pj];\n                currentPoint[pj] = oldX[pj] + lmDir[pj];\n                double s = diag[pj] * lmDir[pj];\n                lmNorm += s * s;\n            }\n            lmNorm = FastMath.sqrt(lmNorm);\n            // on the first iteration, adjust the initial step bound.\n            if (firstIteration) {\n                delta = FastMath.min(delta, lmNorm);\n            }\n\n            // Evaluate the function at x + p and calculate its norm.\n            currentObjective = computeObjectiveValue(currentPoint);\n            currentResiduals = computeResiduals(currentObjective);\n            current = new PointVectorValuePair(currentPoint, currentObjective);\n            currentCost = computeCost(currentResiduals);\n\n            // compute the scaled actual reduction\n            double actRed = -1.0;\n            if (0.1 * currentCost < previousCost) {\n                double r = currentCost / previousCost;\n                actRed = 1.0 - r * r;\n            }\n\n            // compute the scaled predicted reduction\n            // and the scaled directional derivative\n            for (int j = 0; j < solvedCols; ++j) {\n                int pj = permutation[j];\n                double dirJ = lmDir[pj];\n                work1[j] = 0;\n                for (int i = 0; i <= j; ++i) {\n                    work1[i] += weightedJacobian[i][pj] * dirJ;\n                }\n            }\n            double coeff1 = 0;\n            for (int j = 0; j < solvedCols; ++j) {\n                coeff1 += work1[j] * work1[j];\n            }\n            double pc2 = previousCost * previousCost;\n            coeff1 = coeff1 / pc2;\n            double coeff2 = lmPar * lmNorm * lmNorm / pc2;\n            double preRed = coeff1 + 2 * coeff2;\n            double dirDer = -(coeff1 + coeff2);\n\n            // ratio of the actual to the predicted reduction\n            ratio = (preRed == 0) ? 0 : (actRed / preRed);\n\n            // update the step bound\n            if (ratio <= 0.25) {\n                double tmp = (actRed < 0) ? (0.5 * dirDer / (dirDer + 0.5 * actRed)) : 0.5;\n                if ((0.1 * currentCost >= previousCost) || (tmp < 0.1)) {\n                    tmp = 0.1;\n                }\n                delta = tmp * FastMath.min(delta, 10.0 * lmNorm);\n                lmPar /= tmp;\n            } else if ((lmPar == 0) || (ratio >= 0.75)) {\n                delta = 2 * lmNorm;\n                lmPar *= 0.5;\n            }\n\n            // test for successful iteration.\n            if (ratio >= 1.0e-4) {\n                // successful iteration, update the norm\n                firstIteration = false;\n                xNorm = 0;\n                for (int k = 0; k < nC; ++k) {\n                    double xK = diag[k] * currentPoint[k];\n                    xNorm += xK * xK;\n                }\n                xNorm = FastMath.sqrt(xNorm);\n\n                // tests for convergence.\n                if (checker != null) {\n                    // we use the vectorial convergence checker\n                    if (checker.converged(iter, previous, current)) {\n                        setCost(currentCost);\n                        point = current.getPoint();\n                        return current;\n                    }\n                }\n            } else {\n                // failed iteration, reset the previous values\n                currentCost = previousCost;\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    currentPoint[pj] = oldX[pj];\n                }\n                tmpVec = weightedResidual;\n                weightedResidual = oldRes;\n                oldRes = tmpVec;\n                tmpVec = currentObjective;\n                currentObjective = oldObj;\n                oldObj = tmpVec;\n                // Reset \"current\" to previous values.\n                current = new PointVectorValuePair(currentPoint, currentObjective);\n            }\n\n            // Default convergence criteria.\n            if ((FastMath.abs(actRed) <= costRelativeTolerance &&\n                 preRed <= costRelativeTolerance &&\n                 ratio <= 2.0) ||\n                delta <= parRelativeTolerance * xNorm) {\n                setCost(currentCost);\n                point = current.getPoint();\n                return current;\n            }\n\n            // tests for termination and stringent tolerances\n            // (2.2204e-16 is the machine epsilon for IEEE754)\n            if ((FastMath.abs(actRed) <= 2.2204e-16) && (preRed <= 2.2204e-16) && (ratio <= 2.0)) {\n                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_COST_RELATIVE_TOLERANCE,\n                                               costRelativeTolerance);\n            } else if (delta <= 2.2204e-16 * xNorm) {\n                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE,\n                                               parRelativeTolerance);\n            } else if (maxCosine <= 2.2204e-16) {\n                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_ORTHOGONALITY_TOLERANCE,\n                                               orthoTolerance);\n            }\n        }\n    }\n}\n",
                            "\n@Override\nprotected PointVectorValuePair doOptimize() {\n    checkParameters();\n\n    final int nR = getTarget().length; // Number of observed data.\n    final double[] currentPoint = getStartPoint();\n    final int nC = currentPoint.length; // Number of parameters.\n\n    // arrays shared with the other private methods\n    solvedCols  = FastMath.min(nR, nC);\n    diagR       = new double[nC];\n    jacNorm     = new double[nC];\n    beta        = new double[nC];\n    permutation = new int[nC];\n    lmDir       = new double[nC];\n\n    // local point\n    double   delta   = 0;\n    double   xNorm   = 0;\n    double[] diag    = new double[nC];\n    double[] oldX    = new double[nC];\n    double[] oldRes  = new double[nR];\n    double[] oldObj  = new double[nR];\n    double[] qtf     = new double[nR];\n    double[] work1   = new double[nC];\n    double[] work2   = new double[nC];\n    double[] work3   = new double[nC];\n\n    final RealMatrix weightMatrixSqrt = getWeightSquareRoot();\n\n    // Evaluate the function at the starting point and calculate its norm.\n    double[] currentObjective = computeObjectiveValue(currentPoint);\n    double[] currentResiduals = computeResiduals(currentObjective);\n    PointVectorValuePair current = new PointVectorValuePair(currentPoint, currentObjective);\n    double currentCost = computeCost(currentResiduals);\n\n    // Outer loop.\n    lmPar = 0;\n    boolean firstIteration = true;\n    int iter = 0;\n    final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n    while (true) {\n        ++iter;\n        final PointVectorValuePair previous = current;\n\n        // QR decomposition of the jacobian matrix\n        qrDecomposition(computeWeightedJacobian(currentPoint));\n\n        weightedResidual = weightMatrixSqrt.operate(currentResiduals);\n        for (int i = 0; i < nR; i++) {\n            qtf[i] = weightedResidual[i];\n        }\n\n        // compute Qt.res\n        qTy(qtf);\n\n        // now we don't need Q anymore,\n        // so let jacobian contain the R matrix with its diagonal elements\n        for (int k = 0; k < solvedCols; ++k) {\n            int pk = permutation[k];\n            weightedJacobian[k][pk] = diagR[pk];\n        }\n\n        if (firstIteration) {\n            // scale the point according to the norms of the columns\n            // of the initial jacobian\n            xNorm = 0;\n            for (int k = 0; k < nC; ++k) {\n                double dk = jacNorm[k];\n                if (dk == 0) {\n                    dk = 1.0;\n                }\n                double xk = dk * currentPoint[k];\n                xNorm  += xk * xk;\n                diag[k] = dk;\n            }\n            xNorm = FastMath.sqrt(xNorm);\n\n            // initialize the step bound delta\n            delta = (xNorm == 0) ? initialStepBoundFactor : (initialStepBoundFactor * xNorm);\n        }\n\n        // check orthogonality between function vector and jacobian columns\n        double maxCosine = 0;\n        if (currentCost != 0) {\n            for (int j = 0; j < solvedCols; ++j) {\n                int    pj = permutation[j];\n                double s  = jacNorm[pj];\n                if (s != 0) {\n                    double sum = 0;\n                    for (int i = 0; i <= j; ++i) {\n                        sum += weightedJacobian[i][pj] * qtf[i];\n                    }\n                    maxCosine = FastMath.max(maxCosine, FastMath.abs(sum) / (s * currentCost));\n                }\n            }\n        }\n        if (maxCosine <= orthoTolerance) {\n            // Convergence has been reached.\n            setCost(currentCost);\n            return current;\n        }\n\n        // rescale if necessary\n        for (int j = 0; j < nC; ++j) {\n            diag[j] = FastMath.max(diag[j], jacNorm[j]);\n        }\n\n        // Inner loop.\n        for (double ratio = 0; ratio < 1.0e-4;) {\n\n            // save the state\n            for (int j = 0; j < solvedCols; ++j) {\n                int pj = permutation[j];\n                oldX[pj] = currentPoint[pj];\n            }\n            final double previousCost = currentCost;\n            double[] tmpVec = weightedResidual;\n            weightedResidual = oldRes;\n            oldRes    = tmpVec;\n            tmpVec    = currentObjective;\n            currentObjective = oldObj;\n            oldObj    = tmpVec;\n\n            // determine the Levenberg-Marquardt parameter\n            determineLMParameter(qtf, delta, diag, work1, work2, work3);\n\n            // compute the new point and the norm of the evolution direction\n            double lmNorm = 0;\n            for (int j = 0; j < solvedCols; ++j) {\n                int pj = permutation[j];\n                lmDir[pj] = -lmDir[pj];\n                currentPoint[pj] = oldX[pj] + lmDir[pj];\n                double s = diag[pj] * lmDir[pj];\n                lmNorm  += s * s;\n            }\n            lmNorm = FastMath.sqrt(lmNorm);\n            // on the first iteration, adjust the initial step bound.\n            if (firstIteration) {\n                delta = FastMath.min(delta, lmNorm);\n            }\n\n            // Evaluate the function at x + p and calculate its norm.\n            currentObjective = computeObjectiveValue(currentPoint);\n            currentResiduals = computeResiduals(currentObjective);\n            current = new PointVectorValuePair(currentPoint, currentObjective);\n            currentCost = computeCost(currentResiduals);\n\n            // compute the scaled actual reduction\n            double actRed = -1.0;\n            if (0.1 * currentCost < previousCost) {\n                double r = currentCost / previousCost;\n                actRed = 1.0 - r * r;\n            }\n\n            // compute the scaled predicted reduction\n            // and the scaled directional derivative\n            for (int j = 0; j < solvedCols; ++j) {\n                int pj = permutation[j];\n                double dirJ = lmDir[pj];\n                work1[j] = 0;\n                for (int i = 0; i <= j; ++i) {\n                    work1[i] += weightedJacobian[i][pj] * dirJ;\n                }\n            }\n            double coeff1 = 0;\n            for (int j = 0; j < solvedCols; ++j) {\n                coeff1 += work1[j] * work1[j];\n            }\n            double pc2 = previousCost * previousCost;\n            coeff1 = coeff1 / pc2;\n            double coeff2 = lmPar * lmNorm * lmNorm / pc2;\n            double preRed = coeff1 + 2 * coeff2;\n            double dirDer = -(coeff1 + coeff2);\n\n            // ratio of the actual to the predicted reduction\n            ratio = (preRed == 0) ? 0 : (actRed / preRed);\n\n            // update the step bound\n            if (ratio <= 0.25) {\n                double tmp =\n                    (actRed < 0) ? (0.5 * dirDer / (dirDer + 0.5 * actRed)) : 0.5;\n                if ((0.1 * currentCost >= previousCost) || (tmp < 0.1)) {\n                    tmp = 0.1;\n                }\n                delta = tmp * FastMath.min(delta, 10.0 * lmNorm);\n                lmPar /= tmp;\n            } else if ((lmPar == 0) || (ratio >= 0.75)) {\n                delta = 2 * lmNorm;\n                lmPar *= 0.5;\n            }\n\n            // test for successful iteration.\n            if (ratio >= 1.0e-4) {\n                // successful iteration, update the norm\n                firstIteration = false;\n                xNorm = 0;\n                for (int k = 0; k < nC; ++k) {\n                    double xK = diag[k] * currentPoint[k];\n                    xNorm += xK * xK;\n                }\n                xNorm = FastMath.sqrt(xNorm);\n\n                // tests for convergence.\n                if (checker != null) {\n                    // we use the vectorial convergence checker\n                    if (checker.converged(iter, previous, current)) {\n                        setCost(currentCost);\n                        return current;\n                    }\n                }\n            } else {\n                // failed iteration, reset the previous values\n                currentCost = previousCost;\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    currentPoint[pj] = oldX[pj];\n                }\n                tmpVec    = weightedResidual;\n                weightedResidual = oldRes;\n                oldRes    = tmpVec;\n                tmpVec    = currentObjective;\n                currentObjective = oldObj;\n                oldObj    = tmpVec;\n                // Reset \"current\" to previous values.\n                current = new PointVectorValuePair(currentPoint, currentObjective);\n            }\n\n            // Default convergence criteria.\n            if ((FastMath.abs(actRed) <= costRelativeTolerance &&\n                 preRed <= costRelativeTolerance &&\n                 ratio <= 2.0) ||\n                delta <= parRelativeTolerance * xNorm) {\n                setCost(currentCost);\n                return current;\n            }\n\n            // tests for termination and stringent tolerances\n            // (2.2204e-16 is the machine epsilon for IEEE754)\n            if ((FastMath.abs(actRed) <= 2.2204e-16) && (preRed <= 2.2204e-16) && (ratio <= 2.0)) {\n                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_COST_RELATIVE_TOLERANCE,\n                                               costRelativeTolerance);\n            } else if (delta <= 2.2204e-16 * xNorm) {\n                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE,\n                                               parRelativeTolerance);\n            } else if (maxCosine <= 2.2204e-16)  {\n                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_ORTHOGONALITY_TOLERANCE,\n                                               orthoTolerance);\n            }\n        }\n    }\n}\n",
                            "\n@Override\nprotected PointVectorValuePair doOptimize() {\n    checkParameters();\n\n    final int nR = getTarget().length; // Number of observed data.\n    final double[] currentPoint = getStartPoint();\n    final int nC = currentPoint.length; // Number of parameters.\n\n    // arrays shared with the other private methods\n    solvedCols  = FastMath.min(nR, nC);\n    diagR       = new double[nC];\n    jacNorm     = new double[nC];\n    beta        = new double[nC];\n    permutation = new int[nC];\n    lmDir       = new double[nC];\n\n    // local point\n    double   delta   = 0;\n    double   xNorm   = 0;\n    double[] diag    = new double[nC];\n    double[] oldX    = new double[nC];\n    double[] oldRes  = new double[nR];\n    double[] oldObj  = new double[nR];\n    double[] qtf     = new double[nR];\n    double[] work1   = new double[nC];\n    double[] work2   = new double[nC];\n    double[] work3   = new double[nC];\n\n    final RealMatrix weightMatrixSqrt = getWeightSquareRoot();\n\n    // Evaluate the function at the starting point and calculate its norm.\n    double[] currentObjective = computeObjectiveValue(currentPoint);\n    double[] currentResiduals = computeResiduals(currentObjective);\n    PointVectorValuePair current = new PointVectorValuePair(currentPoint, currentObjective);\n    double currentCost = computeCost(currentResiduals);\n\n    // Outer loop.\n    lmPar = 0;\n    boolean firstIteration = true;\n    int iter = 0;\n    final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n    while (true) {\n        ++iter;\n        final PointVectorValuePair previous = current;\n\n        // QR decomposition of the jacobian matrix\n        qrDecomposition(computeWeightedJacobian(currentPoint));\n\n        weightedResidual = weightMatrixSqrt.operate(currentResiduals);\n        for (int i = 0; i < nR; i++) {\n            qtf[i] = weightedResidual[i];\n        }\n\n        // compute Qt.res\n        qTy(qtf);\n\n        // now we don't need Q anymore,\n        // so let jacobian contain the R matrix with its diagonal elements\n        for (int k = 0; k < solvedCols; ++k) {\n            int pk = permutation[k];\n            weightedJacobian[k][pk] = diagR[pk];\n        }\n\n        if (firstIteration) {\n            // scale the point according to the norms of the columns\n            // of the initial jacobian\n            xNorm = 0;\n            for (int k = 0; k < nC; ++k) {\n                double dk = jacNorm[k];\n                if (dk == 0) {\n                    dk = 1.0;\n                }\n                double xk = dk * currentPoint[k];\n                xNorm  += xk * xk;\n                diag[k] = dk;\n            }\n            xNorm = FastMath.sqrt(xNorm);\n\n            // initialize the step bound delta\n            delta = (xNorm == 0) ? initialStepBoundFactor : (initialStepBoundFactor * xNorm);\n        }\n\n        // check orthogonality between function vector and jacobian columns\n        double maxCosine = 0;\n        if (currentCost != 0) {\n            for (int j = 0; j < solvedCols; ++j) {\n                int    pj = permutation[j];\n                double s  = jacNorm[pj];\n                if (s != 0) {\n                    double sum = 0;\n                    for (int i = 0; i <= j; ++i) {\n                        sum += weightedJacobian[i][pj] * qtf[i];\n                    }\n                    maxCosine = FastMath.max(maxCosine, FastMath.abs(sum) / (s * currentCost));\n                }\n            }\n        }\n        if (maxCosine <= orthoTolerance) {\n            // Convergence has been reached.\n            setCost(currentCost);\n            return new PointVectorValuePair(current.getPointRef(), current.getValueRef(), true);\n        }\n\n        // rescale if necessary\n        for (int j = 0; j < nC; ++j) {\n            diag[j] = FastMath.max(diag[j], jacNorm[j]);\n        }\n\n        // Inner loop.\n        for (double ratio = 0; ; ) {\n\n            // save the state\n            for (int j = 0; j < solvedCols; ++j) {\n                int pj = permutation[j];\n                oldX[pj] = currentPoint[pj];\n            }\n            final double previousCost = currentCost;\n            double[] tmpVec = weightedResidual;\n            weightedResidual = oldRes;\n            oldRes    = tmpVec;\n            tmpVec    = currentObjective;\n            currentObjective = oldObj;\n            oldObj    = tmpVec;\n\n            // determine the Levenberg-Marquardt parameter\n            determineLMParameter(qtf, delta, diag, work1, work2, work3);\n\n            // compute the new point and the norm of the evolution direction\n            double lmNorm = 0;\n            for (int j = 0; j < solvedCols; ++j) {\n                int pj = permutation[j];\n                lmDir[pj] = -lmDir[pj];\n                currentPoint[pj] = oldX[pj] + lmDir[pj];\n                double s = diag[pj] * lmDir[pj];\n                lmNorm  += s * s;\n            }\n            lmNorm = FastMath.sqrt(lmNorm);\n            // on the first iteration, adjust the initial step bound.\n            if (firstIteration) {\n                delta = FastMath.min(delta, lmNorm);\n            }\n\n            // Evaluate the function at x + p and calculate its norm.\n            currentObjective = computeObjectiveValue(currentPoint);\n            currentResiduals = computeResiduals(currentObjective);\n            current = new PointVectorValuePair(currentPoint, currentObjective);\n            currentCost = computeCost(currentResiduals);\n\n            // compute the scaled actual reduction\n            double actRed = -1.0;\n            if (0.1 * currentCost < previousCost) {\n                double r = currentCost / previousCost;\n                actRed = 1.0 - r * r;\n            }\n\n            // compute the scaled predicted reduction\n            // and the scaled directional derivative\n            for (int j = 0; j < solvedCols; ++j) {\n                int pj = permutation[j];\n                double dirJ = lmDir[pj];\n                work1[j] = 0;\n                for (int i = 0; i <= j; ++i) {\n                    work1[i] += weightedJacobian[i][pj] * dirJ;\n                }\n            }\n            double coeff1 = 0;\n            for (int j = 0; j < solvedCols; ++j) {\n                coeff1 += work1[j] * work1[j];\n            }\n            double pc2 = previousCost * previousCost;\n            coeff1 = coeff1 / pc2;\n            double coeff2 = lmPar * lmNorm * lmNorm / pc2;\n            double preRed = coeff1 + 2 * coeff2;\n            double dirDer = -(coeff1 + coeff2);\n\n            // ratio of the actual to the predicted reduction\n            ratio = (preRed == 0) ? 0 : (actRed / preRed);\n\n            // update the step bound\n            if (ratio <= 0.25) {\n                double tmp =\n                    (actRed < 0) ? (0.5 * dirDer / (dirDer + 0.5 * actRed)) : 0.5;\n                    if ((0.1 * currentCost >= previousCost) || (tmp < 0.1)) {\n                        tmp = 0.1;\n                    }\n                    delta = tmp * FastMath.min(delta, 10.0 * lmNorm);\n                    lmPar /= tmp;\n            } else if ((lmPar == 0) || (ratio >= 0.75)) {\n                delta = 2 * lmNorm;\n                lmPar *= 0.5;\n            }\n\n            // test for successful iteration.\n            if (ratio >= 1.0e-4) {\n                // successful iteration, update the norm\n                firstIteration = false;\n                xNorm = 0;\n                for (int k = 0; k < nC; ++k) {\n                    double xK = diag[k] * currentPoint[k];\n                    xNorm += xK * xK;\n                }\n                xNorm = FastMath.sqrt(xNorm);\n\n                // tests for convergence.\n                if (checker != null) {\n                    // we use the vectorial convergence checker\n                    if (checker.converged(iter, previous, current)) {\n                        setCost(currentCost);\n                        return new PointVectorValuePair(current.getPointRef(), current.getValueRef(), true);\n                    }\n                }\n            } else {\n                // failed iteration, reset the previous values\n                currentCost = previousCost;\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    currentPoint[pj] = oldX[pj];\n                }\n                tmpVec    = weightedResidual;\n                weightedResidual = oldRes;\n                oldRes    = tmpVec;\n                tmpVec    = currentObjective;\n                currentObjective = oldObj;\n                oldObj    = tmpVec;\n                // Reset \"current\" to previous values.\n                current = new PointVectorValuePair(currentPoint, currentObjective);\n            }\n\n            // Default convergence criteria.\n            if ((FastMath.abs(actRed) <= costRelativeTolerance &&\n                 preRed <= costRelativeTolerance &&\n                 ratio <= 2.0) ||\n                delta <= parRelativeTolerance * xNorm) {\n                setCost(currentCost);\n                return new PointVectorValuePair(current.getPointRef(), current.getValueRef(), true);\n            }\n\n            // tests for termination and stringent tolerances\n            // (2.2204e-16 is the machine epsilon for IEEE754)\n            if ((FastMath.abs(actRed) <= 2.2204e-16) && (preRed <= 2.2204e-16) && (ratio <= 2.0)) {\n                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_COST_RELATIVE_TOLERANCE,\n                                               costRelativeTolerance);\n            } else if (delta <= 2.2204e-16 * xNorm) {\n                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE,\n                                               parRelativeTolerance);\n            } else if (maxCosine <= 2.2204e-16)  {\n                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_ORTHOGONALITY_TOLERANCE,\n                                               orthoTolerance);\n            }\n        }\n    }\n}\n"
                        ]
                    }
                }
            }
        ],
        "trigger_test": {
            "org.apache.commons.math3.optim.nonlinear.scalar.gradient.NonLinearConjugateGradientOptimizerTest::testTrivial": {
                "path": "apache/commons/math3/optim/nonlinear/vector/jacobian/AbstractLeastSquaresOptimizerAbstractTest.java",
                "function_name": "testTrivial",
                "src": "    public void testTrivial() {\n        LinearProblem problem\n            = new LinearProblem(new double[][] { { 2 } }, new double[] { 3 });\n        AbstractLeastSquaresOptimizer optimizer = createOptimizer();\n        PointVectorValuePair optimum =\n            optimizer.optimize(new MaxEval(100),\n                               problem.getModelFunction(),\n                               problem.getModelFunctionJacobian(),\n                               problem.getTarget(),\n                               new Weight(new double[] { 1 }),\n                               new InitialGuess(new double[] { 0 }));\n        Assert.assertEquals(0, optimizer.getRMS(), 1e-10);\n        Assert.assertEquals(1.5, optimum.getPoint()[0], 1e-10);\n        Assert.assertEquals(3.0, optimum.getValue()[0], 1e-10);\n    }",
                "error_msg": "junit.framework.AssertionFailedError\n\tat org.junit.Assert.fail(Assert.java:86)\n\tat org.junit.Assert.assertTrue(Assert.java:41)\n\tat org.junit.Assert.assertTrue(Assert.java:52)\n\tat org.apache.commons.math3.optim.nonlinear.scalar.gradient.NonLinearConjugateGradientOptimizerTest.testTrivial(NonLinearConjugateGradientOptimizerTest.java:141)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n\tat junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:38)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:520)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeInVM(JUnitTask.java:1484)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:872)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeOrQueue(JUnitTask.java:1972)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute1(JUnitTask.java:824)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:2277)\n\tat org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)\n\tat org.apache.tools.ant.Task.perform(Task.java:348)\n\tat org.apache.tools.ant.Target.execute(Target.java:392)\n\tat org.apache.tools.ant.Target.performTasks(Target.java:413)\n\tat org.apache.tools.ant.Project.executeSortedTargets(Project.java:1399)\n\tat org.apache.tools.ant.Project.executeTarget(Project.java:1368)\n\tat org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41)\n\tat org.apache.tools.ant.Project.executeTargets(Project.java:1251)\n\tat org.apache.tools.ant.Main.runBuild(Main.java:811)\n\tat org.apache.tools.ant.Main.startAnt(Main.java:217)\n\tat org.apache.tools.ant.launch.Launcher.run(Launcher.java:280)\n\tat org.apache.tools.ant.launch.Launcher.main(Launcher.java:109)",
                "clean_error_msg": "junit.framework.AssertionFailedError\n\tat org.apache.commons.math3.optim.nonlinear.scalar.gradient.NonLinearConjugateGradientOptimizerTest.testTrivial(NonLinearConjugateGradientOptimizerTest.java:141)  Assert.assertTrue(optimizer.getIterations() > 0);"
            },
            "org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest::testConstrainedRosen": {
                "path": "apache/commons/math3/optim/nonlinear/scalar/noderiv/CMAESOptimizerTest.java",
                "function_name": "testConstrainedRosen",
                "src": "    public void testConstrainedRosen() {\n        double[] startPoint = point(DIM, 0.1);\n        double[] insigma = point(DIM, 0.1);\n        double[][] boundaries = boundaries(DIM, -1, 2);\n        PointValuePair expected =\n            new PointValuePair(point(DIM,1.0),0.0);\n        doTest(new Rosen(), startPoint, insigma, boundaries,\n                GoalType.MINIMIZE, 2*LAMBDA, true, 0, 1e-13,\n                1e-13, 1e-6, 100000, expected);\n        doTest(new Rosen(), startPoint, insigma, boundaries,\n                GoalType.MINIMIZE, 2*LAMBDA, false, 0, 1e-13,\n                1e-13, 1e-6, 100000, expected);\n    }",
                "error_msg": "junit.framework.AssertionFailedError\n\tat org.junit.Assert.fail(Assert.java:86)\n\tat org.junit.Assert.assertTrue(Assert.java:41)\n\tat org.junit.Assert.assertTrue(Assert.java:52)\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.doTest(CMAESOptimizerTest.java:514)\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.testConstrainedRosen(CMAESOptimizerTest.java:348)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.apache.commons.math3.RetryRunner$1.evaluate(RetryRunner.java:60)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n\tat junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:38)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:520)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeInVM(JUnitTask.java:1484)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:872)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeOrQueue(JUnitTask.java:1972)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute1(JUnitTask.java:824)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:2277)\n\tat org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)\n\tat org.apache.tools.ant.Task.perform(Task.java:348)\n\tat org.apache.tools.ant.Target.execute(Target.java:392)\n\tat org.apache.tools.ant.Target.performTasks(Target.java:413)\n\tat org.apache.tools.ant.Project.executeSortedTargets(Project.java:1399)\n\tat org.apache.tools.ant.Project.executeTarget(Project.java:1368)\n\tat org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41)\n\tat org.apache.tools.ant.Project.executeTargets(Project.java:1251)\n\tat org.apache.tools.ant.Main.runBuild(Main.java:811)\n\tat org.apache.tools.ant.Main.startAnt(Main.java:217)\n\tat org.apache.tools.ant.launch.Launcher.run(Launcher.java:280)\n\tat org.apache.tools.ant.launch.Launcher.main(Launcher.java:109)",
                "clean_error_msg": "junit.framework.AssertionFailedError\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.doTest(CMAESOptimizerTest.java:514)  Assert.assertTrue(optim.getIterations() > 0);\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.testConstrainedRosen(CMAESOptimizerTest.java:348)  doTest(new Rosen(), startPoint, insigma, boundaries,                GoalType.MINIMIZE, 2*LAMBDA, true, 0, 1e-13,                1e-13, 1e-6, 100000, expected);"
            },
            "org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest::testElliRotated": {
                "path": "apache/commons/math3/optim/nonlinear/scalar/noderiv/CMAESOptimizerTest.java",
                "function_name": "testElliRotated",
                "src": "    public void testElliRotated() {\n        double[] startPoint = point(DIM,1.0);\n        double[] insigma = point(DIM,0.1);\n        double[][] boundaries = null;\n        PointValuePair expected =\n            new PointValuePair(point(DIM,0.0),0.0);\n        doTest(new ElliRotated(), startPoint, insigma, boundaries,\n                GoalType.MINIMIZE, LAMBDA, true, 0, 1e-13,\n                1e-13, 1e-6, 100000, expected);\n        doTest(new ElliRotated(), startPoint, insigma, boundaries,\n                GoalType.MINIMIZE, LAMBDA, false, 0, 1e-13,\n                1e-13, 1e-6, 100000, expected);\n    }",
                "error_msg": "junit.framework.AssertionFailedError\n\tat org.junit.Assert.fail(Assert.java:86)\n\tat org.junit.Assert.assertTrue(Assert.java:41)\n\tat org.junit.Assert.assertTrue(Assert.java:52)\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.doTest(CMAESOptimizerTest.java:514)\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.testElliRotated(CMAESOptimizerTest.java:183)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.apache.commons.math3.RetryRunner$1.evaluate(RetryRunner.java:60)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n\tat junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:38)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:520)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeInVM(JUnitTask.java:1484)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:872)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeOrQueue(JUnitTask.java:1972)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute1(JUnitTask.java:824)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:2277)\n\tat org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)\n\tat org.apache.tools.ant.Task.perform(Task.java:348)\n\tat org.apache.tools.ant.Target.execute(Target.java:392)\n\tat org.apache.tools.ant.Target.performTasks(Target.java:413)\n\tat org.apache.tools.ant.Project.executeSortedTargets(Project.java:1399)\n\tat org.apache.tools.ant.Project.executeTarget(Project.java:1368)\n\tat org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41)\n\tat org.apache.tools.ant.Project.executeTargets(Project.java:1251)\n\tat org.apache.tools.ant.Main.runBuild(Main.java:811)\n\tat org.apache.tools.ant.Main.startAnt(Main.java:217)\n\tat org.apache.tools.ant.launch.Launcher.run(Launcher.java:280)\n\tat org.apache.tools.ant.launch.Launcher.main(Launcher.java:109)",
                "clean_error_msg": "junit.framework.AssertionFailedError\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.doTest(CMAESOptimizerTest.java:514)  Assert.assertTrue(optim.getIterations() > 0);\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.testElliRotated(CMAESOptimizerTest.java:183)  doTest(new ElliRotated(), startPoint, insigma, boundaries,                GoalType.MINIMIZE, LAMBDA, true, 0, 1e-13,                1e-13, 1e-6, 100000, expected);"
            },
            "org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest::testEllipse": {
                "path": "apache/commons/math3/optim/nonlinear/scalar/noderiv/CMAESOptimizerTest.java",
                "function_name": "testEllipse",
                "src": "    public void testEllipse() {\n        double[] startPoint = point(DIM,1.0);\n        double[] insigma = point(DIM,0.1);\n        double[][] boundaries = null;\n        PointValuePair expected =\n            new PointValuePair(point(DIM,0.0),0.0);\n        doTest(new Elli(), startPoint, insigma, boundaries,\n                GoalType.MINIMIZE, LAMBDA, true, 0, 1e-13,\n                1e-13, 1e-6, 100000, expected);\n        doTest(new Elli(), startPoint, insigma, boundaries,\n                GoalType.MINIMIZE, LAMBDA, false, 0, 1e-13,\n                1e-13, 1e-6, 100000, expected);\n    }",
                "error_msg": "junit.framework.AssertionFailedError\n\tat org.junit.Assert.fail(Assert.java:86)\n\tat org.junit.Assert.assertTrue(Assert.java:41)\n\tat org.junit.Assert.assertTrue(Assert.java:52)\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.doTest(CMAESOptimizerTest.java:514)\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.testEllipse(CMAESOptimizerTest.java:168)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.apache.commons.math3.RetryRunner$1.evaluate(RetryRunner.java:60)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n\tat junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:38)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:520)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeInVM(JUnitTask.java:1484)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:872)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeOrQueue(JUnitTask.java:1972)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute1(JUnitTask.java:824)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:2277)\n\tat org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)\n\tat org.apache.tools.ant.Task.perform(Task.java:348)\n\tat org.apache.tools.ant.Target.execute(Target.java:392)\n\tat org.apache.tools.ant.Target.performTasks(Target.java:413)\n\tat org.apache.tools.ant.Project.executeSortedTargets(Project.java:1399)\n\tat org.apache.tools.ant.Project.executeTarget(Project.java:1368)\n\tat org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41)\n\tat org.apache.tools.ant.Project.executeTargets(Project.java:1251)\n\tat org.apache.tools.ant.Main.runBuild(Main.java:811)\n\tat org.apache.tools.ant.Main.startAnt(Main.java:217)\n\tat org.apache.tools.ant.launch.Launcher.run(Launcher.java:280)\n\tat org.apache.tools.ant.launch.Launcher.main(Launcher.java:109)",
                "clean_error_msg": "junit.framework.AssertionFailedError\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.doTest(CMAESOptimizerTest.java:514)  Assert.assertTrue(optim.getIterations() > 0);\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.testEllipse(CMAESOptimizerTest.java:168)  doTest(new Elli(), startPoint, insigma, boundaries,                GoalType.MINIMIZE, LAMBDA, true, 0, 1e-13,                1e-13, 1e-6, 100000, expected);"
            },
            "org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest::testTwoAxes": {
                "path": "apache/commons/math3/optim/nonlinear/scalar/noderiv/CMAESOptimizerTest.java",
                "function_name": "testTwoAxes",
                "src": "    public void testTwoAxes() {\n        double[] startPoint = point(DIM,1.0);\n        double[] insigma = point(DIM,0.1);\n        double[][] boundaries = null;\n        PointValuePair expected =\n            new PointValuePair(point(DIM,0.0),0.0);\n        doTest(new TwoAxes(), startPoint, insigma, boundaries,\n                GoalType.MINIMIZE, 2*LAMBDA, true, 0, 1e-13,\n                1e-13, 1e-6, 200000, expected);\n        doTest(new TwoAxes(), startPoint, insigma, boundaries,\n                GoalType.MINIMIZE, 2*LAMBDA, false, 0, 1e-13,\n                1e-8, 1e-3, 200000, expected);\n    }",
                "error_msg": "junit.framework.AssertionFailedError\n\tat org.junit.Assert.fail(Assert.java:86)\n\tat org.junit.Assert.assertTrue(Assert.java:41)\n\tat org.junit.Assert.assertTrue(Assert.java:52)\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.doTest(CMAESOptimizerTest.java:514)\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.testTwoAxes(CMAESOptimizerTest.java:228)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.apache.commons.math3.RetryRunner$1.evaluate(RetryRunner.java:60)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n\tat junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:38)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:520)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeInVM(JUnitTask.java:1484)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:872)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeOrQueue(JUnitTask.java:1972)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute1(JUnitTask.java:824)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:2277)\n\tat org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)\n\tat org.apache.tools.ant.Task.perform(Task.java:348)\n\tat org.apache.tools.ant.Target.execute(Target.java:392)\n\tat org.apache.tools.ant.Target.performTasks(Target.java:413)\n\tat org.apache.tools.ant.Project.executeSortedTargets(Project.java:1399)\n\tat org.apache.tools.ant.Project.executeTarget(Project.java:1368)\n\tat org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41)\n\tat org.apache.tools.ant.Project.executeTargets(Project.java:1251)\n\tat org.apache.tools.ant.Main.runBuild(Main.java:811)\n\tat org.apache.tools.ant.Main.startAnt(Main.java:217)\n\tat org.apache.tools.ant.launch.Launcher.run(Launcher.java:280)\n\tat org.apache.tools.ant.launch.Launcher.main(Launcher.java:109)",
                "clean_error_msg": "junit.framework.AssertionFailedError\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.doTest(CMAESOptimizerTest.java:514)  Assert.assertTrue(optim.getIterations() > 0);\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.testTwoAxes(CMAESOptimizerTest.java:228)  doTest(new TwoAxes(), startPoint, insigma, boundaries,                GoalType.MINIMIZE, 2*LAMBDA, true, 0, 1e-13,                1e-13, 1e-6, 200000, expected);"
            },
            "org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest::testCigar": {
                "path": "apache/commons/math3/optim/nonlinear/scalar/noderiv/CMAESOptimizerTest.java",
                "function_name": "testCigar",
                "src": "    public void testCigar() {\n        double[] startPoint = point(DIM,1.0);\n        double[] insigma = point(DIM,0.1);\n        double[][] boundaries = null;\n        PointValuePair expected =\n            new PointValuePair(point(DIM,0.0),0.0);\n        doTest(new Cigar(), startPoint, insigma, boundaries,\n                GoalType.MINIMIZE, LAMBDA, true, 0, 1e-13,\n                1e-13, 1e-6, 200000, expected);\n        doTest(new Cigar(), startPoint, insigma, boundaries,\n                GoalType.MINIMIZE, LAMBDA, false, 0, 1e-13,\n                1e-13, 1e-6, 100000, expected);\n    }",
                "error_msg": "junit.framework.AssertionFailedError\n\tat org.junit.Assert.fail(Assert.java:86)\n\tat org.junit.Assert.assertTrue(Assert.java:41)\n\tat org.junit.Assert.assertTrue(Assert.java:52)\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.doTest(CMAESOptimizerTest.java:514)\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.testCigar(CMAESOptimizerTest.java:198)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.apache.commons.math3.RetryRunner$1.evaluate(RetryRunner.java:60)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n\tat junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:38)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:520)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeInVM(JUnitTask.java:1484)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:872)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeOrQueue(JUnitTask.java:1972)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute1(JUnitTask.java:824)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:2277)\n\tat org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)\n\tat org.apache.tools.ant.Task.perform(Task.java:348)\n\tat org.apache.tools.ant.Target.execute(Target.java:392)\n\tat org.apache.tools.ant.Target.performTasks(Target.java:413)\n\tat org.apache.tools.ant.Project.executeSortedTargets(Project.java:1399)\n\tat org.apache.tools.ant.Project.executeTarget(Project.java:1368)\n\tat org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41)\n\tat org.apache.tools.ant.Project.executeTargets(Project.java:1251)\n\tat org.apache.tools.ant.Main.runBuild(Main.java:811)\n\tat org.apache.tools.ant.Main.startAnt(Main.java:217)\n\tat org.apache.tools.ant.launch.Launcher.run(Launcher.java:280)\n\tat org.apache.tools.ant.launch.Launcher.main(Launcher.java:109)",
                "clean_error_msg": "junit.framework.AssertionFailedError\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.doTest(CMAESOptimizerTest.java:514)  Assert.assertTrue(optim.getIterations() > 0);\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.testCigar(CMAESOptimizerTest.java:198)  doTest(new Cigar(), startPoint, insigma, boundaries,                GoalType.MINIMIZE, LAMBDA, true, 0, 1e-13,                1e-13, 1e-6, 200000, expected);"
            },
            "org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest::testRosen": {
                "path": "apache/commons/math3/optim/nonlinear/scalar/noderiv/CMAESOptimizerTest.java",
                "function_name": "testRosen",
                "src": "    public void testRosen() {\n        double[] startPoint = point(DIM,0.1);\n        double[] insigma = point(DIM,0.1);\n        double[][] boundaries = null;\n        PointValuePair expected =\n            new PointValuePair(point(DIM,1.0),0.0);\n        doTest(new Rosen(), startPoint, insigma, boundaries,\n                GoalType.MINIMIZE, LAMBDA, true, 0, 1e-13,\n                1e-13, 1e-6, 100000, expected);\n        doTest(new Rosen(), startPoint, insigma, boundaries,\n                GoalType.MINIMIZE, LAMBDA, false, 0, 1e-13,\n                1e-13, 1e-6, 100000, expected);\n    }",
                "error_msg": "junit.framework.AssertionFailedError\n\tat org.junit.Assert.fail(Assert.java:86)\n\tat org.junit.Assert.assertTrue(Assert.java:41)\n\tat org.junit.Assert.assertTrue(Assert.java:52)\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.doTest(CMAESOptimizerTest.java:514)\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.testRosen(CMAESOptimizerTest.java:132)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.apache.commons.math3.RetryRunner$1.evaluate(RetryRunner.java:67)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n\tat junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:38)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:520)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeInVM(JUnitTask.java:1484)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:872)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeOrQueue(JUnitTask.java:1972)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute1(JUnitTask.java:824)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:2277)\n\tat org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)\n\tat org.apache.tools.ant.Task.perform(Task.java:348)\n\tat org.apache.tools.ant.Target.execute(Target.java:392)\n\tat org.apache.tools.ant.Target.performTasks(Target.java:413)\n\tat org.apache.tools.ant.Project.executeSortedTargets(Project.java:1399)\n\tat org.apache.tools.ant.Project.executeTarget(Project.java:1368)\n\tat org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41)\n\tat org.apache.tools.ant.Project.executeTargets(Project.java:1251)\n\tat org.apache.tools.ant.Main.runBuild(Main.java:811)\n\tat org.apache.tools.ant.Main.startAnt(Main.java:217)\n\tat org.apache.tools.ant.launch.Launcher.run(Launcher.java:280)\n\tat org.apache.tools.ant.launch.Launcher.main(Launcher.java:109)",
                "clean_error_msg": "junit.framework.AssertionFailedError\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.doTest(CMAESOptimizerTest.java:514)  Assert.assertTrue(optim.getIterations() > 0);\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.testRosen(CMAESOptimizerTest.java:132)  doTest(new Rosen(), startPoint, insigma, boundaries,                GoalType.MINIMIZE, LAMBDA, true, 0, 1e-13,                1e-13, 1e-6, 100000, expected);"
            },
            "org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest::testRastrigin": {
                "path": "apache/commons/math3/optim/nonlinear/scalar/noderiv/CMAESOptimizerTest.java",
                "function_name": "testRastrigin",
                "src": "    public void testRastrigin() {\n        double[] startPoint = point(DIM,0.1);\n        double[] insigma = point(DIM,0.1);\n        double[][] boundaries = null;\n        PointValuePair expected =\n            new PointValuePair(point(DIM,0.0),0.0);\n        doTest(new Rastrigin(), startPoint, insigma, boundaries,\n                GoalType.MINIMIZE, (int)(200*Math.sqrt(DIM)), true, 0, 1e-13,\n                1e-13, 1e-6, 200000, expected);\n        doTest(new Rastrigin(), startPoint, insigma, boundaries,\n                GoalType.MINIMIZE, (int)(200*Math.sqrt(DIM)), false, 0, 1e-13,\n                1e-13, 1e-6, 200000, expected);\n    }",
                "error_msg": "junit.framework.AssertionFailedError\n\tat org.junit.Assert.fail(Assert.java:86)\n\tat org.junit.Assert.assertTrue(Assert.java:41)\n\tat org.junit.Assert.assertTrue(Assert.java:52)\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.doTest(CMAESOptimizerTest.java:514)\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.testRastrigin(CMAESOptimizerTest.java:333)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.apache.commons.math3.RetryRunner$1.evaluate(RetryRunner.java:60)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n\tat junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:38)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:520)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeInVM(JUnitTask.java:1484)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:872)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeOrQueue(JUnitTask.java:1972)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute1(JUnitTask.java:824)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:2277)\n\tat org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)\n\tat org.apache.tools.ant.Task.perform(Task.java:348)\n\tat org.apache.tools.ant.Target.execute(Target.java:392)\n\tat org.apache.tools.ant.Target.performTasks(Target.java:413)\n\tat org.apache.tools.ant.Project.executeSortedTargets(Project.java:1399)\n\tat org.apache.tools.ant.Project.executeTarget(Project.java:1368)\n\tat org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41)\n\tat org.apache.tools.ant.Project.executeTargets(Project.java:1251)\n\tat org.apache.tools.ant.Main.runBuild(Main.java:811)\n\tat org.apache.tools.ant.Main.startAnt(Main.java:217)\n\tat org.apache.tools.ant.launch.Launcher.run(Launcher.java:280)\n\tat org.apache.tools.ant.launch.Launcher.main(Launcher.java:109)",
                "clean_error_msg": "junit.framework.AssertionFailedError\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.doTest(CMAESOptimizerTest.java:514)  Assert.assertTrue(optim.getIterations() > 0);\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.testRastrigin(CMAESOptimizerTest.java:333)  doTest(new Rastrigin(), startPoint, insigma, boundaries,                GoalType.MINIMIZE, (int)(200*Math.sqrt(DIM)), true, 0, 1e-13,                1e-13, 1e-6, 200000, expected);"
            },
            "org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest::testDiagonalRosen": {
                "path": "apache/commons/math3/optim/nonlinear/scalar/noderiv/CMAESOptimizerTest.java",
                "function_name": "testDiagonalRosen",
                "src": "    public void testDiagonalRosen() {\n        double[] startPoint = point(DIM,0.1);\n        double[] insigma = point(DIM,0.1);\n        double[][] boundaries = null;\n        PointValuePair expected =\n            new PointValuePair(point(DIM,1.0),0.0);\n        doTest(new Rosen(), startPoint, insigma, boundaries,\n                GoalType.MINIMIZE, LAMBDA, false, 1, 1e-13,\n                1e-10, 1e-4, 1000000, expected);\n     }",
                "error_msg": "junit.framework.AssertionFailedError\n\tat org.junit.Assert.fail(Assert.java:86)\n\tat org.junit.Assert.assertTrue(Assert.java:41)\n\tat org.junit.Assert.assertTrue(Assert.java:52)\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.doTest(CMAESOptimizerTest.java:514)\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.testDiagonalRosen(CMAESOptimizerTest.java:363)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.apache.commons.math3.RetryRunner$1.evaluate(RetryRunner.java:60)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n\tat junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:38)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:520)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeInVM(JUnitTask.java:1484)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:872)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeOrQueue(JUnitTask.java:1972)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute1(JUnitTask.java:824)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:2277)\n\tat org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)\n\tat org.apache.tools.ant.Task.perform(Task.java:348)\n\tat org.apache.tools.ant.Target.execute(Target.java:392)\n\tat org.apache.tools.ant.Target.performTasks(Target.java:413)\n\tat org.apache.tools.ant.Project.executeSortedTargets(Project.java:1399)\n\tat org.apache.tools.ant.Project.executeTarget(Project.java:1368)\n\tat org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41)\n\tat org.apache.tools.ant.Project.executeTargets(Project.java:1251)\n\tat org.apache.tools.ant.Main.runBuild(Main.java:811)\n\tat org.apache.tools.ant.Main.startAnt(Main.java:217)\n\tat org.apache.tools.ant.launch.Launcher.run(Launcher.java:280)\n\tat org.apache.tools.ant.launch.Launcher.main(Launcher.java:109)",
                "clean_error_msg": "junit.framework.AssertionFailedError\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.doTest(CMAESOptimizerTest.java:514)  Assert.assertTrue(optim.getIterations() > 0);\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.testDiagonalRosen(CMAESOptimizerTest.java:363)  doTest(new Rosen(), startPoint, insigma, boundaries,                GoalType.MINIMIZE, LAMBDA, false, 1, 1e-13,                1e-10, 1e-4, 1000000, expected);"
            },
            "org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest::testSsDiffPow": {
                "path": "apache/commons/math3/optim/nonlinear/scalar/noderiv/CMAESOptimizerTest.java",
                "function_name": "testSsDiffPow",
                "src": "    public void testSsDiffPow() {\n        double[] startPoint = point(DIM,1.0);\n        double[] insigma = point(DIM,0.1);\n        double[][] boundaries = null;\n        PointValuePair expected =\n            new PointValuePair(point(DIM,0.0),0.0);\n        doTest(new SsDiffPow(), startPoint, insigma, boundaries,\n                GoalType.MINIMIZE, 10, true, 0, 1e-13,\n                1e-4, 1e-1, 200000, expected);\n        doTest(new SsDiffPow(), startPoint, insigma, boundaries,\n                GoalType.MINIMIZE, 10, false, 0, 1e-13,\n                1e-4, 1e-1, 200000, expected);\n    }",
                "error_msg": "junit.framework.AssertionFailedError\n\tat org.junit.Assert.fail(Assert.java:86)\n\tat org.junit.Assert.assertTrue(Assert.java:41)\n\tat org.junit.Assert.assertTrue(Assert.java:52)\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.doTest(CMAESOptimizerTest.java:514)\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.testSsDiffPow(CMAESOptimizerTest.java:303)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.apache.commons.math3.RetryRunner$1.evaluate(RetryRunner.java:60)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n\tat junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:38)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:520)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeInVM(JUnitTask.java:1484)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:872)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeOrQueue(JUnitTask.java:1972)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute1(JUnitTask.java:824)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:2277)\n\tat org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)\n\tat org.apache.tools.ant.Task.perform(Task.java:348)\n\tat org.apache.tools.ant.Target.execute(Target.java:392)\n\tat org.apache.tools.ant.Target.performTasks(Target.java:413)\n\tat org.apache.tools.ant.Project.executeSortedTargets(Project.java:1399)\n\tat org.apache.tools.ant.Project.executeTarget(Project.java:1368)\n\tat org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41)\n\tat org.apache.tools.ant.Project.executeTargets(Project.java:1251)\n\tat org.apache.tools.ant.Main.runBuild(Main.java:811)\n\tat org.apache.tools.ant.Main.startAnt(Main.java:217)\n\tat org.apache.tools.ant.launch.Launcher.run(Launcher.java:280)\n\tat org.apache.tools.ant.launch.Launcher.main(Launcher.java:109)",
                "clean_error_msg": "junit.framework.AssertionFailedError\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.doTest(CMAESOptimizerTest.java:514)  Assert.assertTrue(optim.getIterations() > 0);\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.testSsDiffPow(CMAESOptimizerTest.java:303)  doTest(new SsDiffPow(), startPoint, insigma, boundaries,                GoalType.MINIMIZE, 10, true, 0, 1e-13,                1e-4, 1e-1, 200000, expected);"
            },
            "org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest::testMaximize": {
                "path": "apache/commons/math3/optim/nonlinear/scalar/noderiv/CMAESOptimizerTest.java",
                "function_name": "testMaximize",
                "src": "    public void testMaximize() {\n        double[] startPoint = point(DIM,1.0);\n        double[] insigma = point(DIM,0.1);\n        double[][] boundaries = null;\n        PointValuePair expected =\n            new PointValuePair(point(DIM,0.0),1.0);\n        doTest(new MinusElli(), startPoint, insigma, boundaries,\n                GoalType.MAXIMIZE, LAMBDA, true, 0, 1.0-1e-13,\n                2e-10, 5e-6, 100000, expected);\n        doTest(new MinusElli(), startPoint, insigma, boundaries,\n                GoalType.MAXIMIZE, LAMBDA, false, 0, 1.0-1e-13,\n                2e-10, 5e-6, 100000, expected);\n        boundaries = boundaries(DIM,-0.3,0.3); \n        startPoint = point(DIM,0.1);\n        doTest(new MinusElli(), startPoint, insigma, boundaries,\n                GoalType.MAXIMIZE, LAMBDA, true, 0, 1.0-1e-13,\n                2e-10, 5e-6, 100000, expected);\n    }",
                "error_msg": "junit.framework.AssertionFailedError\n\tat org.junit.Assert.fail(Assert.java:86)\n\tat org.junit.Assert.assertTrue(Assert.java:41)\n\tat org.junit.Assert.assertTrue(Assert.java:52)\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.doTest(CMAESOptimizerTest.java:514)\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.testMaximize(CMAESOptimizerTest.java:148)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.apache.commons.math3.RetryRunner$1.evaluate(RetryRunner.java:67)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n\tat junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:38)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:520)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeInVM(JUnitTask.java:1484)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:872)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeOrQueue(JUnitTask.java:1972)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute1(JUnitTask.java:824)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:2277)\n\tat org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)\n\tat org.apache.tools.ant.Task.perform(Task.java:348)\n\tat org.apache.tools.ant.Target.execute(Target.java:392)\n\tat org.apache.tools.ant.Target.performTasks(Target.java:413)\n\tat org.apache.tools.ant.Project.executeSortedTargets(Project.java:1399)\n\tat org.apache.tools.ant.Project.executeTarget(Project.java:1368)\n\tat org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41)\n\tat org.apache.tools.ant.Project.executeTargets(Project.java:1251)\n\tat org.apache.tools.ant.Main.runBuild(Main.java:811)\n\tat org.apache.tools.ant.Main.startAnt(Main.java:217)\n\tat org.apache.tools.ant.launch.Launcher.run(Launcher.java:280)\n\tat org.apache.tools.ant.launch.Launcher.main(Launcher.java:109)",
                "clean_error_msg": "junit.framework.AssertionFailedError\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.doTest(CMAESOptimizerTest.java:514)  Assert.assertTrue(optim.getIterations() > 0);\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.testMaximize(CMAESOptimizerTest.java:148)  doTest(new MinusElli(), startPoint, insigma, boundaries,                GoalType.MAXIMIZE, LAMBDA, true, 0, 1.0-1e-13,                2e-10, 5e-6, 100000, expected);"
            },
            "org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest::testAckley": {
                "path": "apache/commons/math3/optim/nonlinear/scalar/noderiv/CMAESOptimizerTest.java",
                "function_name": "testAckley",
                "src": "    public void testAckley() {\n        double[] startPoint = point(DIM,1.0);\n        double[] insigma = point(DIM,1.0);\n        double[][] boundaries = null;\n        PointValuePair expected =\n            new PointValuePair(point(DIM,0.0),0.0);\n        doTest(new Ackley(), startPoint, insigma, boundaries,\n                GoalType.MINIMIZE, 2*LAMBDA, true, 0, 1e-13,\n                1e-9, 1e-5, 100000, expected);\n        doTest(new Ackley(), startPoint, insigma, boundaries,\n                GoalType.MINIMIZE, 2*LAMBDA, false, 0, 1e-13,\n                1e-9, 1e-5, 100000, expected);\n    }",
                "error_msg": "junit.framework.AssertionFailedError\n\tat org.junit.Assert.fail(Assert.java:86)\n\tat org.junit.Assert.assertTrue(Assert.java:41)\n\tat org.junit.Assert.assertTrue(Assert.java:52)\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.doTest(CMAESOptimizerTest.java:514)\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.testAckley(CMAESOptimizerTest.java:318)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.apache.commons.math3.RetryRunner$1.evaluate(RetryRunner.java:60)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n\tat junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:38)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:520)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeInVM(JUnitTask.java:1484)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:872)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeOrQueue(JUnitTask.java:1972)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute1(JUnitTask.java:824)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:2277)\n\tat org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)\n\tat org.apache.tools.ant.Task.perform(Task.java:348)\n\tat org.apache.tools.ant.Target.execute(Target.java:392)\n\tat org.apache.tools.ant.Target.performTasks(Target.java:413)\n\tat org.apache.tools.ant.Project.executeSortedTargets(Project.java:1399)\n\tat org.apache.tools.ant.Project.executeTarget(Project.java:1368)\n\tat org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41)\n\tat org.apache.tools.ant.Project.executeTargets(Project.java:1251)\n\tat org.apache.tools.ant.Main.runBuild(Main.java:811)\n\tat org.apache.tools.ant.Main.startAnt(Main.java:217)\n\tat org.apache.tools.ant.launch.Launcher.run(Launcher.java:280)\n\tat org.apache.tools.ant.launch.Launcher.main(Launcher.java:109)",
                "clean_error_msg": "junit.framework.AssertionFailedError\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.doTest(CMAESOptimizerTest.java:514)  Assert.assertTrue(optim.getIterations() > 0);\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.testAckley(CMAESOptimizerTest.java:318)  doTest(new Ackley(), startPoint, insigma, boundaries,                GoalType.MINIMIZE, 2*LAMBDA, true, 0, 1e-13,                1e-9, 1e-5, 100000, expected);"
            },
            "org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest::testCigTab": {
                "path": "apache/commons/math3/optim/nonlinear/scalar/noderiv/CMAESOptimizerTest.java",
                "function_name": "testCigTab",
                "src": "    public void testCigTab() {\n        double[] startPoint = point(DIM,1.0);\n        double[] insigma = point(DIM,0.3);\n        double[][] boundaries = null;\n        PointValuePair expected =\n            new PointValuePair(point(DIM,0.0),0.0);\n        doTest(new CigTab(), startPoint, insigma, boundaries,\n                GoalType.MINIMIZE, LAMBDA, true, 0, 1e-13,\n                1e-13, 5e-5, 100000, expected);\n        doTest(new CigTab(), startPoint, insigma, boundaries,\n                GoalType.MINIMIZE, LAMBDA, false, 0, 1e-13,\n                1e-13, 5e-5, 100000, expected);\n    }",
                "error_msg": "junit.framework.AssertionFailedError\n\tat org.junit.Assert.fail(Assert.java:86)\n\tat org.junit.Assert.assertTrue(Assert.java:41)\n\tat org.junit.Assert.assertTrue(Assert.java:52)\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.doTest(CMAESOptimizerTest.java:514)\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.testCigTab(CMAESOptimizerTest.java:243)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.apache.commons.math3.RetryRunner$1.evaluate(RetryRunner.java:60)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n\tat junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:38)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:520)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeInVM(JUnitTask.java:1484)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:872)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeOrQueue(JUnitTask.java:1972)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute1(JUnitTask.java:824)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:2277)\n\tat org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)\n\tat org.apache.tools.ant.Task.perform(Task.java:348)\n\tat org.apache.tools.ant.Target.execute(Target.java:392)\n\tat org.apache.tools.ant.Target.performTasks(Target.java:413)\n\tat org.apache.tools.ant.Project.executeSortedTargets(Project.java:1399)\n\tat org.apache.tools.ant.Project.executeTarget(Project.java:1368)\n\tat org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41)\n\tat org.apache.tools.ant.Project.executeTargets(Project.java:1251)\n\tat org.apache.tools.ant.Main.runBuild(Main.java:811)\n\tat org.apache.tools.ant.Main.startAnt(Main.java:217)\n\tat org.apache.tools.ant.launch.Launcher.run(Launcher.java:280)\n\tat org.apache.tools.ant.launch.Launcher.main(Launcher.java:109)",
                "clean_error_msg": "junit.framework.AssertionFailedError\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.doTest(CMAESOptimizerTest.java:514)  Assert.assertTrue(optim.getIterations() > 0);\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.testCigTab(CMAESOptimizerTest.java:243)  doTest(new CigTab(), startPoint, insigma, boundaries,                GoalType.MINIMIZE, LAMBDA, true, 0, 1e-13,                1e-13, 5e-5, 100000, expected);"
            },
            "org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest::testDiffPow": {
                "path": "apache/commons/math3/optim/nonlinear/scalar/noderiv/CMAESOptimizerTest.java",
                "function_name": "testDiffPow",
                "src": "    public void testDiffPow() {\n        double[] startPoint = point(DIM,1.0);\n        double[] insigma = point(DIM,0.1);\n        double[][] boundaries = null;\n        PointValuePair expected =\n            new PointValuePair(point(DIM,0.0),0.0);\n        doTest(new DiffPow(), startPoint, insigma, boundaries,\n                GoalType.MINIMIZE, 10, true, 0, 1e-13,\n                1e-8, 1e-1, 100000, expected);\n        doTest(new DiffPow(), startPoint, insigma, boundaries,\n                GoalType.MINIMIZE, 10, false, 0, 1e-13,\n                1e-8, 2e-1, 100000, expected);\n    }",
                "error_msg": "junit.framework.AssertionFailedError\n\tat org.junit.Assert.fail(Assert.java:86)\n\tat org.junit.Assert.assertTrue(Assert.java:41)\n\tat org.junit.Assert.assertTrue(Assert.java:52)\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.doTest(CMAESOptimizerTest.java:514)\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.testDiffPow(CMAESOptimizerTest.java:288)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.apache.commons.math3.RetryRunner$1.evaluate(RetryRunner.java:60)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n\tat junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:38)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:520)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeInVM(JUnitTask.java:1484)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:872)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeOrQueue(JUnitTask.java:1972)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute1(JUnitTask.java:824)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:2277)\n\tat org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)\n\tat org.apache.tools.ant.Task.perform(Task.java:348)\n\tat org.apache.tools.ant.Target.execute(Target.java:392)\n\tat org.apache.tools.ant.Target.performTasks(Target.java:413)\n\tat org.apache.tools.ant.Project.executeSortedTargets(Project.java:1399)\n\tat org.apache.tools.ant.Project.executeTarget(Project.java:1368)\n\tat org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41)\n\tat org.apache.tools.ant.Project.executeTargets(Project.java:1251)\n\tat org.apache.tools.ant.Main.runBuild(Main.java:811)\n\tat org.apache.tools.ant.Main.startAnt(Main.java:217)\n\tat org.apache.tools.ant.launch.Launcher.run(Launcher.java:280)\n\tat org.apache.tools.ant.launch.Launcher.main(Launcher.java:109)",
                "clean_error_msg": "junit.framework.AssertionFailedError\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.doTest(CMAESOptimizerTest.java:514)  Assert.assertTrue(optim.getIterations() > 0);\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.testDiffPow(CMAESOptimizerTest.java:288)  doTest(new DiffPow(), startPoint, insigma, boundaries,                GoalType.MINIMIZE, 10, true, 0, 1e-13,                1e-8, 1e-1, 100000, expected);"
            },
            "org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest::testSphere": {
                "path": "apache/commons/math3/optim/nonlinear/scalar/noderiv/CMAESOptimizerTest.java",
                "function_name": "testSphere",
                "src": "    public void testSphere() {\n        double[] startPoint = point(DIM,1.0);\n        double[] insigma = point(DIM,0.1);\n        double[][] boundaries = null;\n        PointValuePair expected =\n            new PointValuePair(point(DIM,0.0),0.0);\n        doTest(new Sphere(), startPoint, insigma, boundaries,\n                GoalType.MINIMIZE, LAMBDA, true, 0, 1e-13,\n                1e-13, 1e-6, 100000, expected);\n        doTest(new Sphere(), startPoint, insigma, boundaries,\n                GoalType.MINIMIZE, LAMBDA, false, 0, 1e-13,\n                1e-13, 1e-6, 100000, expected);\n    }",
                "error_msg": "junit.framework.AssertionFailedError\n\tat org.junit.Assert.fail(Assert.java:86)\n\tat org.junit.Assert.assertTrue(Assert.java:41)\n\tat org.junit.Assert.assertTrue(Assert.java:52)\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.doTest(CMAESOptimizerTest.java:514)\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.testSphere(CMAESOptimizerTest.java:258)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.apache.commons.math3.RetryRunner$1.evaluate(RetryRunner.java:60)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n\tat junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:38)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:520)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeInVM(JUnitTask.java:1484)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:872)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeOrQueue(JUnitTask.java:1972)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute1(JUnitTask.java:824)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:2277)\n\tat org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)\n\tat org.apache.tools.ant.Task.perform(Task.java:348)\n\tat org.apache.tools.ant.Target.execute(Target.java:392)\n\tat org.apache.tools.ant.Target.performTasks(Target.java:413)\n\tat org.apache.tools.ant.Project.executeSortedTargets(Project.java:1399)\n\tat org.apache.tools.ant.Project.executeTarget(Project.java:1368)\n\tat org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41)\n\tat org.apache.tools.ant.Project.executeTargets(Project.java:1251)\n\tat org.apache.tools.ant.Main.runBuild(Main.java:811)\n\tat org.apache.tools.ant.Main.startAnt(Main.java:217)\n\tat org.apache.tools.ant.launch.Launcher.run(Launcher.java:280)\n\tat org.apache.tools.ant.launch.Launcher.main(Launcher.java:109)",
                "clean_error_msg": "junit.framework.AssertionFailedError\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.doTest(CMAESOptimizerTest.java:514)  Assert.assertTrue(optim.getIterations() > 0);\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.testSphere(CMAESOptimizerTest.java:258)  doTest(new Sphere(), startPoint, insigma, boundaries,                GoalType.MINIMIZE, LAMBDA, true, 0, 1e-13,                1e-13, 1e-6, 100000, expected);"
            },
            "org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest::testTablet": {
                "path": "apache/commons/math3/optim/nonlinear/scalar/noderiv/CMAESOptimizerTest.java",
                "function_name": "testTablet",
                "src": "    public void testTablet() {\n        double[] startPoint = point(DIM,1.0);\n        double[] insigma = point(DIM,0.1);\n        double[][] boundaries = null;\n        PointValuePair expected =\n            new PointValuePair(point(DIM,0.0),0.0);\n        doTest(new Tablet(), startPoint, insigma, boundaries,\n                GoalType.MINIMIZE, LAMBDA, true, 0, 1e-13,\n                1e-13, 1e-6, 100000, expected);\n        doTest(new Tablet(), startPoint, insigma, boundaries,\n                GoalType.MINIMIZE, LAMBDA, false, 0, 1e-13,\n                1e-13, 1e-6, 100000, expected);\n    }",
                "error_msg": "junit.framework.AssertionFailedError\n\tat org.junit.Assert.fail(Assert.java:86)\n\tat org.junit.Assert.assertTrue(Assert.java:41)\n\tat org.junit.Assert.assertTrue(Assert.java:52)\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.doTest(CMAESOptimizerTest.java:514)\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.testTablet(CMAESOptimizerTest.java:273)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.apache.commons.math3.RetryRunner$1.evaluate(RetryRunner.java:60)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n\tat junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:38)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:520)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeInVM(JUnitTask.java:1484)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:872)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeOrQueue(JUnitTask.java:1972)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute1(JUnitTask.java:824)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:2277)\n\tat org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)\n\tat org.apache.tools.ant.Task.perform(Task.java:348)\n\tat org.apache.tools.ant.Target.execute(Target.java:392)\n\tat org.apache.tools.ant.Target.performTasks(Target.java:413)\n\tat org.apache.tools.ant.Project.executeSortedTargets(Project.java:1399)\n\tat org.apache.tools.ant.Project.executeTarget(Project.java:1368)\n\tat org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41)\n\tat org.apache.tools.ant.Project.executeTargets(Project.java:1251)\n\tat org.apache.tools.ant.Main.runBuild(Main.java:811)\n\tat org.apache.tools.ant.Main.startAnt(Main.java:217)\n\tat org.apache.tools.ant.launch.Launcher.run(Launcher.java:280)\n\tat org.apache.tools.ant.launch.Launcher.main(Launcher.java:109)",
                "clean_error_msg": "junit.framework.AssertionFailedError\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.doTest(CMAESOptimizerTest.java:514)  Assert.assertTrue(optim.getIterations() > 0);\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.testTablet(CMAESOptimizerTest.java:273)  doTest(new Tablet(), startPoint, insigma, boundaries,                GoalType.MINIMIZE, LAMBDA, true, 0, 1e-13,                1e-13, 1e-6, 100000, expected);"
            },
            "org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest::testCigarWithBoundaries": {
                "path": "apache/commons/math3/optim/nonlinear/scalar/noderiv/CMAESOptimizerTest.java",
                "function_name": "testCigarWithBoundaries",
                "src": "    public void testCigarWithBoundaries() {\n        double[] startPoint = point(DIM,1.0);\n        double[] insigma = point(DIM,0.1);\n        double[][] boundaries = boundaries(DIM, -1e100, Double.POSITIVE_INFINITY);\n        PointValuePair expected =\n            new PointValuePair(point(DIM,0.0),0.0);\n        doTest(new Cigar(), startPoint, insigma, boundaries,\n                GoalType.MINIMIZE, LAMBDA, true, 0, 1e-13,\n                1e-13, 1e-6, 200000, expected);\n        doTest(new Cigar(), startPoint, insigma, boundaries,\n                GoalType.MINIMIZE, LAMBDA, false, 0, 1e-13,\n                1e-13, 1e-6, 100000, expected);\n    }",
                "error_msg": "junit.framework.AssertionFailedError\n\tat org.junit.Assert.fail(Assert.java:86)\n\tat org.junit.Assert.assertTrue(Assert.java:41)\n\tat org.junit.Assert.assertTrue(Assert.java:52)\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.doTest(CMAESOptimizerTest.java:514)\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.testCigarWithBoundaries(CMAESOptimizerTest.java:213)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.apache.commons.math3.RetryRunner$1.evaluate(RetryRunner.java:60)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n\tat junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:38)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:520)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeInVM(JUnitTask.java:1484)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:872)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeOrQueue(JUnitTask.java:1972)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute1(JUnitTask.java:824)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:2277)\n\tat org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)\n\tat org.apache.tools.ant.Task.perform(Task.java:348)\n\tat org.apache.tools.ant.Target.execute(Target.java:392)\n\tat org.apache.tools.ant.Target.performTasks(Target.java:413)\n\tat org.apache.tools.ant.Project.executeSortedTargets(Project.java:1399)\n\tat org.apache.tools.ant.Project.executeTarget(Project.java:1368)\n\tat org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41)\n\tat org.apache.tools.ant.Project.executeTargets(Project.java:1251)\n\tat org.apache.tools.ant.Main.runBuild(Main.java:811)\n\tat org.apache.tools.ant.Main.startAnt(Main.java:217)\n\tat org.apache.tools.ant.launch.Launcher.run(Launcher.java:280)\n\tat org.apache.tools.ant.launch.Launcher.main(Launcher.java:109)",
                "clean_error_msg": "junit.framework.AssertionFailedError\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.doTest(CMAESOptimizerTest.java:514)  Assert.assertTrue(optim.getIterations() > 0);\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.testCigarWithBoundaries(CMAESOptimizerTest.java:213)  doTest(new Cigar(), startPoint, insigma, boundaries,                GoalType.MINIMIZE, LAMBDA, true, 0, 1e-13,                1e-13, 1e-6, 200000, expected);"
            },
            "org.apache.commons.math3.optim.nonlinear.scalar.noderiv.PowellOptimizerTest::testSumSinc": {
                "path": "apache/commons/math3/optim/nonlinear/scalar/noderiv/PowellOptimizerTest.java",
                "function_name": "testSumSinc",
                "src": "    public void testSumSinc() {\n        final MultivariateFunction func = new SumSincFunction(-1);\n\n        int dim = 2;\n        final double[] minPoint = new double[dim];\n        for (int i = 0; i < dim; i++) {\n            minPoint[i] = 0;\n        }\n\n        double[] init = new double[dim];\n\n        // Initial is minimum.\n        for (int i = 0; i < dim; i++) {\n            init[i] = minPoint[i];\n        }\n        doTest(func, minPoint, init, GoalType.MINIMIZE, 1e-9, 1e-9);\n\n        // Initial is far from minimum.\n        for (int i = 0; i < dim; i++) {\n            init[i] = minPoint[i] + 3;\n        }\n        doTest(func, minPoint, init, GoalType.MINIMIZE, 1e-9, 1e-5);\n        // More stringent line search tolerance enhances the precision\n        // of the result.\n        doTest(func, minPoint, init, GoalType.MINIMIZE, 1e-9, 1e-9, 1e-7);\n    }",
                "error_msg": "junit.framework.AssertionFailedError\n\tat org.junit.Assert.fail(Assert.java:86)\n\tat org.junit.Assert.assertTrue(Assert.java:41)\n\tat org.junit.Assert.assertTrue(Assert.java:52)\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.PowellOptimizerTest.doTest(PowellOptimizerTest.java:266)\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.PowellOptimizerTest.testSumSinc(PowellOptimizerTest.java:75)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n\tat junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:38)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:520)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeInVM(JUnitTask.java:1484)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:872)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeOrQueue(JUnitTask.java:1972)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute1(JUnitTask.java:824)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:2277)\n\tat org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)\n\tat org.apache.tools.ant.Task.perform(Task.java:348)\n\tat org.apache.tools.ant.Target.execute(Target.java:392)\n\tat org.apache.tools.ant.Target.performTasks(Target.java:413)\n\tat org.apache.tools.ant.Project.executeSortedTargets(Project.java:1399)\n\tat org.apache.tools.ant.Project.executeTarget(Project.java:1368)\n\tat org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41)\n\tat org.apache.tools.ant.Project.executeTargets(Project.java:1251)\n\tat org.apache.tools.ant.Main.runBuild(Main.java:811)\n\tat org.apache.tools.ant.Main.startAnt(Main.java:217)\n\tat org.apache.tools.ant.launch.Launcher.run(Launcher.java:280)\n\tat org.apache.tools.ant.launch.Launcher.main(Launcher.java:109)",
                "clean_error_msg": "junit.framework.AssertionFailedError\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.PowellOptimizerTest.doTest(PowellOptimizerTest.java:266)  Assert.assertTrue(optim.getIterations() > 0);\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.PowellOptimizerTest.testSumSinc(PowellOptimizerTest.java:75)  doTest(func, minPoint, init, GoalType.MINIMIZE, 1e-9, 1e-9, 1e-7);"
            },
            "org.apache.commons.math3.optim.nonlinear.scalar.noderiv.SimplexOptimizerMultiDirectionalTest::testMaximize1": {
                "path": "apache/commons/math3/optim/nonlinear/scalar/noderiv/SimplexOptimizerNelderMeadTest.java",
                "function_name": "testMaximize1",
                "src": "    public void testMaximize1() {\n        SimplexOptimizer optimizer = new SimplexOptimizer(1e-10, 1e-30);\n        final FourExtrema fourExtrema = new FourExtrema();\n\n        final PointValuePair optimum\n            = optimizer.optimize(new MaxEval(100),\n                                 new ObjectiveFunction(fourExtrema),\n                                 GoalType.MAXIMIZE,\n                                 new InitialGuess(new double[] { -3, 0 }),\n                                 new NelderMeadSimplex(new double[] { 0.2, 0.2 }));\n        Assert.assertEquals(fourExtrema.xM, optimum.getPoint()[0], 1e-5);\n        Assert.assertEquals(fourExtrema.yM, optimum.getPoint()[1], 3e-6);\n        Assert.assertEquals(fourExtrema.valueXmYm, optimum.getValue(), 3e-12);\n        Assert.assertTrue(optimizer.getEvaluations() > 60);\n        Assert.assertTrue(optimizer.getEvaluations() < 90);\n\n        // Check that the number of iterations is updated (MATH-949).\n        Assert.assertTrue(optimizer.getIterations() > 0);\n    }",
                "error_msg": "junit.framework.AssertionFailedError\n\tat org.junit.Assert.fail(Assert.java:86)\n\tat org.junit.Assert.assertTrue(Assert.java:41)\n\tat org.junit.Assert.assertTrue(Assert.java:52)\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.SimplexOptimizerMultiDirectionalTest.testMaximize1(SimplexOptimizerMultiDirectionalTest.java:108)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n\tat junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:38)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:520)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeInVM(JUnitTask.java:1484)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:872)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeOrQueue(JUnitTask.java:1972)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute1(JUnitTask.java:824)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:2277)\n\tat org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)\n\tat org.apache.tools.ant.Task.perform(Task.java:348)\n\tat org.apache.tools.ant.Target.execute(Target.java:392)\n\tat org.apache.tools.ant.Target.performTasks(Target.java:413)\n\tat org.apache.tools.ant.Project.executeSortedTargets(Project.java:1399)\n\tat org.apache.tools.ant.Project.executeTarget(Project.java:1368)\n\tat org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41)\n\tat org.apache.tools.ant.Project.executeTargets(Project.java:1251)\n\tat org.apache.tools.ant.Main.runBuild(Main.java:811)\n\tat org.apache.tools.ant.Main.startAnt(Main.java:217)\n\tat org.apache.tools.ant.launch.Launcher.run(Launcher.java:280)\n\tat org.apache.tools.ant.launch.Launcher.main(Launcher.java:109)",
                "clean_error_msg": "junit.framework.AssertionFailedError\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.SimplexOptimizerMultiDirectionalTest.testMaximize1(SimplexOptimizerMultiDirectionalTest.java:108)  Assert.assertTrue(optimizer.getIterations() > 0);"
            },
            "org.apache.commons.math3.optim.nonlinear.scalar.noderiv.SimplexOptimizerMultiDirectionalTest::testMaximize2": {
                "path": "apache/commons/math3/optim/nonlinear/scalar/noderiv/SimplexOptimizerNelderMeadTest.java",
                "function_name": "testMaximize2",
                "src": "    public void testMaximize2() {\n        SimplexOptimizer optimizer = new SimplexOptimizer(1e-10, 1e-30);\n        final FourExtrema fourExtrema = new FourExtrema();\n\n        final PointValuePair optimum\n            = optimizer.optimize(new MaxEval(100),\n                                 new ObjectiveFunction(fourExtrema),\n                                 GoalType.MAXIMIZE,\n                                 new InitialGuess(new double[] { 1, 0 }),\n                                 new NelderMeadSimplex(new double[] { 0.2, 0.2 }));\n        Assert.assertEquals(fourExtrema.xP, optimum.getPoint()[0], 4e-6);\n        Assert.assertEquals(fourExtrema.yP, optimum.getPoint()[1], 5e-6);\n        Assert.assertEquals(fourExtrema.valueXpYp, optimum.getValue(), 7e-12);\n        Assert.assertTrue(optimizer.getEvaluations() > 60);\n        Assert.assertTrue(optimizer.getEvaluations() < 90);\n\n        // Check that the number of iterations is updated (MATH-949).\n        Assert.assertTrue(optimizer.getIterations() > 0);\n    }",
                "error_msg": "junit.framework.AssertionFailedError\n\tat org.junit.Assert.fail(Assert.java:86)\n\tat org.junit.Assert.assertTrue(Assert.java:41)\n\tat org.junit.Assert.assertTrue(Assert.java:52)\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.SimplexOptimizerMultiDirectionalTest.testMaximize2(SimplexOptimizerMultiDirectionalTest.java:129)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n\tat junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:38)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:520)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeInVM(JUnitTask.java:1484)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:872)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeOrQueue(JUnitTask.java:1972)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute1(JUnitTask.java:824)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:2277)\n\tat org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)\n\tat org.apache.tools.ant.Task.perform(Task.java:348)\n\tat org.apache.tools.ant.Target.execute(Target.java:392)\n\tat org.apache.tools.ant.Target.performTasks(Target.java:413)\n\tat org.apache.tools.ant.Project.executeSortedTargets(Project.java:1399)\n\tat org.apache.tools.ant.Project.executeTarget(Project.java:1368)\n\tat org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41)\n\tat org.apache.tools.ant.Project.executeTargets(Project.java:1251)\n\tat org.apache.tools.ant.Main.runBuild(Main.java:811)\n\tat org.apache.tools.ant.Main.startAnt(Main.java:217)\n\tat org.apache.tools.ant.launch.Launcher.run(Launcher.java:280)\n\tat org.apache.tools.ant.launch.Launcher.main(Launcher.java:109)",
                "clean_error_msg": "junit.framework.AssertionFailedError\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.SimplexOptimizerMultiDirectionalTest.testMaximize2(SimplexOptimizerMultiDirectionalTest.java:129)  Assert.assertTrue(optimizer.getIterations() > 0);"
            },
            "org.apache.commons.math3.optim.nonlinear.scalar.noderiv.SimplexOptimizerMultiDirectionalTest::testMinimize1": {
                "path": "apache/commons/math3/optim/nonlinear/scalar/noderiv/SimplexOptimizerNelderMeadTest.java",
                "function_name": "testMinimize1",
                "src": "    public void testMinimize1() {\n        SimplexOptimizer optimizer = new SimplexOptimizer(1e-10, 1e-30);\n        final FourExtrema fourExtrema = new FourExtrema();\n\n        final PointValuePair optimum\n            = optimizer.optimize(new MaxEval(100),\n                                 new ObjectiveFunction(fourExtrema),\n                                 GoalType.MINIMIZE,\n                                 new InitialGuess(new double[] { -3, 0 }),\n                                 new NelderMeadSimplex(new double[] { 0.2, 0.2 }));\n        Assert.assertEquals(fourExtrema.xM, optimum.getPoint()[0], 2e-7);\n        Assert.assertEquals(fourExtrema.yP, optimum.getPoint()[1], 2e-5);\n        Assert.assertEquals(fourExtrema.valueXmYp, optimum.getValue(), 6e-12);\n        Assert.assertTrue(optimizer.getEvaluations() > 60);\n        Assert.assertTrue(optimizer.getEvaluations() < 90);\n\n        // Check that the number of iterations is updated (MATH-949).\n        Assert.assertTrue(optimizer.getIterations() > 0);\n    }",
                "error_msg": "junit.framework.AssertionFailedError\n\tat org.junit.Assert.fail(Assert.java:86)\n\tat org.junit.Assert.assertTrue(Assert.java:41)\n\tat org.junit.Assert.assertTrue(Assert.java:52)\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.SimplexOptimizerMultiDirectionalTest.testMinimize1(SimplexOptimizerMultiDirectionalTest.java:66)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n\tat junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:38)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:520)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeInVM(JUnitTask.java:1484)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:872)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeOrQueue(JUnitTask.java:1972)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute1(JUnitTask.java:824)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:2277)\n\tat org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)\n\tat org.apache.tools.ant.Task.perform(Task.java:348)\n\tat org.apache.tools.ant.Target.execute(Target.java:392)\n\tat org.apache.tools.ant.Target.performTasks(Target.java:413)\n\tat org.apache.tools.ant.Project.executeSortedTargets(Project.java:1399)\n\tat org.apache.tools.ant.Project.executeTarget(Project.java:1368)\n\tat org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41)\n\tat org.apache.tools.ant.Project.executeTargets(Project.java:1251)\n\tat org.apache.tools.ant.Main.runBuild(Main.java:811)\n\tat org.apache.tools.ant.Main.startAnt(Main.java:217)\n\tat org.apache.tools.ant.launch.Launcher.run(Launcher.java:280)\n\tat org.apache.tools.ant.launch.Launcher.main(Launcher.java:109)",
                "clean_error_msg": "junit.framework.AssertionFailedError\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.SimplexOptimizerMultiDirectionalTest.testMinimize1(SimplexOptimizerMultiDirectionalTest.java:66)  Assert.assertTrue(optimizer.getIterations() > 0);"
            },
            "org.apache.commons.math3.optim.nonlinear.scalar.noderiv.SimplexOptimizerMultiDirectionalTest::testMinimize2": {
                "path": "apache/commons/math3/optim/nonlinear/scalar/noderiv/SimplexOptimizerNelderMeadTest.java",
                "function_name": "testMinimize2",
                "src": "    public void testMinimize2() {\n        SimplexOptimizer optimizer = new SimplexOptimizer(1e-10, 1e-30);\n        final FourExtrema fourExtrema = new FourExtrema();\n\n        final PointValuePair optimum\n            = optimizer.optimize(new MaxEval(100),\n                                 new ObjectiveFunction(fourExtrema),\n                                 GoalType.MINIMIZE,\n                                 new InitialGuess(new double[] { 1, 0 }),\n                                 new NelderMeadSimplex(new double[] { 0.2, 0.2 }));\n        Assert.assertEquals(fourExtrema.xP, optimum.getPoint()[0], 5e-6);\n        Assert.assertEquals(fourExtrema.yM, optimum.getPoint()[1], 6e-6);\n        Assert.assertEquals(fourExtrema.valueXpYm, optimum.getValue(), 1e-11);\n        Assert.assertTrue(optimizer.getEvaluations() > 60);\n        Assert.assertTrue(optimizer.getEvaluations() < 90);\n\n        // Check that the number of iterations is updated (MATH-949).\n        Assert.assertTrue(optimizer.getIterations() > 0);\n    }",
                "error_msg": "junit.framework.AssertionFailedError\n\tat org.junit.Assert.fail(Assert.java:86)\n\tat org.junit.Assert.assertTrue(Assert.java:41)\n\tat org.junit.Assert.assertTrue(Assert.java:52)\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.SimplexOptimizerMultiDirectionalTest.testMinimize2(SimplexOptimizerMultiDirectionalTest.java:87)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n\tat junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:38)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:520)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeInVM(JUnitTask.java:1484)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:872)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeOrQueue(JUnitTask.java:1972)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute1(JUnitTask.java:824)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:2277)\n\tat org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)\n\tat org.apache.tools.ant.Task.perform(Task.java:348)\n\tat org.apache.tools.ant.Target.execute(Target.java:392)\n\tat org.apache.tools.ant.Target.performTasks(Target.java:413)\n\tat org.apache.tools.ant.Project.executeSortedTargets(Project.java:1399)\n\tat org.apache.tools.ant.Project.executeTarget(Project.java:1368)\n\tat org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41)\n\tat org.apache.tools.ant.Project.executeTargets(Project.java:1251)\n\tat org.apache.tools.ant.Main.runBuild(Main.java:811)\n\tat org.apache.tools.ant.Main.startAnt(Main.java:217)\n\tat org.apache.tools.ant.launch.Launcher.run(Launcher.java:280)\n\tat org.apache.tools.ant.launch.Launcher.main(Launcher.java:109)",
                "clean_error_msg": "junit.framework.AssertionFailedError\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.SimplexOptimizerMultiDirectionalTest.testMinimize2(SimplexOptimizerMultiDirectionalTest.java:87)  Assert.assertTrue(optimizer.getIterations() > 0);"
            },
            "org.apache.commons.math3.optim.nonlinear.scalar.noderiv.SimplexOptimizerNelderMeadTest::testMaximize1": {
                "path": "apache/commons/math3/optim/nonlinear/scalar/noderiv/SimplexOptimizerNelderMeadTest.java",
                "function_name": "testMaximize1",
                "src": "    public void testMaximize1() {\n        SimplexOptimizer optimizer = new SimplexOptimizer(1e-10, 1e-30);\n        final FourExtrema fourExtrema = new FourExtrema();\n\n        final PointValuePair optimum\n            = optimizer.optimize(new MaxEval(100),\n                                 new ObjectiveFunction(fourExtrema),\n                                 GoalType.MAXIMIZE,\n                                 new InitialGuess(new double[] { -3, 0 }),\n                                 new NelderMeadSimplex(new double[] { 0.2, 0.2 }));\n        Assert.assertEquals(fourExtrema.xM, optimum.getPoint()[0], 1e-5);\n        Assert.assertEquals(fourExtrema.yM, optimum.getPoint()[1], 3e-6);\n        Assert.assertEquals(fourExtrema.valueXmYm, optimum.getValue(), 3e-12);\n        Assert.assertTrue(optimizer.getEvaluations() > 60);\n        Assert.assertTrue(optimizer.getEvaluations() < 90);\n\n        // Check that the number of iterations is updated (MATH-949).\n        Assert.assertTrue(optimizer.getIterations() > 0);\n    }",
                "error_msg": "junit.framework.AssertionFailedError\n\tat org.junit.Assert.fail(Assert.java:86)\n\tat org.junit.Assert.assertTrue(Assert.java:41)\n\tat org.junit.Assert.assertTrue(Assert.java:52)\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.SimplexOptimizerNelderMeadTest.testMaximize1(SimplexOptimizerNelderMeadTest.java:113)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n\tat junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:38)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:520)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeInVM(JUnitTask.java:1484)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:872)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeOrQueue(JUnitTask.java:1972)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute1(JUnitTask.java:824)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:2277)\n\tat org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)\n\tat org.apache.tools.ant.Task.perform(Task.java:348)\n\tat org.apache.tools.ant.Target.execute(Target.java:392)\n\tat org.apache.tools.ant.Target.performTasks(Target.java:413)\n\tat org.apache.tools.ant.Project.executeSortedTargets(Project.java:1399)\n\tat org.apache.tools.ant.Project.executeTarget(Project.java:1368)\n\tat org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41)\n\tat org.apache.tools.ant.Project.executeTargets(Project.java:1251)\n\tat org.apache.tools.ant.Main.runBuild(Main.java:811)\n\tat org.apache.tools.ant.Main.startAnt(Main.java:217)\n\tat org.apache.tools.ant.launch.Launcher.run(Launcher.java:280)\n\tat org.apache.tools.ant.launch.Launcher.main(Launcher.java:109)",
                "clean_error_msg": "junit.framework.AssertionFailedError\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.SimplexOptimizerNelderMeadTest.testMaximize1(SimplexOptimizerNelderMeadTest.java:113)  Assert.assertTrue(optimizer.getIterations() > 0);"
            },
            "org.apache.commons.math3.optim.nonlinear.scalar.noderiv.SimplexOptimizerNelderMeadTest::testMaximize2": {
                "path": "apache/commons/math3/optim/nonlinear/scalar/noderiv/SimplexOptimizerNelderMeadTest.java",
                "function_name": "testMaximize2",
                "src": "    public void testMaximize2() {\n        SimplexOptimizer optimizer = new SimplexOptimizer(1e-10, 1e-30);\n        final FourExtrema fourExtrema = new FourExtrema();\n\n        final PointValuePair optimum\n            = optimizer.optimize(new MaxEval(100),\n                                 new ObjectiveFunction(fourExtrema),\n                                 GoalType.MAXIMIZE,\n                                 new InitialGuess(new double[] { 1, 0 }),\n                                 new NelderMeadSimplex(new double[] { 0.2, 0.2 }));\n        Assert.assertEquals(fourExtrema.xP, optimum.getPoint()[0], 4e-6);\n        Assert.assertEquals(fourExtrema.yP, optimum.getPoint()[1], 5e-6);\n        Assert.assertEquals(fourExtrema.valueXpYp, optimum.getValue(), 7e-12);\n        Assert.assertTrue(optimizer.getEvaluations() > 60);\n        Assert.assertTrue(optimizer.getEvaluations() < 90);\n\n        // Check that the number of iterations is updated (MATH-949).\n        Assert.assertTrue(optimizer.getIterations() > 0);\n    }",
                "error_msg": "junit.framework.AssertionFailedError\n\tat org.junit.Assert.fail(Assert.java:86)\n\tat org.junit.Assert.assertTrue(Assert.java:41)\n\tat org.junit.Assert.assertTrue(Assert.java:52)\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.SimplexOptimizerNelderMeadTest.testMaximize2(SimplexOptimizerNelderMeadTest.java:134)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n\tat junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:38)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:520)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeInVM(JUnitTask.java:1484)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:872)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeOrQueue(JUnitTask.java:1972)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute1(JUnitTask.java:824)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:2277)\n\tat org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)\n\tat org.apache.tools.ant.Task.perform(Task.java:348)\n\tat org.apache.tools.ant.Target.execute(Target.java:392)\n\tat org.apache.tools.ant.Target.performTasks(Target.java:413)\n\tat org.apache.tools.ant.Project.executeSortedTargets(Project.java:1399)\n\tat org.apache.tools.ant.Project.executeTarget(Project.java:1368)\n\tat org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41)\n\tat org.apache.tools.ant.Project.executeTargets(Project.java:1251)\n\tat org.apache.tools.ant.Main.runBuild(Main.java:811)\n\tat org.apache.tools.ant.Main.startAnt(Main.java:217)\n\tat org.apache.tools.ant.launch.Launcher.run(Launcher.java:280)\n\tat org.apache.tools.ant.launch.Launcher.main(Launcher.java:109)",
                "clean_error_msg": "junit.framework.AssertionFailedError\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.SimplexOptimizerNelderMeadTest.testMaximize2(SimplexOptimizerNelderMeadTest.java:134)  Assert.assertTrue(optimizer.getIterations() > 0);"
            },
            "org.apache.commons.math3.optim.nonlinear.scalar.noderiv.SimplexOptimizerNelderMeadTest::testMinimize1": {
                "path": "apache/commons/math3/optim/nonlinear/scalar/noderiv/SimplexOptimizerNelderMeadTest.java",
                "function_name": "testMinimize1",
                "src": "    public void testMinimize1() {\n        SimplexOptimizer optimizer = new SimplexOptimizer(1e-10, 1e-30);\n        final FourExtrema fourExtrema = new FourExtrema();\n\n        final PointValuePair optimum\n            = optimizer.optimize(new MaxEval(100),\n                                 new ObjectiveFunction(fourExtrema),\n                                 GoalType.MINIMIZE,\n                                 new InitialGuess(new double[] { -3, 0 }),\n                                 new NelderMeadSimplex(new double[] { 0.2, 0.2 }));\n        Assert.assertEquals(fourExtrema.xM, optimum.getPoint()[0], 2e-7);\n        Assert.assertEquals(fourExtrema.yP, optimum.getPoint()[1], 2e-5);\n        Assert.assertEquals(fourExtrema.valueXmYp, optimum.getValue(), 6e-12);\n        Assert.assertTrue(optimizer.getEvaluations() > 60);\n        Assert.assertTrue(optimizer.getEvaluations() < 90);\n\n        // Check that the number of iterations is updated (MATH-949).\n        Assert.assertTrue(optimizer.getIterations() > 0);\n    }",
                "error_msg": "junit.framework.AssertionFailedError\n\tat org.junit.Assert.fail(Assert.java:86)\n\tat org.junit.Assert.assertTrue(Assert.java:41)\n\tat org.junit.Assert.assertTrue(Assert.java:52)\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.SimplexOptimizerNelderMeadTest.testMinimize1(SimplexOptimizerNelderMeadTest.java:71)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n\tat junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:38)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:520)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeInVM(JUnitTask.java:1484)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:872)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeOrQueue(JUnitTask.java:1972)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute1(JUnitTask.java:824)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:2277)\n\tat org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)\n\tat org.apache.tools.ant.Task.perform(Task.java:348)\n\tat org.apache.tools.ant.Target.execute(Target.java:392)\n\tat org.apache.tools.ant.Target.performTasks(Target.java:413)\n\tat org.apache.tools.ant.Project.executeSortedTargets(Project.java:1399)\n\tat org.apache.tools.ant.Project.executeTarget(Project.java:1368)\n\tat org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41)\n\tat org.apache.tools.ant.Project.executeTargets(Project.java:1251)\n\tat org.apache.tools.ant.Main.runBuild(Main.java:811)\n\tat org.apache.tools.ant.Main.startAnt(Main.java:217)\n\tat org.apache.tools.ant.launch.Launcher.run(Launcher.java:280)\n\tat org.apache.tools.ant.launch.Launcher.main(Launcher.java:109)",
                "clean_error_msg": "junit.framework.AssertionFailedError\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.SimplexOptimizerNelderMeadTest.testMinimize1(SimplexOptimizerNelderMeadTest.java:71)  Assert.assertTrue(optimizer.getIterations() > 0);"
            },
            "org.apache.commons.math3.optim.nonlinear.scalar.noderiv.SimplexOptimizerNelderMeadTest::testMinimize2": {
                "path": "apache/commons/math3/optim/nonlinear/scalar/noderiv/SimplexOptimizerNelderMeadTest.java",
                "function_name": "testMinimize2",
                "src": "    public void testMinimize2() {\n        SimplexOptimizer optimizer = new SimplexOptimizer(1e-10, 1e-30);\n        final FourExtrema fourExtrema = new FourExtrema();\n\n        final PointValuePair optimum\n            = optimizer.optimize(new MaxEval(100),\n                                 new ObjectiveFunction(fourExtrema),\n                                 GoalType.MINIMIZE,\n                                 new InitialGuess(new double[] { 1, 0 }),\n                                 new NelderMeadSimplex(new double[] { 0.2, 0.2 }));\n        Assert.assertEquals(fourExtrema.xP, optimum.getPoint()[0], 5e-6);\n        Assert.assertEquals(fourExtrema.yM, optimum.getPoint()[1], 6e-6);\n        Assert.assertEquals(fourExtrema.valueXpYm, optimum.getValue(), 1e-11);\n        Assert.assertTrue(optimizer.getEvaluations() > 60);\n        Assert.assertTrue(optimizer.getEvaluations() < 90);\n\n        // Check that the number of iterations is updated (MATH-949).\n        Assert.assertTrue(optimizer.getIterations() > 0);\n    }",
                "error_msg": "junit.framework.AssertionFailedError\n\tat org.junit.Assert.fail(Assert.java:86)\n\tat org.junit.Assert.assertTrue(Assert.java:41)\n\tat org.junit.Assert.assertTrue(Assert.java:52)\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.SimplexOptimizerNelderMeadTest.testMinimize2(SimplexOptimizerNelderMeadTest.java:92)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n\tat junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:38)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:520)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeInVM(JUnitTask.java:1484)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:872)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeOrQueue(JUnitTask.java:1972)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute1(JUnitTask.java:824)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:2277)\n\tat org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)\n\tat org.apache.tools.ant.Task.perform(Task.java:348)\n\tat org.apache.tools.ant.Target.execute(Target.java:392)\n\tat org.apache.tools.ant.Target.performTasks(Target.java:413)\n\tat org.apache.tools.ant.Project.executeSortedTargets(Project.java:1399)\n\tat org.apache.tools.ant.Project.executeTarget(Project.java:1368)\n\tat org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41)\n\tat org.apache.tools.ant.Project.executeTargets(Project.java:1251)\n\tat org.apache.tools.ant.Main.runBuild(Main.java:811)\n\tat org.apache.tools.ant.Main.startAnt(Main.java:217)\n\tat org.apache.tools.ant.launch.Launcher.run(Launcher.java:280)\n\tat org.apache.tools.ant.launch.Launcher.main(Launcher.java:109)",
                "clean_error_msg": "junit.framework.AssertionFailedError\n\tat org.apache.commons.math3.optim.nonlinear.scalar.noderiv.SimplexOptimizerNelderMeadTest.testMinimize2(SimplexOptimizerNelderMeadTest.java:92)  Assert.assertTrue(optimizer.getIterations() > 0);"
            },
            "org.apache.commons.math3.optim.nonlinear.vector.jacobian.GaussNewtonOptimizerTest::testGetIterations": {
                "path": "apache/commons/math3/optim/nonlinear/vector/jacobian/AbstractLeastSquaresOptimizerAbstractTest.java",
                "function_name": "testGetIterations",
                "src": "    public void testGetIterations() {\n        AbstractLeastSquaresOptimizer optim = createOptimizer();\n        optim.optimize(new MaxEval(100), new Target(new double[] { 1 }),\n                       new Weight(new double[] { 1 }),\n                       new InitialGuess(new double[] { 3 }),\n                       new ModelFunction(new MultivariateVectorFunction() {\n                               @Override\n                               public double[] value(double[] point) {\n                                   return new double[] {\n                                       FastMath.pow(point[0], 4)\n                                   };\n                               }\n                           }),\n                       new ModelFunctionJacobian(new MultivariateMatrixFunction() {\n                               @Override\n                               public double[][] value(double[] point) {\n                                   return new double[][] {\n                                       { 0.25 * FastMath.pow(point[0], 3) }\n                                   };\n                               }\n                           }));\n\n        Assert.assertTrue(optim.getIterations() > 0);\n    }",
                "error_msg": "junit.framework.AssertionFailedError\n\tat org.junit.Assert.fail(Assert.java:86)\n\tat org.junit.Assert.assertTrue(Assert.java:41)\n\tat org.junit.Assert.assertTrue(Assert.java:52)\n\tat org.apache.commons.math3.optim.nonlinear.vector.jacobian.AbstractLeastSquaresOptimizerAbstractTest.testGetIterations(AbstractLeastSquaresOptimizerAbstractTest.java:131)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n\tat junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:38)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:520)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeInVM(JUnitTask.java:1484)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:872)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeOrQueue(JUnitTask.java:1972)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute1(JUnitTask.java:824)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:2277)\n\tat org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)\n\tat org.apache.tools.ant.Task.perform(Task.java:348)\n\tat org.apache.tools.ant.Target.execute(Target.java:392)\n\tat org.apache.tools.ant.Target.performTasks(Target.java:413)\n\tat org.apache.tools.ant.Project.executeSortedTargets(Project.java:1399)\n\tat org.apache.tools.ant.Project.executeTarget(Project.java:1368)\n\tat org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41)\n\tat org.apache.tools.ant.Project.executeTargets(Project.java:1251)\n\tat org.apache.tools.ant.Main.runBuild(Main.java:811)\n\tat org.apache.tools.ant.Main.startAnt(Main.java:217)\n\tat org.apache.tools.ant.launch.Launcher.run(Launcher.java:280)\n\tat org.apache.tools.ant.launch.Launcher.main(Launcher.java:109)",
                "clean_error_msg": "junit.framework.AssertionFailedError\n\tat org.apache.commons.math3.optim.nonlinear.vector.jacobian.AbstractLeastSquaresOptimizerAbstractTest.testGetIterations(AbstractLeastSquaresOptimizerAbstractTest.java:131)  Assert.assertTrue(optim.getIterations() > 0);"
            },
            "org.apache.commons.math3.optim.nonlinear.vector.jacobian.LevenbergMarquardtOptimizerTest::testGetIterations": {
                "path": "apache/commons/math3/optim/nonlinear/vector/jacobian/AbstractLeastSquaresOptimizerAbstractTest.java",
                "function_name": "testGetIterations",
                "src": "    public void testGetIterations() {\n        AbstractLeastSquaresOptimizer optim = createOptimizer();\n        optim.optimize(new MaxEval(100), new Target(new double[] { 1 }),\n                       new Weight(new double[] { 1 }),\n                       new InitialGuess(new double[] { 3 }),\n                       new ModelFunction(new MultivariateVectorFunction() {\n                               @Override\n                               public double[] value(double[] point) {\n                                   return new double[] {\n                                       FastMath.pow(point[0], 4)\n                                   };\n                               }\n                           }),\n                       new ModelFunctionJacobian(new MultivariateMatrixFunction() {\n                               @Override\n                               public double[][] value(double[] point) {\n                                   return new double[][] {\n                                       { 0.25 * FastMath.pow(point[0], 3) }\n                                   };\n                               }\n                           }));\n\n        Assert.assertTrue(optim.getIterations() > 0);\n    }",
                "error_msg": "junit.framework.AssertionFailedError\n\tat org.junit.Assert.fail(Assert.java:86)\n\tat org.junit.Assert.assertTrue(Assert.java:41)\n\tat org.junit.Assert.assertTrue(Assert.java:52)\n\tat org.apache.commons.math3.optim.nonlinear.vector.jacobian.AbstractLeastSquaresOptimizerAbstractTest.testGetIterations(AbstractLeastSquaresOptimizerAbstractTest.java:131)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n\tat junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:38)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:520)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeInVM(JUnitTask.java:1484)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:872)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeOrQueue(JUnitTask.java:1972)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute1(JUnitTask.java:824)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:2277)\n\tat org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)\n\tat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)\n\tat org.apache.tools.ant.Task.perform(Task.java:348)\n\tat org.apache.tools.ant.Target.execute(Target.java:392)\n\tat org.apache.tools.ant.Target.performTasks(Target.java:413)\n\tat org.apache.tools.ant.Project.executeSortedTargets(Project.java:1399)\n\tat org.apache.tools.ant.Project.executeTarget(Project.java:1368)\n\tat org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41)\n\tat org.apache.tools.ant.Project.executeTargets(Project.java:1251)\n\tat org.apache.tools.ant.Main.runBuild(Main.java:811)\n\tat org.apache.tools.ant.Main.startAnt(Main.java:217)\n\tat org.apache.tools.ant.launch.Launcher.run(Launcher.java:280)\n\tat org.apache.tools.ant.launch.Launcher.main(Launcher.java:109)",
                "clean_error_msg": "junit.framework.AssertionFailedError\n\tat org.apache.commons.math3.optim.nonlinear.vector.jacobian.AbstractLeastSquaresOptimizerAbstractTest.testGetIterations(AbstractLeastSquaresOptimizerAbstractTest.java:131)  Assert.assertTrue(optim.getIterations() > 0);"
            }
        },
        "evaluation_done": true,
        "context_generation_done": true
    }
}